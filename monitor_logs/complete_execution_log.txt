================================================================================
TORCHRUN EXECUTION WITH GITHUB BACKUP
Start Time: 2026-01-15 07:53:18
GitHub Repo: https://github.com/Arnav2Prasad/Transformer_GPU.git
Complete log will be saved to: complete_execution_log.txt
================================================================================

Configuring git identity globally...

================================================================================
Setting up GitHub repository...
Cloning repository with authentication...
âœ“ Repository cloned successfully

################################################################################
STARTING RUN 1/8
Time: 07:53:18
################################################################################

train.py updated with parallel_flag = 1
Running torchrun command for i=1...
W0115 07:53:26.076000 107 torch/distributed/run.py:774] 
W0115 07:53:26.076000 107 torch/distributed/run.py:774] *****************************************
W0115 07:53:26.076000 107 torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0115 07:53:26.076000 107 torch/distributed/run.py:774] *****************************************
[W115 07:53:26.191539238 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W115 07:53:26.192309455 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
parallel_flag :  1
parallel_flag :  1
[W115 07:53:38.122693672 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W115 07:53:38.122981077 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W115 07:53:38.123458980 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W115 07:53:38.124017975 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
DDP_WORLD_SIZE = 2
=============
parallel_flag -  1
parallel_flag -  1
parallel_flag -  1
=============
=============
parallel_flag -  1
parallel_flag -  1
parallel_flag -  1
=============
total parameters = 186,274,816, active parameters = 110,777,344
Using compiled model
[rank1]:W0115 07:53:50.246000 115 torch/_logging/_internal.py:1154] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
wandb: Currently logged in as: adeeb-idris (adeeb-idris-coep-technological-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run vz6m5yga
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /kaggle/working/Transformer_GPU/project/wandb/run-20260115_075350-vz6m5yga
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run shakespeare_gqa_20260115_075350
wandb: â­ï¸ View project at https://wandb.ai/adeeb-idris-coep-technological-university/llm-training
wandb: ğŸš€ View run at https://wandb.ai/adeeb-idris-coep-technological-university/llm-training/runs/vz6m5yga
WandB initialized: project=llm-training, run=shakespeare_gqa_20260115_075350
[rank0]:W0115 07:53:53.993000 114 torch/_logging/_internal.py:1154] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank1]:W0115 07:53:59.001000 115 torch/_inductor/utils.py:1436] [7/0_1] Not enough SMs to use max_autotune_gemm mode
[rank0]:W0115 07:53:59.002000 114 torch/_inductor/utils.py:1436] [7/0_1] Not enough SMs to use max_autotune_gemm mode
active-> 110777344
MFU: 0.08%
mfu 0.08%
step: 0 | loss:10.9839 | dt:15095.76ms | tok/s:543 | MFU:0.08% | GPU RAM:6.14GB
active-> 110777344
MFU: 0.75%
mfu 0.14%
step: 1 | loss:11.0666 | dt:1549.81ms | tok/s:5,286 | MFU:0.75% | GPU RAM:7.35GB
active-> 110777344
MFU: 0.81%
mfu 0.21%
step: 2 | loss:10.9682 | dt:1434.65ms | tok/s:5,710 | MFU:0.81% | GPU RAM:7.35GB
active-> 110777344
MFU: 0.81%
mfu 0.27%
step: 3 | loss:11.1122 | dt:1438.90ms | tok/s:5,693 | MFU:0.81% | GPU RAM:7.35GB
active-> 110777344
MFU: 0.81%
mfu 0.32%
step: 4 | loss:11.0980 | dt:1446.89ms | tok/s:5,662 | MFU:0.81% | GPU RAM:7.36GB
active-> 110777344
MFU: 0.81%
mfu 0.37%
step: 5 | loss:11.0153 | dt:1439.95ms | tok/s:5,689 | MFU:0.81% | GPU RAM:7.36GB
active-> 110777344
MFU: 0.80%
mfu 0.42%
step: 6 | loss:11.1145 | dt:1451.00ms | tok/s:5,646 | MFU:0.80% | GPU RAM:7.36GB
active-> 110777344
MFU: 0.81%
mfu 0.46%
step: 7 | loss:11.0012 | dt:1445.44ms | tok/s:5,667 | MFU:0.81% | GPU RAM:7.36GB
active-> 110777344
MFU: 0.80%
mfu 0.49%
step: 8 | loss:11.0494 | dt:1457.12ms | tok/s:5,622 | MFU:0.80% | GPU RAM:7.36GB
active-> 110777344
MFU: 0.80%
mfu 0.52%
step: 9 | loss:11.0502 | dt:1456.04ms | tok/s:5,626 | MFU:0.80% | GPU RAM:7.36GB
active-> 110777344
MFU: 0.79%
mfu 0.55%
step: 10 | loss:11.0286 | dt:1479.15ms | tok/s:5,538 | MFU:0.79% | GPU RAM:7.36GB
active-> 110777344
MFU: 0.79%
mfu 0.57%
step: 11 | loss:10.9935 | dt:1475.00ms | tok/s:5,554 | MFU:0.79% | GPU RAM:7.36GB
active-> 110777344
MFU: 0.78%
mfu 0.59%
step: 12 | loss:11.0236 | dt:1502.85ms | tok/s:5,451 | MFU:0.78% | GPU RAM:7.36GB
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                               aten::mm         2.29%     106.367ms         3.96%     183.982ms      73.241us        1.761s        55.33%        1.761s     700.997us          2512   5081986.499  
                                           aten::linear         0.25%      11.724ms         4.55%     211.463ms     115.934us       0.000us         0.00%        1.151s     631.023us          1824            --  
       autograd::engine::evaluate_function: MmBackward0         0.27%      12.649ms         3.75%     174.461ms     231.996us       0.000us         0.00%        1.127s       1.499ms           752            --  
                                            MmBackward0         0.35%      16.201ms         3.48%     161.812ms     215.176us       0.000us         0.00%        1.127s       1.499ms           752            --  
                        DistributedDataParallel.forward         0.00%       0.000us         0.00%       0.000us       0.000us        1.075s        33.77%        1.075s     268.660ms             4            --  
                                          ProfilerStep*         0.66%      30.758ms        66.22%        3.079s        1.539s       0.000us         0.00%     998.685ms     499.343ms             2            --  
                                           forward_pass         0.02%       1.156ms        23.58%        1.096s     274.049ms       0.000us         0.00%     913.249ms     228.312ms             4            --  
                             Torch-Compiled Region: 0/0         0.02%     750.428us        23.55%        1.095s     273.687ms       0.000us         0.00%     913.239ms     228.310ms             4            --  
                        DistributedDataParallel.forward         0.11%       5.092ms        23.53%        1.094s     273.499ms       0.000us         0.00%     913.239ms     228.310ms             4            --  
                             Torch-Compiled Region: 4/0         0.00%      40.159us        23.28%        1.082s     270.610ms       0.000us         0.00%     905.053ms     226.263ms             4            --  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 4.649s
Self CUDA time total: 3.183s

active-> 110777344
MFU: 0.05%
mfu 0.54%
step: 13 | loss:10.9566 | dt:24307.05ms | tok/s:337 | MFU:0.05% | GPU RAM:7.36GB
active-> 110777344
MFU: 0.76%
mfu 0.56%
step: 14 | loss:10.9520 | dt:1532.13ms | tok/s:5,347 | MFU:0.76% | GPU RAM:7.36GB
active-> 110777344
MFU: 0.79%
mfu 0.58%
step: 15 | loss:10.9506 | dt:1474.87ms | tok/s:5,554 | MFU:0.79% | GPU RAM:7.36GB
active-> 110777344
MFU: 0.79%
mfu 0.60%
step: 16 | loss:11.0116 | dt:1474.81ms | tok/s:5,555 | MFU:0.79% | GPU RAM:7.36GB
active-> 110777344
MFU: 0.79%
mfu 0.62%
step: 17 | loss:10.9148 | dt:1482.39ms | tok/s:5,526 | MFU:0.79% | GPU RAM:7.36GB
active-> 110777344
MFU: 0.79%
mfu 0.64%
step: 18 | loss:10.9888 | dt:1484.62ms | tok/s:5,518 | MFU:0.79% | GPU RAM:7.36GB
active-> 110777344
MFU: 0.78%
mfu 0.65%
step: 19 | loss:11.0425 | dt:1493.42ms | tok/s:5,485 | MFU:0.78% | GPU RAM:7.36GB
active-> 110777344
MFU: 0.78%
mfu 0.67%
step: 20 | loss:10.9024 | dt:1487.31ms | tok/s:5,508 | MFU:0.78% | GPU RAM:7.36GB
active-> 110777344
MFU: 0.78%
mfu 0.68%
step: 21 | loss:10.8622 | dt:1493.43ms | tok/s:5,485 | MFU:0.78% | GPU RAM:7.36GB
active-> 110777344
MFU: 0.78%
mfu 0.69%
step: 22 | loss:10.9393 | dt:1492.96ms | tok/s:5,487 | MFU:0.78% | GPU RAM:7.36GB
active-> 110777344
MFU: 0.78%
mfu 0.70%
step: 23 | loss:10.8511 | dt:1499.61ms | tok/s:5,463 | MFU:0.78% | GPU RAM:7.36GB
active-> 110777344
MFU: 0.78%
mfu 0.70%
step: 24 | loss:10.9447 | dt:1500.70ms | tok/s:5,459 | MFU:0.78% | GPU RAM:7.36GB
active-> 110777344
MFU: 0.78%
mfu 0.71%
step: 25 | loss:10.9635 | dt:1503.79ms | tok/s:5,448 | MFU:0.78% | GPU RAM:7.36GB
active-> 110777344
MFU: 0.77%
mfu 0.72%
step: 26 | loss:10.8812 | dt:1505.97ms | tok/s:5,440 | MFU:0.77% | GPU RAM:7.36GB
active-> 110777344
MFU: 0.77%
mfu 0.72%
step: 27 | loss:10.7737 | dt:1510.77ms | tok/s:5,422 | MFU:0.77% | GPU RAM:7.36GB
active-> 110777344
MFU: 0.77%
mfu 0.73%
step: 28 | loss:10.8028 | dt:1518.52ms | tok/s:5,395 | MFU:0.77% | GPU RAM:7.36GB
active-> 110777344
MFU: 0.77%
mfu 0.73%
step: 29 | loss:10.7564 | dt:1519.58ms | tok/s:5,391 | MFU:0.77% | GPU RAM:7.36GB
active-> 110777344
MFU: 0.77%
mfu 0.74%
step: 30 | loss:10.9134 | dt:1523.31ms | tok/s:5,378 | MFU:0.77% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.76%
mfu 0.74%
step: 31 | loss:10.8430 | dt:1532.83ms | tok/s:5,344 | MFU:0.76% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.77%
mfu 0.74%
step: 32 | loss:10.7698 | dt:1510.90ms | tok/s:5,422 | MFU:0.77% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.76%
mfu 0.74%
step: 33 | loss:10.7970 | dt:1534.52ms | tok/s:5,338 | MFU:0.76% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.76%
mfu 0.74%
step: 34 | loss:10.7591 | dt:1542.86ms | tok/s:5,310 | MFU:0.76% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.76%
mfu 0.75%
step: 35 | loss:10.5613 | dt:1543.99ms | tok/s:5,306 | MFU:0.76% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.75%
mfu 0.75%
step: 36 | loss:10.6321 | dt:1547.56ms | tok/s:5,293 | MFU:0.75% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.75%
mfu 0.75%
step: 37 | loss:10.7513 | dt:1553.27ms | tok/s:5,274 | MFU:0.75% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.75%
mfu 0.75%
step: 38 | loss:10.6103 | dt:1559.18ms | tok/s:5,254 | MFU:0.75% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.75%
mfu 0.75%
step: 39 | loss:10.6334 | dt:1553.25ms | tok/s:5,274 | MFU:0.75% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.74%
mfu 0.75%
step: 40 | loss:10.4404 | dt:1567.72ms | tok/s:5,225 | MFU:0.74% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.74%
mfu 0.75%
step: 41 | loss:10.5239 | dt:1569.91ms | tok/s:5,218 | MFU:0.74% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.74%
mfu 0.75%
step: 42 | loss:10.5276 | dt:1574.09ms | tok/s:5,204 | MFU:0.74% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.74%
mfu 0.75%
step: 43 | loss:10.6020 | dt:1571.62ms | tok/s:5,212 | MFU:0.74% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.74%
mfu 0.74%
step: 44 | loss:10.4043 | dt:1585.01ms | tok/s:5,168 | MFU:0.74% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.74%
mfu 0.74%
step: 45 | loss:10.3746 | dt:1567.79ms | tok/s:5,225 | MFU:0.74% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.73%
mfu 0.74%
step: 46 | loss:10.4890 | dt:1589.71ms | tok/s:5,153 | MFU:0.73% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.74%
mfu 0.74%
step: 47 | loss:10.3911 | dt:1585.67ms | tok/s:5,166 | MFU:0.74% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.73%
mfu 0.74%
step: 48 | loss:10.3861 | dt:1595.54ms | tok/s:5,134 | MFU:0.73% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.19%
mfu 0.69%
step: 49 | loss:10.3106 | dt:6120.03ms | tok/s:1,339 | MFU:0.19% | GPU RAM:7.37GB
--------val run-------- train loss 10.3024 | val loss 10.3351 | dt 69773.7108ms
active-> 110777344
MFU: 0.67%
mfu 0.68%
step: 50 | loss:10.2340 | dt:1746.75ms | tok/s:4,690 | MFU:0.67% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.69%
step: 51 | loss:10.3210 | dt:1635.11ms | tok/s:5,010 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.69%
step: 52 | loss:10.2519 | dt:1623.17ms | tok/s:5,047 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.69%
step: 53 | loss:10.2179 | dt:1627.76ms | tok/s:5,033 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.70%
step: 54 | loss:10.0204 | dt:1611.54ms | tok/s:5,083 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.70%
step: 55 | loss:10.1588 | dt:1615.30ms | tok/s:5,071 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.70%
step: 56 | loss:10.0181 | dt:1619.64ms | tok/s:5,058 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.70%
step: 57 | loss:10.1149 | dt:1629.26ms | tok/s:5,028 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 58 | loss:9.9981 | dt:1631.46ms | tok/s:5,021 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 59 | loss:9.8916 | dt:1631.04ms | tok/s:5,023 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.71%
step: 60 | loss:10.0087 | dt:1630.56ms | tok/s:5,024 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.71%
step: 61 | loss:9.9135 | dt:1623.19ms | tok/s:5,047 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.71%
step: 62 | loss:9.8876 | dt:1623.77ms | tok/s:5,045 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.71%
step: 63 | loss:9.8362 | dt:1623.96ms | tok/s:5,044 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 64 | loss:9.7045 | dt:1631.18ms | tok/s:5,022 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 65 | loss:9.8790 | dt:1637.11ms | tok/s:5,004 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 66 | loss:9.8374 | dt:1631.41ms | tok/s:5,021 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 67 | loss:9.6534 | dt:1649.53ms | tok/s:4,966 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 68 | loss:9.6840 | dt:1647.46ms | tok/s:4,972 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 69 | loss:9.3974 | dt:1640.62ms | tok/s:4,993 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 70 | loss:9.4047 | dt:1642.44ms | tok/s:4,988 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 71 | loss:9.2792 | dt:1643.04ms | tok/s:4,986 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.71%
step: 72 | loss:9.2605 | dt:1660.21ms | tok/s:4,934 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.71%
step: 73 | loss:9.2655 | dt:1655.64ms | tok/s:4,948 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.71%
step: 74 | loss:9.1542 | dt:1656.57ms | tok/s:4,945 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 75 | loss:9.0679 | dt:1653.41ms | tok/s:4,955 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 76 | loss:9.1543 | dt:1650.18ms | tok/s:4,964 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.71%
step: 77 | loss:9.0040 | dt:1654.90ms | tok/s:4,950 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.71%
step: 78 | loss:8.8624 | dt:1659.63ms | tok/s:4,936 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.71%
step: 79 | loss:9.1013 | dt:1656.96ms | tok/s:4,944 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 80 | loss:8.9350 | dt:1650.17ms | tok/s:4,964 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.71%
step: 81 | loss:8.6074 | dt:1655.32ms | tok/s:4,949 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 82 | loss:8.6172 | dt:1645.85ms | tok/s:4,977 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 83 | loss:8.6237 | dt:1644.85ms | tok/s:4,980 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 84 | loss:8.7526 | dt:1647.43ms | tok/s:4,973 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 85 | loss:8.8919 | dt:1638.07ms | tok/s:5,001 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 86 | loss:8.3963 | dt:1635.34ms | tok/s:5,009 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 87 | loss:8.5382 | dt:1640.41ms | tok/s:4,994 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.71%
step: 88 | loss:8.7396 | dt:1626.20ms | tok/s:5,038 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 89 | loss:8.1300 | dt:1641.52ms | tok/s:4,990 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 90 | loss:8.2176 | dt:1640.63ms | tok/s:4,993 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.71%
step: 91 | loss:8.0347 | dt:1630.59ms | tok/s:5,024 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.71%
step: 92 | loss:8.1491 | dt:1624.80ms | tok/s:5,042 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.71%
step: 93 | loss:7.9365 | dt:1627.73ms | tok/s:5,033 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.71%
step: 94 | loss:7.6996 | dt:1620.39ms | tok/s:5,056 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.71%
step: 95 | loss:8.1162 | dt:1621.06ms | tok/s:5,053 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.71%
step: 96 | loss:7.9752 | dt:1626.23ms | tok/s:5,037 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 97 | loss:7.7553 | dt:1637.91ms | tok/s:5,001 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.71%
step: 98 | loss:7.3923 | dt:1620.04ms | tok/s:5,057 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.20%
mfu 0.66%
step: 99 | loss:7.4357 | dt:5956.94ms | tok/s:1,375 | MFU:0.20% | GPU RAM:7.37GB
--------val run-------- train loss 7.5953 | val loss 8.0133 | dt 66456.3223ms
active-> 110777344
MFU: 0.70%
mfu 0.67%
step: 100 | loss:7.3382 | dt:1666.77ms | tok/s:4,915 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.67%
step: 101 | loss:7.6332 | dt:1644.58ms | tok/s:4,981 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.67%
step: 102 | loss:7.3634 | dt:1640.57ms | tok/s:4,993 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.68%
step: 103 | loss:7.4331 | dt:1620.21ms | tok/s:5,056 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.68%
step: 104 | loss:7.3693 | dt:1631.34ms | tok/s:5,022 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.69%
step: 105 | loss:7.1968 | dt:1615.31ms | tok/s:5,071 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.69%
step: 106 | loss:7.4845 | dt:1626.45ms | tok/s:5,037 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.69%
step: 107 | loss:6.9967 | dt:1620.98ms | tok/s:5,054 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.70%
step: 108 | loss:7.2027 | dt:1615.05ms | tok/s:5,072 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.70%
step: 109 | loss:6.8607 | dt:1621.25ms | tok/s:5,053 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.70%
step: 110 | loss:7.1346 | dt:1626.49ms | tok/s:5,037 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.70%
step: 111 | loss:6.8137 | dt:1625.53ms | tok/s:5,040 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.70%
step: 112 | loss:6.5791 | dt:1623.41ms | tok/s:5,046 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 113 | loss:7.3860 | dt:1638.67ms | tok/s:4,999 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 114 | loss:7.5692 | dt:1636.91ms | tok/s:5,005 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.71%
step: 115 | loss:6.8293 | dt:1619.87ms | tok/s:5,057 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.71%
step: 116 | loss:6.9423 | dt:1629.04ms | tok/s:5,029 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 117 | loss:6.6627 | dt:1632.77ms | tok/s:5,017 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 118 | loss:7.0634 | dt:1639.51ms | tok/s:4,997 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.71%
step: 119 | loss:7.1348 | dt:1619.53ms | tok/s:5,058 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.71%
step: 120 | loss:6.6989 | dt:1620.36ms | tok/s:5,056 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.71%
step: 121 | loss:6.9252 | dt:1626.66ms | tok/s:5,036 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 122 | loss:6.8224 | dt:1642.84ms | tok/s:4,986 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.71%
step: 123 | loss:6.6929 | dt:1625.91ms | tok/s:5,038 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.71%
step: 124 | loss:6.5623 | dt:1614.86ms | tok/s:5,073 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.71%
step: 125 | loss:6.2433 | dt:1624.78ms | tok/s:5,042 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.71%
step: 126 | loss:6.6417 | dt:1628.26ms | tok/s:5,031 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.71%
step: 127 | loss:6.7615 | dt:1608.80ms | tok/s:5,092 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.71%
step: 128 | loss:6.8684 | dt:1628.33ms | tok/s:5,031 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.71%
step: 129 | loss:6.7066 | dt:1625.96ms | tok/s:5,038 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.72%
step: 130 | loss:6.5313 | dt:1614.32ms | tok/s:5,075 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.72%
step: 131 | loss:6.5206 | dt:1610.15ms | tok/s:5,088 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.72%
step: 132 | loss:6.6481 | dt:1615.72ms | tok/s:5,070 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.72%
step: 133 | loss:6.4332 | dt:1620.92ms | tok/s:5,054 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.72%
step: 134 | loss:6.3022 | dt:1618.44ms | tok/s:5,062 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.72%
step: 135 | loss:6.5862 | dt:1609.18ms | tok/s:5,091 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.73%
mfu 0.72%
step: 136 | loss:6.5422 | dt:1605.02ms | tok/s:5,104 | MFU:0.73% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.72%
step: 137 | loss:6.5608 | dt:1609.59ms | tok/s:5,089 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.72%
step: 138 | loss:6.7085 | dt:1621.06ms | tok/s:5,053 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.72%
step: 139 | loss:6.3950 | dt:1610.09ms | tok/s:5,088 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.73%
mfu 0.72%
step: 140 | loss:7.0272 | dt:1592.27ms | tok/s:5,145 | MFU:0.73% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.73%
mfu 0.72%
step: 141 | loss:6.4450 | dt:1607.74ms | tok/s:5,095 | MFU:0.73% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.72%
step: 142 | loss:6.7797 | dt:1639.16ms | tok/s:4,998 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.73%
mfu 0.72%
step: 143 | loss:6.6982 | dt:1607.28ms | tok/s:5,097 | MFU:0.73% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.72%
step: 144 | loss:6.4184 | dt:1613.33ms | tok/s:5,078 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.72%
step: 145 | loss:6.1940 | dt:1643.78ms | tok/s:4,984 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.72%
step: 146 | loss:6.5398 | dt:1624.35ms | tok/s:5,043 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.73%
mfu 0.72%
step: 147 | loss:6.0869 | dt:1600.53ms | tok/s:5,118 | MFU:0.73% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.72%
step: 148 | loss:6.2977 | dt:1609.08ms | tok/s:5,091 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.20%
mfu 0.67%
step: 149 | loss:6.7628 | dt:5826.92ms | tok/s:1,406 | MFU:0.20% | GPU RAM:7.37GB
--------val run-------- train loss 6.3075 | val loss 7.0009 | dt 66590.5496ms
active-> 110777344
MFU: 0.68%
mfu 0.67%
step: 150 | loss:6.2516 | dt:1707.77ms | tok/s:4,797 | MFU:0.68% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.74%
mfu 0.68%
step: 151 | loss:6.6292 | dt:1583.79ms | tok/s:5,172 | MFU:0.74% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.73%
mfu 0.68%
step: 152 | loss:6.0766 | dt:1592.78ms | tok/s:5,143 | MFU:0.73% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.73%
mfu 0.69%
step: 153 | loss:6.0789 | dt:1600.81ms | tok/s:5,117 | MFU:0.73% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.69%
step: 154 | loss:6.3147 | dt:1612.02ms | tok/s:5,082 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.73%
mfu 0.69%
step: 155 | loss:6.2743 | dt:1606.22ms | tok/s:5,100 | MFU:0.73% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.70%
step: 156 | loss:6.4778 | dt:1624.36ms | tok/s:5,043 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.70%
step: 157 | loss:6.4877 | dt:1617.67ms | tok/s:5,064 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.70%
step: 158 | loss:6.2835 | dt:1617.12ms | tok/s:5,066 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.70%
step: 159 | loss:6.1715 | dt:1612.71ms | tok/s:5,080 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.71%
step: 160 | loss:6.3795 | dt:1618.24ms | tok/s:5,062 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 161 | loss:6.5156 | dt:1634.71ms | tok/s:5,011 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.71%
step: 162 | loss:6.0661 | dt:1625.35ms | tok/s:5,040 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.71%
step: 163 | loss:6.3675 | dt:1629.71ms | tok/s:5,027 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 164 | loss:5.8650 | dt:1645.84ms | tok/s:4,977 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 165 | loss:6.2356 | dt:1630.87ms | tok/s:5,023 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 166 | loss:6.1247 | dt:1644.54ms | tok/s:4,981 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 167 | loss:6.1239 | dt:1647.43ms | tok/s:4,973 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 168 | loss:6.1090 | dt:1631.42ms | tok/s:5,021 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 169 | loss:6.0636 | dt:1633.16ms | tok/s:5,016 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 170 | loss:5.9487 | dt:1636.74ms | tok/s:5,005 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 171 | loss:5.6329 | dt:1637.71ms | tok/s:5,002 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 172 | loss:6.1249 | dt:1636.89ms | tok/s:5,005 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 173 | loss:6.1262 | dt:1643.23ms | tok/s:4,985 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 174 | loss:6.1636 | dt:1639.00ms | tok/s:4,998 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.71%
step: 175 | loss:5.9169 | dt:1654.21ms | tok/s:4,952 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 176 | loss:6.2908 | dt:1636.15ms | tok/s:5,007 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.71%
step: 177 | loss:5.9612 | dt:1620.22ms | tok/s:5,056 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 178 | loss:6.1481 | dt:1636.94ms | tok/s:5,004 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.71%
step: 179 | loss:5.8583 | dt:1629.96ms | tok/s:5,026 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 180 | loss:6.4447 | dt:1632.18ms | tok/s:5,019 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 181 | loss:6.1753 | dt:1645.96ms | tok/s:4,977 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 182 | loss:6.1546 | dt:1635.34ms | tok/s:5,009 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 183 | loss:6.1827 | dt:1640.94ms | tok/s:4,992 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.71%
step: 184 | loss:5.8303 | dt:1628.14ms | tok/s:5,032 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.71%
step: 185 | loss:6.1242 | dt:1624.80ms | tok/s:5,042 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.71%
step: 186 | loss:6.1506 | dt:1629.09ms | tok/s:5,029 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 187 | loss:6.2461 | dt:1641.25ms | tok/s:4,991 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.71%
step: 188 | loss:6.0527 | dt:1627.03ms | tok/s:5,035 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 189 | loss:6.0487 | dt:1634.59ms | tok/s:5,012 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 190 | loss:5.9734 | dt:1639.41ms | tok/s:4,997 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 191 | loss:5.8383 | dt:1644.72ms | tok/s:4,981 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 192 | loss:6.1680 | dt:1650.20ms | tok/s:4,964 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 193 | loss:6.1750 | dt:1640.87ms | tok/s:4,992 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 194 | loss:6.0509 | dt:1636.15ms | tok/s:5,007 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 195 | loss:6.2034 | dt:1643.21ms | tok/s:4,985 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.71%
step: 196 | loss:6.0135 | dt:1627.66ms | tok/s:5,033 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 197 | loss:5.7613 | dt:1638.28ms | tok/s:5,000 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 198 | loss:5.8125 | dt:1642.25ms | tok/s:4,988 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.20%
mfu 0.66%
step: 199 | loss:5.9705 | dt:5841.44ms | tok/s:1,402 | MFU:0.20% | GPU RAM:7.37GB
--------val run-------- train loss 6.0558 | val loss 6.7422 | dt 67202.8057ms
active-> 110777344
MFU: 0.68%
mfu 0.66%
step: 200 | loss:6.1743 | dt:1713.77ms | tok/s:4,780 | MFU:0.68% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.67%
step: 201 | loss:5.7347 | dt:1634.57ms | tok/s:5,012 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.67%
step: 202 | loss:6.0088 | dt:1633.82ms | tok/s:5,014 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.68%
step: 203 | loss:5.6163 | dt:1619.56ms | tok/s:5,058 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.68%
step: 204 | loss:5.7843 | dt:1639.88ms | tok/s:4,995 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.68%
step: 205 | loss:6.2108 | dt:1640.40ms | tok/s:4,994 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.69%
step: 206 | loss:6.0506 | dt:1636.30ms | tok/s:5,006 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.69%
step: 207 | loss:5.8725 | dt:1646.96ms | tok/s:4,974 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.69%
step: 208 | loss:5.8248 | dt:1647.83ms | tok/s:4,971 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.69%
step: 209 | loss:6.2551 | dt:1649.51ms | tok/s:4,966 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.69%
step: 210 | loss:6.0509 | dt:1642.56ms | tok/s:4,987 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 211 | loss:5.9756 | dt:1653.86ms | tok/s:4,953 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 212 | loss:6.1545 | dt:1658.68ms | tok/s:4,939 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 213 | loss:6.0097 | dt:1665.62ms | tok/s:4,918 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 214 | loss:6.5572 | dt:1662.28ms | tok/s:4,928 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 215 | loss:5.9243 | dt:1661.56ms | tok/s:4,930 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 216 | loss:6.0725 | dt:1669.09ms | tok/s:4,908 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 217 | loss:5.7648 | dt:1665.19ms | tok/s:4,920 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 218 | loss:5.8187 | dt:1662.74ms | tok/s:4,927 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 219 | loss:6.0886 | dt:1665.56ms | tok/s:4,918 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 220 | loss:6.0791 | dt:1675.95ms | tok/s:4,888 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 221 | loss:6.0415 | dt:1673.62ms | tok/s:4,895 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 222 | loss:6.3287 | dt:1657.56ms | tok/s:4,942 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 223 | loss:6.2183 | dt:1664.26ms | tok/s:4,922 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.70%
step: 224 | loss:5.9012 | dt:1682.33ms | tok/s:4,869 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 225 | loss:5.6826 | dt:1650.70ms | tok/s:4,963 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 226 | loss:5.8684 | dt:1655.92ms | tok/s:4,947 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 227 | loss:5.9682 | dt:1656.58ms | tok/s:4,945 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 228 | loss:6.2138 | dt:1656.20ms | tok/s:4,946 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 229 | loss:5.5369 | dt:1668.25ms | tok/s:4,911 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 230 | loss:5.8333 | dt:1660.48ms | tok/s:4,934 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 231 | loss:5.7973 | dt:1660.98ms | tok/s:4,932 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 232 | loss:5.9615 | dt:1656.01ms | tok/s:4,947 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 233 | loss:5.7389 | dt:1653.01ms | tok/s:4,956 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 234 | loss:6.0439 | dt:1660.51ms | tok/s:4,933 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 235 | loss:6.2746 | dt:1657.77ms | tok/s:4,942 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 236 | loss:6.1106 | dt:1651.12ms | tok/s:4,961 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 237 | loss:6.0965 | dt:1675.78ms | tok/s:4,888 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 238 | loss:5.8397 | dt:1655.62ms | tok/s:4,948 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 239 | loss:6.1761 | dt:1655.92ms | tok/s:4,947 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 240 | loss:5.7168 | dt:1655.06ms | tok/s:4,950 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 241 | loss:6.3055 | dt:1650.38ms | tok/s:4,964 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 242 | loss:6.4949 | dt:1659.27ms | tok/s:4,937 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 243 | loss:5.9649 | dt:1656.65ms | tok/s:4,945 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 244 | loss:5.8863 | dt:1658.86ms | tok/s:4,938 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 245 | loss:6.0437 | dt:1645.11ms | tok/s:4,980 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 246 | loss:6.2050 | dt:1658.74ms | tok/s:4,939 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 247 | loss:6.1607 | dt:1653.34ms | tok/s:4,955 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 248 | loss:6.1616 | dt:1654.28ms | tok/s:4,952 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.20%
mfu 0.65%
step: 249 | loss:6.4267 | dt:5879.54ms | tok/s:1,393 | MFU:0.20% | GPU RAM:7.37GB
--------val run-------- train loss 6.0027 | val loss 6.6594 | dt 67701.0856ms

ğŸ“Š PROFILER USAGE GUIDE:

1. Traces are saved to ./profiler_logs/
2. View in Chrome: chrome://tracing (load JSON files)
3. View in TensorBoard: 
   tensorboard --logdir=./profiler_logs
   
4. Key metrics to look for:
   - CUDA kernel launch overhead
   - Memory allocation patterns
   - CPU/GPU utilization
   - Communication overhead (DDP/FSDP)
   - Kernel execution time

active-> 110777344
MFU: 0.68%
mfu 0.66%
step: 250 | loss:5.9807 | dt:1706.46ms | tok/s:4,801 | MFU:0.68% | GPU RAM:7.37GB
wandb: 
wandb: Run history:
wandb:                    memory/allocated_gb â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–ˆâ–‡â–‡â–…â–…â–…â–â–„â–‚â–„â–‚â–‚â–â–â–…â–…â–„â–‚â–ƒâ–…â–…â–†â–…â–„â–‡â–‡â–‡â–ˆâ–ˆ
wandb:                memory/max_allocated_gb â–â–â–â–â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                     memory/reserved_gb â–â–‚â–…â–…â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 perf/iteration_time_ms â–„â–â–ƒâ–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–…â–…â–ˆâ–†â–†â–†â–†â–†â–†â–‡â–†â–†â–†â–†â–‡â–‡â–†
wandb:                       perf/mfu_percent â–ˆâ–ˆâ–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–
wandb:         perf/throughput_tokens_per_sec â–ˆâ–ˆâ–‡â–‡â–‡â–…â–…â–„â–„â–â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒâ–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ
wandb: perf/throughput_tokens_per_sec_per_gpu â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–â–‡â–‡â–‡â–‡â–‡â–â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–
wandb:                        train/grad_norm â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–„â–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–‚
wandb:                             train/loss â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–â–‚â–â–â–‚â–‚â–â–‚
wandb:                               train/lr â–â–‚â–‚â–ƒâ–ƒâ–„â–„â–„â–…â–…â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–†â–†â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚
wandb:                                     +1 ...
wandb: 
wandb: Run summary:
wandb:                    memory/allocated_gb 3.37989
wandb:                memory/max_allocated_gb 6.83824
wandb:                     memory/reserved_gb 7.37109
wandb:                 perf/iteration_time_ms 1706.46315
wandb:                       perf/mfu_percent 0.68331
wandb:         perf/throughput_tokens_per_sec 4800.57246
wandb: perf/throughput_tokens_per_sec_per_gpu 2400.28623
wandb:                        train/grad_norm 0.36644
wandb:                             train/loss 5.98072
wandb:                               train/lr 3e-05
wandb:                                     +1 ...
wandb: 
wandb: ğŸš€ View run shakespeare_gqa_20260115_075350 at: https://wandb.ai/adeeb-idris-coep-technological-university/llm-training/runs/vz6m5yga
wandb: â­ï¸ View project at: https://wandb.ai/adeeb-idris-coep-technological-university/llm-training
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260115_075350-vz6m5yga/logs
WandB run completed

ğŸ“Š PROFILER USAGE GUIDE:

1. Traces are saved to ./profiler_logs/
2. View in Chrome: chrome://tracing (load JSON files)
3. View in TensorBoard: 
   tensorboard --logdir=./profiler_logs
   
4. Key metrics to look for:
   - CUDA kernel launch overhead
   - Memory allocation patterns
   - CPU/GPU utilization
   - Communication overhead (DDP/FSDP)
   - Kernel execution time

[rank0]:[W115 08:07:27.465089401 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

Logs saved to: run_1_console_20260115_075318.log and run_1_error_20260115_075318.log

Copying output files for run 1...
  Scanning: /kaggle/working
  Scanning: .
  Scanning: ./run_1_logs
âœ“ Copied 0 files to ./Transformer_GPU/monitor_logs/run_1_20260115_080731

================================================================================
Committing and pushing to GitHub for run 1...
Configuring git identity...
âœ“ Files added to git
âœ“ Committed: Add monitor logs for run 1 - 0 files - 2026-01-15 08:07:32
âœ“ Pushed to GitHub successfully!

âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“
RUN 1 COMPLETED SUCCESSFULLY
Files saved: 0
Pushed to GitHub: âœ“
âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“

================================================================================
WAITING 10 SECONDS BEFORE NEXT RUN...
================================================================================

################################################################################
STARTING RUN 2/8
Time: 08:07:42
################################################################################

train.py updated with parallel_flag = 2
Running torchrun command for i=2...
W0115 08:07:44.361000 440 torch/distributed/run.py:774] 
W0115 08:07:44.361000 440 torch/distributed/run.py:774] *****************************************
W0115 08:07:44.361000 440 torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0115 08:07:44.361000 440 torch/distributed/run.py:774] *****************************************
[W115 08:07:44.412761558 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W115 08:07:44.413422523 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
parallel_flag :  2
parallel_flag :  2
[W115 08:07:48.430850513 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W115 08:07:48.431399890 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W115 08:07:48.431524380 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W115 08:07:48.432031891 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
DDP_WORLD_SIZE = 2
=============
parallel_flag -  2
parallel_flag -  2
parallel_flag -  2
=============
=============
parallel_flag -  2
parallel_flag -  2
parallel_flag -  2
=============
total parameters = 186,274,816, active parameters = 110,777,344
Using compiled model
[rank1]:W0115 08:07:55.124000 448 torch/_logging/_internal.py:1154] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
wandb: Currently logged in as: adeeb-idris (adeeb-idris-coep-technological-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /kaggle/working/Transformer_GPU/project/wandb/run-20260115_080755-p0o6we15
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run shakespeare_gqa_20260115_080755
wandb: â­ï¸ View project at https://wandb.ai/adeeb-idris-coep-technological-university/llm-training
wandb: ğŸš€ View run at https://wandb.ai/adeeb-idris-coep-technological-university/llm-training/runs/p0o6we15
WandB initialized: project=llm-training, run=shakespeare_gqa_20260115_080755
[rank0]:W0115 08:07:57.141000 447 torch/_logging/_internal.py:1154] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank1]:W0115 08:08:00.758000 448 torch/_inductor/utils.py:1436] [7/0_1] Not enough SMs to use max_autotune_gemm mode
[rank0]:W0115 08:08:02.125000 447 torch/_inductor/utils.py:1436] [7/0_1] Not enough SMs to use max_autotune_gemm mode
active-> 110777344
MFU: 0.09%
mfu 0.09%
step: 0 | loss:10.9839 | dt:13372.60ms | tok/s:613 | MFU:0.09% | GPU RAM:6.14GB
active-> 110777344
MFU: 0.72%
mfu 0.15%
step: 1 | loss:11.0666 | dt:1626.31ms | tok/s:5,037 | MFU:0.72% | GPU RAM:7.35GB
active-> 110777344
MFU: 0.71%
mfu 0.21%
step: 2 | loss:10.9682 | dt:1639.85ms | tok/s:4,996 | MFU:0.71% | GPU RAM:7.35GB
active-> 110777344
MFU: 0.71%
mfu 0.26%
step: 3 | loss:11.1122 | dt:1644.22ms | tok/s:4,982 | MFU:0.71% | GPU RAM:7.35GB
active-> 110777344
MFU: 0.71%
mfu 0.30%
step: 4 | loss:11.0980 | dt:1645.97ms | tok/s:4,977 | MFU:0.71% | GPU RAM:7.35GB
active-> 110777344
MFU: 0.70%
mfu 0.34%
step: 5 | loss:11.0153 | dt:1655.57ms | tok/s:4,948 | MFU:0.70% | GPU RAM:7.35GB
active-> 110777344
MFU: 0.70%
mfu 0.38%
step: 6 | loss:11.1145 | dt:1660.45ms | tok/s:4,934 | MFU:0.70% | GPU RAM:7.35GB
active-> 110777344
MFU: 0.71%
mfu 0.41%
step: 7 | loss:11.0012 | dt:1632.20ms | tok/s:5,019 | MFU:0.71% | GPU RAM:7.36GB
active-> 110777344
MFU: 0.69%
mfu 0.44%
step: 8 | loss:11.0494 | dt:1681.11ms | tok/s:4,873 | MFU:0.69% | GPU RAM:7.36GB
active-> 110777344
MFU: 0.69%
mfu 0.46%
step: 9 | loss:11.0502 | dt:1687.05ms | tok/s:4,856 | MFU:0.69% | GPU RAM:7.36GB
active-> 110777344
MFU: 0.68%
mfu 0.49%
step: 10 | loss:11.0286 | dt:1710.07ms | tok/s:4,790 | MFU:0.68% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.68%
mfu 0.51%
step: 11 | loss:10.9935 | dt:1717.22ms | tok/s:4,770 | MFU:0.68% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.67%
mfu 0.52%
step: 12 | loss:11.0236 | dt:1737.78ms | tok/s:4,714 | MFU:0.67% | GPU RAM:7.37GB
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                               aten::mm         2.03%     108.954ms         3.62%     194.871ms      77.576us        2.127s        58.08%        2.127s     846.611us          2512   5081986.499  
       autograd::engine::evaluate_function: MmBackward0         0.24%      13.021ms         3.46%     186.094ms     247.466us       0.000us         0.00%        1.383s       1.839ms           752            --  
                                            MmBackward0         0.32%      16.971ms         3.22%     173.073ms     230.151us       0.000us         0.00%        1.383s       1.839ms           752            --  
                                           aten::linear         0.23%      12.251ms         3.96%     212.827ms     116.681us       0.000us         0.00%        1.354s     742.294us          1824            --  
                        DistributedDataParallel.forward         0.00%       0.000us         0.00%       0.000us       0.000us        1.184s        32.35%        1.184s     296.093ms             4            --  
                                          ProfilerStep*         0.52%      28.156ms        65.75%        3.538s        1.769s       0.000us         0.00%        1.173s     586.388ms             2            --  
                                           forward_pass         0.02%       1.115ms        22.37%        1.204s     300.944ms       0.000us         0.00%        1.029s     257.260ms             4            --  
                             Torch-Compiled Region: 0/0         0.01%     750.699us        22.35%        1.202s     300.595ms       0.000us         0.00%        1.029s     257.257ms             4            --  
                        DistributedDataParallel.forward         0.09%       4.957ms        22.33%        1.202s     300.407ms       0.000us         0.00%        1.029s     257.257ms             4            --  
                             Torch-Compiled Region: 4/0         0.00%      46.316us        22.12%        1.190s     297.555ms       0.000us         0.00%        1.021s     255.163ms             4            --  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 5.380s
Self CUDA time total: 3.661s

active-> 110777344
MFU: 0.05%
mfu 0.47%
step: 13 | loss:10.9566 | dt:25278.89ms | tok/s:324 | MFU:0.05% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.66%
mfu 0.49%
step: 14 | loss:10.9520 | dt:1756.39ms | tok/s:4,664 | MFU:0.66% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.68%
mfu 0.51%
step: 15 | loss:10.9506 | dt:1702.47ms | tok/s:4,812 | MFU:0.68% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.68%
mfu 0.53%
step: 16 | loss:11.0116 | dt:1708.36ms | tok/s:4,795 | MFU:0.68% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.68%
mfu 0.54%
step: 17 | loss:10.9148 | dt:1718.65ms | tok/s:4,767 | MFU:0.68% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.68%
mfu 0.56%
step: 18 | loss:10.9888 | dt:1722.45ms | tok/s:4,756 | MFU:0.68% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.68%
mfu 0.57%
step: 19 | loss:11.0425 | dt:1713.10ms | tok/s:4,782 | MFU:0.68% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.68%
mfu 0.58%
step: 20 | loss:10.9024 | dt:1713.02ms | tok/s:4,782 | MFU:0.68% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.68%
mfu 0.59%
step: 21 | loss:10.8622 | dt:1715.57ms | tok/s:4,775 | MFU:0.68% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.68%
mfu 0.60%
step: 22 | loss:10.9393 | dt:1712.44ms | tok/s:4,784 | MFU:0.68% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.61%
step: 23 | loss:10.8511 | dt:1700.46ms | tok/s:4,818 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.62%
step: 24 | loss:10.9447 | dt:1688.02ms | tok/s:4,853 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.62%
step: 25 | loss:10.9635 | dt:1697.76ms | tok/s:4,825 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.63%
step: 26 | loss:10.8812 | dt:1694.22ms | tok/s:4,835 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.64%
step: 27 | loss:10.7737 | dt:1695.45ms | tok/s:4,832 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.64%
step: 28 | loss:10.8028 | dt:1697.46ms | tok/s:4,826 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.65%
step: 29 | loss:10.7564 | dt:1689.61ms | tok/s:4,848 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.65%
step: 30 | loss:10.9134 | dt:1684.61ms | tok/s:4,863 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.66%
step: 31 | loss:10.8430 | dt:1678.45ms | tok/s:4,881 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.66%
step: 32 | loss:10.7698 | dt:1683.27ms | tok/s:4,867 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.66%
step: 33 | loss:10.7970 | dt:1673.90ms | tok/s:4,894 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.67%
step: 34 | loss:10.7591 | dt:1676.31ms | tok/s:4,887 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.67%
step: 35 | loss:10.5613 | dt:1673.45ms | tok/s:4,895 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.67%
step: 36 | loss:10.6321 | dt:1665.61ms | tok/s:4,918 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.67%
step: 37 | loss:10.7513 | dt:1677.40ms | tok/s:4,884 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.68%
step: 38 | loss:10.6103 | dt:1668.15ms | tok/s:4,911 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.68%
step: 39 | loss:10.6334 | dt:1670.24ms | tok/s:4,905 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.68%
step: 40 | loss:10.4404 | dt:1676.26ms | tok/s:4,887 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.68%
step: 41 | loss:10.5239 | dt:1674.01ms | tok/s:4,894 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.68%
step: 42 | loss:10.5276 | dt:1675.59ms | tok/s:4,889 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.68%
step: 43 | loss:10.6021 | dt:1672.30ms | tok/s:4,899 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 44 | loss:10.4042 | dt:1678.81ms | tok/s:4,880 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 45 | loss:10.3746 | dt:1674.62ms | tok/s:4,892 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 46 | loss:10.4890 | dt:1667.72ms | tok/s:4,912 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 47 | loss:10.3911 | dt:1658.76ms | tok/s:4,939 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 48 | loss:10.3861 | dt:1674.92ms | tok/s:4,891 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.19%
mfu 0.64%
step: 49 | loss:10.3106 | dt:6117.35ms | tok/s:1,339 | MFU:0.19% | GPU RAM:7.37GB
--------val run-------- train loss 10.3025 | val loss 10.3083 | dt 70171.6523ms
active-> 110777344
MFU: 0.65%
mfu 0.64%
step: 50 | loss:10.2341 | dt:1805.51ms | tok/s:4,537 | MFU:0.65% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.65%
step: 51 | loss:10.3210 | dt:1660.34ms | tok/s:4,934 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.65%
step: 52 | loss:10.2520 | dt:1655.40ms | tok/s:4,949 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.66%
step: 53 | loss:10.2179 | dt:1667.44ms | tok/s:4,913 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.66%
step: 54 | loss:10.0204 | dt:1665.86ms | tok/s:4,918 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.67%
step: 55 | loss:10.1588 | dt:1675.31ms | tok/s:4,890 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.67%
step: 56 | loss:10.0181 | dt:1676.59ms | tok/s:4,886 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.67%
step: 57 | loss:10.1149 | dt:1663.23ms | tok/s:4,925 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.67%
step: 58 | loss:9.9981 | dt:1671.78ms | tok/s:4,900 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.68%
step: 59 | loss:9.8916 | dt:1673.46ms | tok/s:4,895 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 60 | loss:10.0087 | dt:1688.21ms | tok/s:4,852 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 61 | loss:9.9135 | dt:1679.08ms | tok/s:4,879 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 62 | loss:9.8876 | dt:1689.07ms | tok/s:4,850 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 63 | loss:9.8362 | dt:1684.39ms | tok/s:4,863 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 64 | loss:9.7045 | dt:1680.98ms | tok/s:4,873 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 65 | loss:9.8790 | dt:1693.17ms | tok/s:4,838 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 66 | loss:9.8374 | dt:1689.89ms | tok/s:4,848 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 67 | loss:9.6535 | dt:1698.00ms | tok/s:4,824 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 68 | loss:9.6840 | dt:1692.13ms | tok/s:4,841 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 69 | loss:9.3974 | dt:1696.47ms | tok/s:4,829 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 70 | loss:9.4047 | dt:1694.19ms | tok/s:4,835 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 71 | loss:9.2793 | dt:1686.94ms | tok/s:4,856 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.68%
mfu 0.69%
step: 72 | loss:9.2605 | dt:1706.70ms | tok/s:4,800 | MFU:0.68% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 73 | loss:9.2655 | dt:1695.32ms | tok/s:4,832 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 74 | loss:9.1542 | dt:1688.14ms | tok/s:4,853 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 75 | loss:9.0681 | dt:1694.54ms | tok/s:4,834 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 76 | loss:9.1543 | dt:1695.59ms | tok/s:4,831 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 77 | loss:9.0039 | dt:1685.05ms | tok/s:4,862 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 78 | loss:8.8625 | dt:1686.18ms | tok/s:4,858 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 79 | loss:9.1012 | dt:1685.48ms | tok/s:4,860 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 80 | loss:8.9350 | dt:1682.69ms | tok/s:4,868 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 81 | loss:8.6074 | dt:1683.81ms | tok/s:4,865 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 82 | loss:8.6172 | dt:1675.93ms | tok/s:4,888 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 83 | loss:8.6237 | dt:1660.17ms | tok/s:4,934 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 84 | loss:8.7526 | dt:1675.83ms | tok/s:4,888 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 85 | loss:8.8921 | dt:1673.42ms | tok/s:4,895 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 86 | loss:8.3963 | dt:1668.50ms | tok/s:4,910 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 87 | loss:8.5381 | dt:1679.54ms | tok/s:4,878 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 88 | loss:8.7395 | dt:1669.11ms | tok/s:4,908 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 89 | loss:8.1298 | dt:1675.84ms | tok/s:4,888 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 90 | loss:8.2180 | dt:1674.67ms | tok/s:4,892 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 91 | loss:8.0346 | dt:1655.74ms | tok/s:4,948 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 92 | loss:8.1492 | dt:1645.77ms | tok/s:4,978 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 93 | loss:7.9365 | dt:1651.63ms | tok/s:4,960 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 94 | loss:7.6996 | dt:1661.34ms | tok/s:4,931 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 95 | loss:8.1164 | dt:1652.40ms | tok/s:4,958 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 96 | loss:7.9752 | dt:1644.02ms | tok/s:4,983 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.64%
mfu 0.69%
step: 97 | loss:7.7553 | dt:1809.10ms | tok/s:4,528 | MFU:0.64% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 98 | loss:7.3923 | dt:1657.20ms | tok/s:4,943 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.20%
mfu 0.65%
step: 99 | loss:7.4357 | dt:5844.29ms | tok/s:1,402 | MFU:0.20% | GPU RAM:7.37GB
--------val run-------- train loss 7.5953 | val loss 8.0079 | dt 66705.4550ms
active-> 110777344
MFU: 0.67%
mfu 0.65%
step: 100 | loss:7.3381 | dt:1730.04ms | tok/s:4,735 | MFU:0.67% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.65%
step: 101 | loss:7.6335 | dt:1669.41ms | tok/s:4,907 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.66%
step: 102 | loss:7.3635 | dt:1658.74ms | tok/s:4,939 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.66%
step: 103 | loss:7.4330 | dt:1650.76ms | tok/s:4,963 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.67%
step: 104 | loss:7.3693 | dt:1681.52ms | tok/s:4,872 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.67%
step: 105 | loss:7.1968 | dt:1637.00ms | tok/s:5,004 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.67%
step: 106 | loss:7.4845 | dt:1659.62ms | tok/s:4,936 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.68%
step: 107 | loss:6.9968 | dt:1657.87ms | tok/s:4,941 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.68%
step: 108 | loss:7.2028 | dt:1650.34ms | tok/s:4,964 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.68%
step: 109 | loss:6.8606 | dt:1656.32ms | tok/s:4,946 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.68%
step: 110 | loss:7.1346 | dt:1666.28ms | tok/s:4,916 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 111 | loss:6.8136 | dt:1664.70ms | tok/s:4,921 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 112 | loss:6.5790 | dt:1656.07ms | tok/s:4,947 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 113 | loss:7.3862 | dt:1668.73ms | tok/s:4,909 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.69%
step: 114 | loss:7.5691 | dt:1653.23ms | tok/s:4,955 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 115 | loss:6.8289 | dt:1658.88ms | tok/s:4,938 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 116 | loss:6.9423 | dt:1665.76ms | tok/s:4,918 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 117 | loss:6.6628 | dt:1662.82ms | tok/s:4,927 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 118 | loss:7.0638 | dt:1665.99ms | tok/s:4,917 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 119 | loss:7.1349 | dt:1646.67ms | tok/s:4,975 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 120 | loss:6.6991 | dt:1646.80ms | tok/s:4,975 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 121 | loss:6.9250 | dt:1635.89ms | tok/s:5,008 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 122 | loss:6.8214 | dt:1653.25ms | tok/s:4,955 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 123 | loss:6.6928 | dt:1657.44ms | tok/s:4,943 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 124 | loss:6.5622 | dt:1643.40ms | tok/s:4,985 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 125 | loss:6.2435 | dt:1654.07ms | tok/s:4,953 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 126 | loss:6.6417 | dt:1633.49ms | tok/s:5,015 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 127 | loss:6.7613 | dt:1645.08ms | tok/s:4,980 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 128 | loss:6.8685 | dt:1645.98ms | tok/s:4,977 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 129 | loss:6.7064 | dt:1652.08ms | tok/s:4,959 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 130 | loss:6.5312 | dt:1643.65ms | tok/s:4,984 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 131 | loss:6.5207 | dt:1647.58ms | tok/s:4,972 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 132 | loss:6.6471 | dt:1648.13ms | tok/s:4,970 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 133 | loss:6.4333 | dt:1641.50ms | tok/s:4,991 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.71%
step: 134 | loss:6.3021 | dt:1661.46ms | tok/s:4,931 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 135 | loss:6.5864 | dt:1657.65ms | tok/s:4,942 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 136 | loss:6.5423 | dt:1661.99ms | tok/s:4,929 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 137 | loss:6.5610 | dt:1645.92ms | tok/s:4,977 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 138 | loss:6.7090 | dt:1652.20ms | tok/s:4,958 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 139 | loss:6.3950 | dt:1660.29ms | tok/s:4,934 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 140 | loss:7.0270 | dt:1645.81ms | tok/s:4,977 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 141 | loss:6.4456 | dt:1666.33ms | tok/s:4,916 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 142 | loss:6.7796 | dt:1672.04ms | tok/s:4,899 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 143 | loss:6.6984 | dt:1646.37ms | tok/s:4,976 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 144 | loss:6.4185 | dt:1651.37ms | tok/s:4,961 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 145 | loss:6.1937 | dt:1671.14ms | tok/s:4,902 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 146 | loss:6.5400 | dt:1662.71ms | tok/s:4,927 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 147 | loss:6.0869 | dt:1643.48ms | tok/s:4,985 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 148 | loss:6.2978 | dt:1647.68ms | tok/s:4,972 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.20%
mfu 0.65%
step: 149 | loss:6.7625 | dt:5778.30ms | tok/s:1,418 | MFU:0.20% | GPU RAM:7.37GB
--------val run-------- train loss 6.3077 | val loss 7.0457 | dt 66577.3499ms
active-> 110777344
MFU: 0.67%
mfu 0.66%
step: 150 | loss:6.2518 | dt:1730.05ms | tok/s:4,735 | MFU:0.67% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.66%
step: 151 | loss:6.6295 | dt:1628.41ms | tok/s:5,031 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.67%
step: 152 | loss:6.0766 | dt:1629.17ms | tok/s:5,028 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.67%
step: 153 | loss:6.0791 | dt:1625.85ms | tok/s:5,039 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.68%
step: 154 | loss:6.3142 | dt:1631.29ms | tok/s:5,022 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.68%
step: 155 | loss:6.2747 | dt:1620.75ms | tok/s:5,054 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.68%
step: 156 | loss:6.4781 | dt:1635.47ms | tok/s:5,009 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.69%
step: 157 | loss:6.4876 | dt:1635.29ms | tok/s:5,009 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.69%
step: 158 | loss:6.2835 | dt:1637.51ms | tok/s:5,003 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.69%
step: 159 | loss:6.1710 | dt:1634.96ms | tok/s:5,011 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.69%
step: 160 | loss:6.3796 | dt:1633.70ms | tok/s:5,014 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 161 | loss:6.5150 | dt:1650.99ms | tok/s:4,962 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 162 | loss:6.0658 | dt:1657.89ms | tok/s:4,941 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 163 | loss:6.3675 | dt:1658.80ms | tok/s:4,939 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 164 | loss:5.8663 | dt:1655.32ms | tok/s:4,949 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 165 | loss:6.2358 | dt:1644.88ms | tok/s:4,980 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 166 | loss:6.1244 | dt:1667.65ms | tok/s:4,912 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 167 | loss:6.1239 | dt:1663.91ms | tok/s:4,923 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 168 | loss:6.1080 | dt:1667.37ms | tok/s:4,913 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 169 | loss:6.0634 | dt:1649.11ms | tok/s:4,968 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 170 | loss:5.9486 | dt:1662.13ms | tok/s:4,929 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 171 | loss:5.6327 | dt:1674.35ms | tok/s:4,893 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 172 | loss:6.1243 | dt:1652.59ms | tok/s:4,957 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 173 | loss:6.1260 | dt:1650.01ms | tok/s:4,965 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.70%
step: 174 | loss:6.1637 | dt:1681.25ms | tok/s:4,873 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.70%
step: 175 | loss:5.9166 | dt:1683.56ms | tok/s:4,866 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 176 | loss:6.2905 | dt:1646.87ms | tok/s:4,974 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 177 | loss:5.9611 | dt:1662.06ms | tok/s:4,929 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 178 | loss:6.1476 | dt:1666.35ms | tok/s:4,916 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 179 | loss:5.8575 | dt:1670.16ms | tok/s:4,905 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 180 | loss:6.4448 | dt:1667.40ms | tok/s:4,913 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 181 | loss:6.1752 | dt:1670.04ms | tok/s:4,905 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 182 | loss:6.1535 | dt:1647.49ms | tok/s:4,972 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 183 | loss:6.1827 | dt:1657.86ms | tok/s:4,941 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 184 | loss:5.8304 | dt:1631.93ms | tok/s:5,020 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 185 | loss:6.1243 | dt:1652.41ms | tok/s:4,958 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 186 | loss:6.1507 | dt:1653.35ms | tok/s:4,955 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 187 | loss:6.2459 | dt:1655.22ms | tok/s:4,949 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 188 | loss:6.0526 | dt:1661.26ms | tok/s:4,931 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 189 | loss:6.0489 | dt:1660.51ms | tok/s:4,933 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 190 | loss:5.9732 | dt:1657.31ms | tok/s:4,943 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 191 | loss:5.8384 | dt:1649.33ms | tok/s:4,967 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 192 | loss:6.1690 | dt:1647.92ms | tok/s:4,971 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 193 | loss:6.1749 | dt:1640.03ms | tok/s:4,995 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 194 | loss:6.0505 | dt:1637.25ms | tok/s:5,004 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 195 | loss:6.2034 | dt:1659.63ms | tok/s:4,936 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 196 | loss:6.0133 | dt:1670.08ms | tok/s:4,905 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 197 | loss:5.7618 | dt:1666.10ms | tok/s:4,917 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 198 | loss:5.8128 | dt:1662.19ms | tok/s:4,928 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.20%
mfu 0.65%
step: 199 | loss:5.9698 | dt:5838.93ms | tok/s:1,403 | MFU:0.20% | GPU RAM:7.37GB
--------val run-------- train loss 6.0557 | val loss 6.7927 | dt 67123.0905ms
active-> 110777344
MFU: 0.66%
mfu 0.65%
step: 200 | loss:6.1744 | dt:1756.35ms | tok/s:4,664 | MFU:0.66% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.66%
step: 201 | loss:5.7349 | dt:1654.36ms | tok/s:4,952 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.66%
step: 202 | loss:6.0090 | dt:1651.81ms | tok/s:4,959 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.67%
step: 203 | loss:5.6156 | dt:1655.58ms | tok/s:4,948 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.67%
step: 204 | loss:5.7848 | dt:1664.27ms | tok/s:4,922 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.67%
step: 205 | loss:6.2111 | dt:1683.34ms | tok/s:4,867 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 206 | loss:6.0509 | dt:1685.60ms | tok/s:4,860 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.68%
step: 207 | loss:5.8724 | dt:1671.89ms | tok/s:4,900 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.68%
step: 208 | loss:5.8244 | dt:1676.86ms | tok/s:4,885 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 209 | loss:6.2550 | dt:1679.44ms | tok/s:4,878 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.68%
step: 210 | loss:6.0510 | dt:1676.86ms | tok/s:4,885 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 211 | loss:5.9752 | dt:1696.57ms | tok/s:4,829 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 212 | loss:6.1546 | dt:1693.51ms | tok/s:4,837 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 213 | loss:6.0095 | dt:1694.40ms | tok/s:4,835 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.68%
mfu 0.68%
step: 214 | loss:6.5572 | dt:1705.91ms | tok/s:4,802 | MFU:0.68% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.68%
mfu 0.68%
step: 215 | loss:5.9244 | dt:1711.84ms | tok/s:4,785 | MFU:0.68% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.68%
mfu 0.68%
step: 216 | loss:6.0726 | dt:1721.89ms | tok/s:4,758 | MFU:0.68% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.68%
mfu 0.68%
step: 217 | loss:5.7650 | dt:1713.59ms | tok/s:4,781 | MFU:0.68% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 218 | loss:5.8185 | dt:1696.12ms | tok/s:4,830 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 219 | loss:6.0888 | dt:1694.66ms | tok/s:4,834 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 220 | loss:6.0794 | dt:1692.39ms | tok/s:4,840 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 221 | loss:6.0415 | dt:1696.65ms | tok/s:4,828 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 222 | loss:6.3289 | dt:1690.56ms | tok/s:4,846 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 223 | loss:6.2182 | dt:1696.04ms | tok/s:4,830 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 224 | loss:5.9010 | dt:1692.04ms | tok/s:4,841 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 225 | loss:5.6827 | dt:1700.42ms | tok/s:4,818 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.68%
mfu 0.68%
step: 226 | loss:5.8689 | dt:1718.71ms | tok/s:4,766 | MFU:0.68% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 227 | loss:5.9684 | dt:1680.06ms | tok/s:4,876 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 228 | loss:6.2140 | dt:1689.00ms | tok/s:4,850 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 229 | loss:5.5373 | dt:1689.70ms | tok/s:4,848 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 230 | loss:5.8344 | dt:1679.06ms | tok/s:4,879 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 231 | loss:5.7979 | dt:1696.26ms | tok/s:4,829 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 232 | loss:5.9620 | dt:1689.54ms | tok/s:4,849 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 233 | loss:5.7392 | dt:1686.38ms | tok/s:4,858 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 234 | loss:6.0441 | dt:1689.21ms | tok/s:4,850 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 235 | loss:6.2726 | dt:1679.03ms | tok/s:4,879 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 236 | loss:6.1106 | dt:1686.59ms | tok/s:4,857 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 237 | loss:6.0965 | dt:1682.02ms | tok/s:4,870 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 238 | loss:5.8395 | dt:1682.80ms | tok/s:4,868 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 239 | loss:6.1759 | dt:1678.17ms | tok/s:4,882 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 240 | loss:5.7170 | dt:1674.36ms | tok/s:4,893 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 241 | loss:6.3055 | dt:1682.34ms | tok/s:4,869 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 242 | loss:6.4949 | dt:1678.40ms | tok/s:4,881 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 243 | loss:5.9644 | dt:1679.81ms | tok/s:4,877 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 244 | loss:5.8864 | dt:1680.61ms | tok/s:4,874 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 245 | loss:6.0429 | dt:1684.32ms | tok/s:4,864 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 246 | loss:6.2053 | dt:1670.58ms | tok/s:4,904 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 247 | loss:6.1608 | dt:1684.43ms | tok/s:4,863 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 248 | loss:6.1614 | dt:1676.43ms | tok/s:4,887 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.19%
mfu 0.64%
step: 249 | loss:6.4266 | dt:6107.20ms | tok/s:1,341 | MFU:0.19% | GPU RAM:7.37GB
--------val run-------- train loss 6.0026 | val loss 6.8116 | dt 68163.4808ms

ğŸ“Š PROFILER USAGE GUIDE:

1. Traces are saved to ./profiler_logs/
2. View in Chrome: chrome://tracing (load JSON files)
3. View in TensorBoard: 
   tensorboard --logdir=./profiler_logs
   
4. Key metrics to look for:
   - CUDA kernel launch overhead
   - Memory allocation patterns
   - CPU/GPU utilization
   - Communication overhead (DDP/FSDP)
   - Kernel execution time

active-> 110777344
MFU: 0.67%
mfu 0.65%
step: 250 | loss:5.9805 | dt:1733.10ms | tok/s:4,727 | MFU:0.67% | GPU RAM:7.37GB
wandb: 
wandb: Run history:
wandb:                    memory/allocated_gb â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–‡â–†â–‚â–â–‚â–„â–‚â–ƒâ–†â–ƒâ–‚â–â–‚â–ƒâ–ƒâ–†â–„â–†â–‡â–†â–…â–‡â–‡â–‡â–‡â–ˆ
wandb:                memory/max_allocated_gb â–â–‚â–‚â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                     memory/reserved_gb â–â–…â–…â–…â–…â–…â–…â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆ
wandb:                 perf/iteration_time_ms â–‚â–„â–‡â–†â–‡â–†â–…â–„â–„â–ƒâ–…â–…â–…â–„â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–„â–‚â–ˆâ–â–â–ƒâ–‚â–‚â–ƒâ–„â–‚â–„â–†â–…â–†â–…â–„â–„
wandb:                       perf/mfu_percent â–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–
wandb:         perf/throughput_tokens_per_sec â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ƒâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: perf/throughput_tokens_per_sec_per_gpu â–ƒâ–ƒâ–ƒâ–„â–…â–†â–…â–…â–„â–†â–…â–†â–…â–†â–ˆâ–†â–…â–‡â–…â–†â–†â–ˆâ–ˆâ–‡â–†â–…â–…â–„â–‚â–„â–„â–„â–ƒâ–„â–„â–…â–…â–„â–…â–
wandb:                        train/grad_norm â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–…â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–‚
wandb:                             train/loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–…â–…â–…â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–‚â–‚â–â–
wandb:                               train/lr â–â–â–‚â–ƒâ–ƒâ–„â–„â–…â–…â–†â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–†â–†â–†â–…â–…â–…â–„â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–
wandb:                                     +1 ...
wandb: 
wandb: Run summary:
wandb:                    memory/allocated_gb 3.38572
wandb:                memory/max_allocated_gb 6.83772
wandb:                     memory/reserved_gb 7.37305
wandb:                 perf/iteration_time_ms 1733.09937
wandb:                       perf/mfu_percent 0.67281
wandb:         perf/throughput_tokens_per_sec 4726.79187
wandb: perf/throughput_tokens_per_sec_per_gpu 2363.39593
wandb:                        train/grad_norm 0.36642
wandb:                             train/loss 5.98055
wandb:                               train/lr 3e-05
wandb:                                     +1 ...
wandb: 
wandb: ğŸš€ View run shakespeare_gqa_20260115_080755 at: https://wandb.ai/adeeb-idris-coep-technological-university/llm-training/runs/p0o6we15
wandb: â­ï¸ View project at: https://wandb.ai/adeeb-idris-coep-technological-university/llm-training
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260115_080755-p0o6we15/logs
WandB run completed

ğŸ“Š PROFILER USAGE GUIDE:

1. Traces are saved to ./profiler_logs/
2. View in Chrome: chrome://tracing (load JSON files)
3. View in TensorBoard: 
   tensorboard --logdir=./profiler_logs
   
4. Key metrics to look for:
   - CUDA kernel launch overhead
   - Memory allocation patterns
   - CPU/GPU utilization
   - Communication overhead (DDP/FSDP)
   - Kernel execution time

[rank0]:[W115 08:21:42.498116064 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

Logs saved to: run_2_console_20260115_080742.log and run_2_error_20260115_080742.log

Copying output files for run 2...
  Scanning: /kaggle/working
  Scanning: .
  Scanning: ./run_2_logs
âœ“ Copied 0 files to ./Transformer_GPU/monitor_logs/run_2_20260115_082147

================================================================================
Committing and pushing to GitHub for run 2...
Configuring git identity...
âœ“ Files added to git
âœ“ Committed: Add monitor logs for run 2 - 0 files - 2026-01-15 08:21:47
âœ“ Pushed to GitHub successfully!

âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“
RUN 2 COMPLETED SUCCESSFULLY
Files saved: 0
Pushed to GitHub: âœ“
âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“

================================================================================
WAITING 10 SECONDS BEFORE NEXT RUN...
================================================================================

################################################################################
STARTING RUN 3/8
Time: 08:21:57
################################################################################

train.py updated with parallel_flag = 3
Running torchrun command for i=3...
W0115 08:21:59.649000 571 torch/distributed/run.py:774] 
W0115 08:21:59.649000 571 torch/distributed/run.py:774] *****************************************
W0115 08:21:59.649000 571 torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0115 08:21:59.649000 571 torch/distributed/run.py:774] *****************************************
[W115 08:21:59.647143014 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W115 08:21:59.647818931 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
parallel_flag :  3
parallel_flag :  3
[W115 08:22:04.078706325 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W115 08:22:04.079562766 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
DDP_WORLD_SIZE = 2
=============
parallel_flag -  3
parallel_flag -  3
parallel_flag -  3
=============
[W115 08:22:04.085621655 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W115 08:22:04.086254662 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
=============
parallel_flag -  3
parallel_flag -  3
parallel_flag -  3
=============
total parameters = 186,274,816, active parameters = 110,777,344
Using compiled model
[rank1]:W0115 08:22:10.947000 579 torch/_logging/_internal.py:1154] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
wandb: Currently logged in as: adeeb-idris (adeeb-idris-coep-technological-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /kaggle/working/Transformer_GPU/project/wandb/run-20260115_082211-wnr4043o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run shakespeare_gqa_20260115_082210
wandb: â­ï¸ View project at https://wandb.ai/adeeb-idris-coep-technological-university/llm-training
wandb: ğŸš€ View run at https://wandb.ai/adeeb-idris-coep-technological-university/llm-training/runs/wnr4043o
WandB initialized: project=llm-training, run=shakespeare_gqa_20260115_082210
[rank0]:W0115 08:22:12.782000 578 torch/_logging/_internal.py:1154] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank1]:W0115 08:22:16.529000 579 torch/_inductor/utils.py:1436] [7/0_1] Not enough SMs to use max_autotune_gemm mode
[rank0]:W0115 08:22:17.626000 578 torch/_inductor/utils.py:1436] [7/0_1] Not enough SMs to use max_autotune_gemm mode
active-> 110777344
MFU: 0.09%
mfu 0.09%
step: 0 | loss:10.9839 | dt:13520.87ms | tok/s:606 | MFU:0.09% | GPU RAM:6.14GB
active-> 110777344
MFU: 0.72%
mfu 0.15%
step: 1 | loss:11.0666 | dt:1625.27ms | tok/s:5,040 | MFU:0.72% | GPU RAM:7.35GB
active-> 110777344
MFU: 0.71%
mfu 0.21%
step: 2 | loss:10.9682 | dt:1645.79ms | tok/s:4,978 | MFU:0.71% | GPU RAM:7.35GB
active-> 110777344
MFU: 0.71%
mfu 0.26%
step: 3 | loss:11.1122 | dt:1648.84ms | tok/s:4,968 | MFU:0.71% | GPU RAM:7.35GB
active-> 110777344
MFU: 0.71%
mfu 0.30%
step: 4 | loss:11.0980 | dt:1645.99ms | tok/s:4,977 | MFU:0.71% | GPU RAM:7.35GB
active-> 110777344
MFU: 0.70%
mfu 0.34%
step: 5 | loss:11.0153 | dt:1654.80ms | tok/s:4,950 | MFU:0.70% | GPU RAM:7.35GB
active-> 110777344
MFU: 0.70%
mfu 0.38%
step: 6 | loss:11.1145 | dt:1661.09ms | tok/s:4,932 | MFU:0.70% | GPU RAM:7.35GB
active-> 110777344
MFU: 0.70%
mfu 0.41%
step: 7 | loss:11.0012 | dt:1667.44ms | tok/s:4,913 | MFU:0.70% | GPU RAM:7.35GB
active-> 110777344
MFU: 0.69%
mfu 0.44%
step: 8 | loss:11.0494 | dt:1691.82ms | tok/s:4,842 | MFU:0.69% | GPU RAM:7.35GB
active-> 110777344
MFU: 0.69%
mfu 0.46%
step: 9 | loss:11.0502 | dt:1685.60ms | tok/s:4,860 | MFU:0.69% | GPU RAM:7.35GB
active-> 110777344
MFU: 0.68%
mfu 0.48%
step: 10 | loss:11.0286 | dt:1708.52ms | tok/s:4,795 | MFU:0.68% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.68%
mfu 0.50%
step: 11 | loss:10.9935 | dt:1715.43ms | tok/s:4,775 | MFU:0.68% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.67%
mfu 0.52%
step: 12 | loss:11.0236 | dt:1736.74ms | tok/s:4,717 | MFU:0.67% | GPU RAM:7.37GB
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                               aten::mm         2.06%     110.990ms         3.68%     198.353ms      78.962us        2.132s        57.95%        2.132s     848.648us          2512   5081986.499  
       autograd::engine::evaluate_function: MmBackward0         0.32%      17.142ms         3.62%     195.261ms     259.656us       0.000us         0.00%        1.378s       1.832ms           752            --  
                                            MmBackward0         0.33%      17.785ms         3.30%     178.119ms     236.861us       0.000us         0.00%        1.378s       1.832ms           752            --  
                                           aten::linear         0.24%      12.885ms         4.06%     219.080ms     120.110us       0.000us         0.00%        1.370s     751.014us          1824            --  
                        DistributedDataParallel.forward         0.00%       0.000us         0.00%       0.000us       0.000us        1.193s        32.43%        1.193s     298.232ms             4            --  
                                          ProfilerStep*         0.57%      30.647ms        65.98%        3.556s        1.778s       0.000us         0.00%        1.182s     590.940ms             2            --  
                                           forward_pass         0.02%       1.274ms        22.59%        1.218s     304.456ms       0.000us         0.00%        1.038s     259.568ms             4            --  
                             Torch-Compiled Region: 0/0         0.01%     788.839us        22.56%        1.216s     304.060ms       0.000us         0.00%        1.038s     259.565ms             4            --  
                        DistributedDataParallel.forward         0.10%       5.311ms        22.55%        1.215s     303.862ms       0.000us         0.00%        1.038s     259.565ms             4            --  
                             Torch-Compiled Region: 4/0         0.00%      47.601us        22.32%        1.203s     300.708ms       0.000us         0.00%        1.030s     257.481ms             4            --  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 5.390s
Self CUDA time total: 3.679s

active-> 110777344
MFU: 0.04%
mfu 0.47%
step: 13 | loss:10.9566 | dt:26957.10ms | tok/s:304 | MFU:0.04% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.66%
mfu 0.49%
step: 14 | loss:10.9520 | dt:1778.73ms | tok/s:4,606 | MFU:0.66% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.51%
step: 15 | loss:10.9506 | dt:1689.55ms | tok/s:4,849 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.53%
step: 16 | loss:11.0116 | dt:1688.96ms | tok/s:4,850 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.68%
mfu 0.54%
step: 17 | loss:10.9148 | dt:1713.79ms | tok/s:4,780 | MFU:0.68% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.68%
mfu 0.56%
step: 18 | loss:10.9888 | dt:1719.77ms | tok/s:4,763 | MFU:0.68% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.68%
mfu 0.57%
step: 19 | loss:11.0425 | dt:1711.84ms | tok/s:4,785 | MFU:0.68% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.68%
mfu 0.58%
step: 20 | loss:10.9024 | dt:1716.98ms | tok/s:4,771 | MFU:0.68% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.68%
mfu 0.59%
step: 21 | loss:10.8622 | dt:1708.98ms | tok/s:4,794 | MFU:0.68% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.68%
mfu 0.60%
step: 22 | loss:10.9393 | dt:1713.83ms | tok/s:4,780 | MFU:0.68% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.68%
mfu 0.61%
step: 23 | loss:10.8511 | dt:1709.52ms | tok/s:4,792 | MFU:0.68% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.62%
step: 24 | loss:10.9447 | dt:1691.79ms | tok/s:4,842 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.62%
step: 25 | loss:10.9635 | dt:1691.20ms | tok/s:4,844 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.63%
step: 26 | loss:10.8812 | dt:1693.33ms | tok/s:4,838 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.64%
step: 27 | loss:10.7737 | dt:1688.21ms | tok/s:4,852 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.64%
step: 28 | loss:10.8028 | dt:1699.11ms | tok/s:4,821 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.65%
step: 29 | loss:10.7564 | dt:1680.04ms | tok/s:4,876 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.65%
step: 30 | loss:10.9134 | dt:1682.00ms | tok/s:4,870 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.66%
step: 31 | loss:10.8430 | dt:1677.96ms | tok/s:4,882 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.66%
step: 32 | loss:10.7698 | dt:1678.84ms | tok/s:4,880 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.66%
step: 33 | loss:10.7970 | dt:1675.07ms | tok/s:4,891 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.67%
step: 34 | loss:10.7591 | dt:1667.67ms | tok/s:4,912 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.67%
step: 35 | loss:10.5613 | dt:1670.33ms | tok/s:4,904 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.67%
step: 36 | loss:10.6321 | dt:1664.36ms | tok/s:4,922 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.68%
step: 37 | loss:10.7513 | dt:1663.35ms | tok/s:4,925 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.68%
step: 38 | loss:10.6103 | dt:1664.35ms | tok/s:4,922 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.68%
step: 39 | loss:10.6334 | dt:1667.15ms | tok/s:4,914 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.68%
step: 40 | loss:10.4404 | dt:1671.45ms | tok/s:4,901 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.68%
step: 41 | loss:10.5239 | dt:1654.45ms | tok/s:4,951 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 42 | loss:10.5275 | dt:1657.32ms | tok/s:4,943 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 43 | loss:10.6021 | dt:1666.01ms | tok/s:4,917 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 44 | loss:10.4042 | dt:1659.63ms | tok/s:4,936 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 45 | loss:10.3746 | dt:1664.88ms | tok/s:4,920 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 46 | loss:10.4890 | dt:1656.89ms | tok/s:4,944 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 47 | loss:10.3911 | dt:1657.34ms | tok/s:4,943 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 48 | loss:10.3861 | dt:1659.89ms | tok/s:4,935 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.18%
mfu 0.64%
step: 49 | loss:10.3106 | dt:6325.20ms | tok/s:1,295 | MFU:0.18% | GPU RAM:7.37GB
--------val run-------- train loss 10.3024 | val loss 10.3082 | dt 70760.4431ms
active-> 110777344
MFU: 0.65%
mfu 0.64%
step: 50 | loss:10.2340 | dt:1795.41ms | tok/s:4,563 | MFU:0.65% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.65%
step: 51 | loss:10.3210 | dt:1680.23ms | tok/s:4,876 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.65%
step: 52 | loss:10.2519 | dt:1658.11ms | tok/s:4,941 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.66%
step: 53 | loss:10.2179 | dt:1660.25ms | tok/s:4,934 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.66%
step: 54 | loss:10.0204 | dt:1671.01ms | tok/s:4,902 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.67%
step: 55 | loss:10.1588 | dt:1655.43ms | tok/s:4,949 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.67%
step: 56 | loss:10.0181 | dt:1673.32ms | tok/s:4,896 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.67%
step: 57 | loss:10.1149 | dt:1670.35ms | tok/s:4,904 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.68%
step: 58 | loss:9.9981 | dt:1672.34ms | tok/s:4,899 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.68%
step: 59 | loss:9.8909 | dt:1672.81ms | tok/s:4,897 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 60 | loss:10.0087 | dt:1687.55ms | tok/s:4,854 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 61 | loss:9.9135 | dt:1692.19ms | tok/s:4,841 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 62 | loss:9.8876 | dt:1686.87ms | tok/s:4,856 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 63 | loss:9.8362 | dt:1693.42ms | tok/s:4,838 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 64 | loss:9.7045 | dt:1692.42ms | tok/s:4,840 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 65 | loss:9.8790 | dt:1697.78ms | tok/s:4,825 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 66 | loss:9.8374 | dt:1688.70ms | tok/s:4,851 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.68%
mfu 0.68%
step: 67 | loss:9.6535 | dt:1709.02ms | tok/s:4,793 | MFU:0.68% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 68 | loss:9.6840 | dt:1698.16ms | tok/s:4,824 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 69 | loss:9.3974 | dt:1694.52ms | tok/s:4,834 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 70 | loss:9.4047 | dt:1698.76ms | tok/s:4,822 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 71 | loss:9.2793 | dt:1701.33ms | tok/s:4,815 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 72 | loss:9.2605 | dt:1695.32ms | tok/s:4,832 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 73 | loss:9.2655 | dt:1699.18ms | tok/s:4,821 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 74 | loss:9.1542 | dt:1699.94ms | tok/s:4,819 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.68%
mfu 0.68%
step: 75 | loss:9.0680 | dt:1706.52ms | tok/s:4,800 | MFU:0.68% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 76 | loss:9.1543 | dt:1691.88ms | tok/s:4,842 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 77 | loss:9.0039 | dt:1689.63ms | tok/s:4,848 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.68%
mfu 0.69%
step: 78 | loss:8.8624 | dt:1712.80ms | tok/s:4,783 | MFU:0.68% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 79 | loss:9.1013 | dt:1695.85ms | tok/s:4,831 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 80 | loss:8.9350 | dt:1688.95ms | tok/s:4,850 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 81 | loss:8.6074 | dt:1680.78ms | tok/s:4,874 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 82 | loss:8.6172 | dt:1677.69ms | tok/s:4,883 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 83 | loss:8.6237 | dt:1671.75ms | tok/s:4,900 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 84 | loss:8.7526 | dt:1673.08ms | tok/s:4,896 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 85 | loss:8.8919 | dt:1673.90ms | tok/s:4,894 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 86 | loss:8.3962 | dt:1690.78ms | tok/s:4,845 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 87 | loss:8.5382 | dt:1681.97ms | tok/s:4,870 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 88 | loss:8.7396 | dt:1675.37ms | tok/s:4,890 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 89 | loss:8.1298 | dt:1674.07ms | tok/s:4,893 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 90 | loss:8.2182 | dt:1679.00ms | tok/s:4,879 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 91 | loss:8.0346 | dt:1656.47ms | tok/s:4,945 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 92 | loss:8.1491 | dt:1671.57ms | tok/s:4,901 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 93 | loss:7.9365 | dt:1665.31ms | tok/s:4,919 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 94 | loss:7.6996 | dt:1654.91ms | tok/s:4,950 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 95 | loss:8.1164 | dt:1656.32ms | tok/s:4,946 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 96 | loss:7.9752 | dt:1675.31ms | tok/s:4,890 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.63%
mfu 0.69%
step: 97 | loss:7.7549 | dt:1855.14ms | tok/s:4,416 | MFU:0.63% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.69%
step: 98 | loss:7.3923 | dt:1651.93ms | tok/s:4,959 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.19%
mfu 0.64%
step: 99 | loss:7.4358 | dt:5986.94ms | tok/s:1,368 | MFU:0.19% | GPU RAM:7.37GB
--------val run-------- train loss 7.5953 | val loss 8.0079 | dt 67638.3474ms
active-> 110777344
MFU: 0.68%
mfu 0.65%
step: 100 | loss:7.3381 | dt:1704.67ms | tok/s:4,806 | MFU:0.68% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.65%
step: 101 | loss:7.6335 | dt:1696.63ms | tok/s:4,828 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.66%
step: 102 | loss:7.3635 | dt:1648.75ms | tok/s:4,969 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.66%
step: 103 | loss:7.4331 | dt:1640.74ms | tok/s:4,993 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.67%
step: 104 | loss:7.3692 | dt:1653.43ms | tok/s:4,955 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.67%
step: 105 | loss:7.1968 | dt:1639.43ms | tok/s:4,997 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.67%
step: 106 | loss:7.4845 | dt:1632.54ms | tok/s:5,018 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.68%
step: 107 | loss:6.9967 | dt:1641.24ms | tok/s:4,991 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.68%
step: 108 | loss:7.2026 | dt:1661.29ms | tok/s:4,931 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.68%
step: 109 | loss:6.8607 | dt:1651.27ms | tok/s:4,961 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.69%
step: 110 | loss:7.1349 | dt:1646.57ms | tok/s:4,975 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.69%
step: 111 | loss:6.8136 | dt:1646.38ms | tok/s:4,976 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 112 | loss:6.5790 | dt:1661.50ms | tok/s:4,930 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.69%
step: 113 | loss:7.3862 | dt:1648.87ms | tok/s:4,968 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.69%
step: 114 | loss:7.5690 | dt:1649.60ms | tok/s:4,966 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 115 | loss:6.8291 | dt:1655.43ms | tok/s:4,949 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 116 | loss:6.9424 | dt:1643.23ms | tok/s:4,985 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 117 | loss:6.6629 | dt:1655.15ms | tok/s:4,949 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 118 | loss:7.0633 | dt:1668.55ms | tok/s:4,910 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 119 | loss:7.1350 | dt:1652.04ms | tok/s:4,959 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 120 | loss:6.6990 | dt:1666.23ms | tok/s:4,916 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 121 | loss:6.9253 | dt:1655.64ms | tok/s:4,948 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 122 | loss:6.8227 | dt:1663.57ms | tok/s:4,924 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 123 | loss:6.6928 | dt:1643.27ms | tok/s:4,985 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 124 | loss:6.5622 | dt:1659.96ms | tok/s:4,935 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 125 | loss:6.2435 | dt:1660.55ms | tok/s:4,933 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 126 | loss:6.6417 | dt:1643.31ms | tok/s:4,985 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 127 | loss:6.7613 | dt:1650.65ms | tok/s:4,963 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 128 | loss:6.8659 | dt:1645.02ms | tok/s:4,980 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 129 | loss:6.7066 | dt:1661.55ms | tok/s:4,930 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 130 | loss:6.5315 | dt:1664.35ms | tok/s:4,922 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 131 | loss:6.5241 | dt:1655.07ms | tok/s:4,950 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 132 | loss:6.6471 | dt:1651.27ms | tok/s:4,961 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 133 | loss:6.4329 | dt:1633.47ms | tok/s:5,015 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 134 | loss:6.3020 | dt:1653.59ms | tok/s:4,954 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 135 | loss:6.5859 | dt:1669.91ms | tok/s:4,906 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 136 | loss:6.5423 | dt:1662.70ms | tok/s:4,927 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 137 | loss:6.5605 | dt:1642.14ms | tok/s:4,989 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 138 | loss:6.7091 | dt:1648.85ms | tok/s:4,968 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 139 | loss:6.3953 | dt:1637.98ms | tok/s:5,001 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 140 | loss:7.0269 | dt:1648.94ms | tok/s:4,968 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 141 | loss:6.4454 | dt:1650.30ms | tok/s:4,964 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 142 | loss:6.7797 | dt:1672.74ms | tok/s:4,897 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.71%
step: 143 | loss:6.6983 | dt:1640.43ms | tok/s:4,994 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 144 | loss:6.4187 | dt:1662.57ms | tok/s:4,927 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 145 | loss:6.1939 | dt:1664.00ms | tok/s:4,923 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 146 | loss:6.5398 | dt:1659.89ms | tok/s:4,935 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 147 | loss:6.0872 | dt:1647.30ms | tok/s:4,973 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 148 | loss:6.2978 | dt:1648.74ms | tok/s:4,969 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.20%
mfu 0.65%
step: 149 | loss:6.7625 | dt:5886.62ms | tok/s:1,392 | MFU:0.20% | GPU RAM:7.37GB
--------val run-------- train loss 6.3077 | val loss 7.0458 | dt 67041.4465ms
active-> 110777344
MFU: 0.68%
mfu 0.66%
step: 150 | loss:6.2519 | dt:1710.46ms | tok/s:4,789 | MFU:0.68% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.66%
step: 151 | loss:6.6291 | dt:1648.05ms | tok/s:4,971 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.67%
step: 152 | loss:6.0766 | dt:1628.21ms | tok/s:5,031 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.67%
step: 153 | loss:6.0789 | dt:1637.21ms | tok/s:5,004 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.67%
step: 154 | loss:6.3146 | dt:1659.98ms | tok/s:4,935 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.72%
mfu 0.68%
step: 155 | loss:6.2748 | dt:1625.21ms | tok/s:5,041 | MFU:0.72% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.68%
step: 156 | loss:6.4783 | dt:1644.44ms | tok/s:4,982 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.68%
step: 157 | loss:6.4877 | dt:1641.33ms | tok/s:4,991 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 158 | loss:6.2833 | dt:1677.68ms | tok/s:4,883 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 159 | loss:6.1708 | dt:1658.78ms | tok/s:4,939 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 160 | loss:6.3795 | dt:1655.77ms | tok/s:4,948 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.69%
step: 161 | loss:6.5154 | dt:1653.52ms | tok/s:4,954 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 162 | loss:6.0655 | dt:1660.45ms | tok/s:4,934 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.69%
step: 163 | loss:6.3675 | dt:1644.15ms | tok/s:4,983 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 164 | loss:5.8651 | dt:1666.27ms | tok/s:4,916 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 165 | loss:6.2357 | dt:1671.22ms | tok/s:4,902 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 166 | loss:6.1242 | dt:1659.11ms | tok/s:4,938 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 167 | loss:6.1241 | dt:1668.16ms | tok/s:4,911 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 168 | loss:6.1081 | dt:1661.47ms | tok/s:4,931 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.71%
mfu 0.70%
step: 169 | loss:6.0633 | dt:1652.38ms | tok/s:4,958 | MFU:0.71% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.70%
step: 170 | loss:5.9484 | dt:1681.59ms | tok/s:4,872 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 171 | loss:5.6326 | dt:1671.08ms | tok/s:4,902 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.70%
step: 172 | loss:6.1243 | dt:1688.89ms | tok/s:4,851 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.70%
step: 173 | loss:6.1257 | dt:1689.88ms | tok/s:4,848 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.70%
step: 174 | loss:6.1634 | dt:1677.84ms | tok/s:4,882 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 175 | loss:5.9165 | dt:1665.37ms | tok/s:4,919 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 176 | loss:6.2906 | dt:1668.95ms | tok/s:4,908 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 177 | loss:5.9605 | dt:1655.25ms | tok/s:4,949 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 178 | loss:6.1483 | dt:1677.49ms | tok/s:4,883 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.70%
step: 179 | loss:5.8584 | dt:1695.54ms | tok/s:4,832 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.70%
step: 180 | loss:6.4446 | dt:1682.33ms | tok/s:4,869 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 181 | loss:6.1751 | dt:1677.07ms | tok/s:4,885 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 182 | loss:6.1545 | dt:1655.59ms | tok/s:4,948 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 183 | loss:6.1826 | dt:1658.75ms | tok/s:4,939 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.70%
step: 184 | loss:5.8302 | dt:1680.48ms | tok/s:4,875 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.70%
step: 185 | loss:6.1247 | dt:1682.59ms | tok/s:4,869 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.70%
step: 186 | loss:6.1506 | dt:1678.17ms | tok/s:4,882 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.70%
step: 187 | loss:6.2458 | dt:1691.01ms | tok/s:4,844 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.70%
step: 188 | loss:6.0528 | dt:1684.52ms | tok/s:4,863 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.70%
step: 189 | loss:6.0490 | dt:1678.25ms | tok/s:4,881 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 190 | loss:5.9733 | dt:1691.31ms | tok/s:4,844 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 191 | loss:5.8383 | dt:1678.00ms | tok/s:4,882 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 192 | loss:6.1688 | dt:1668.54ms | tok/s:4,910 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 193 | loss:6.1757 | dt:1667.68ms | tok/s:4,912 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 194 | loss:6.0506 | dt:1674.88ms | tok/s:4,891 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 195 | loss:6.2032 | dt:1676.38ms | tok/s:4,887 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 196 | loss:6.0135 | dt:1658.54ms | tok/s:4,939 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.70%
step: 197 | loss:5.7625 | dt:1680.67ms | tok/s:4,874 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.70%
step: 198 | loss:5.8126 | dt:1671.07ms | tok/s:4,902 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.19%
mfu 0.65%
step: 199 | loss:5.9707 | dt:5987.63ms | tok/s:1,368 | MFU:0.19% | GPU RAM:7.37GB
--------val run-------- train loss 6.0555 | val loss 6.7926 | dt 67810.4950ms
active-> 110777344
MFU: 0.67%
mfu 0.65%
step: 200 | loss:6.1741 | dt:1743.97ms | tok/s:4,697 | MFU:0.67% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.65%
step: 201 | loss:5.7346 | dt:1670.02ms | tok/s:4,905 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.66%
step: 202 | loss:6.0090 | dt:1663.73ms | tok/s:4,924 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.66%
step: 203 | loss:5.6155 | dt:1677.61ms | tok/s:4,883 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.67%
step: 204 | loss:5.7843 | dt:1675.60ms | tok/s:4,889 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.67%
step: 205 | loss:6.2113 | dt:1679.73ms | tok/s:4,877 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.67%
step: 206 | loss:6.0507 | dt:1668.10ms | tok/s:4,911 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.67%
step: 207 | loss:5.8721 | dt:1670.37ms | tok/s:4,904 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 208 | loss:5.8244 | dt:1678.99ms | tok/s:4,879 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.68%
step: 209 | loss:6.2542 | dt:1677.16ms | tok/s:4,884 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 210 | loss:6.0506 | dt:1685.85ms | tok/s:4,859 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 211 | loss:5.9750 | dt:1682.53ms | tok/s:4,869 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 212 | loss:6.1546 | dt:1680.45ms | tok/s:4,875 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 213 | loss:6.0120 | dt:1698.66ms | tok/s:4,823 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.68%
mfu 0.68%
step: 214 | loss:6.5571 | dt:1710.04ms | tok/s:4,791 | MFU:0.68% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 215 | loss:5.9244 | dt:1688.01ms | tok/s:4,853 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.68%
mfu 0.68%
step: 216 | loss:6.0729 | dt:1712.49ms | tok/s:4,784 | MFU:0.68% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.68%
mfu 0.68%
step: 217 | loss:5.7660 | dt:1718.83ms | tok/s:4,766 | MFU:0.68% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 218 | loss:5.8185 | dt:1691.61ms | tok/s:4,843 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 219 | loss:6.0885 | dt:1696.49ms | tok/s:4,829 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 220 | loss:6.0795 | dt:1695.00ms | tok/s:4,833 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 221 | loss:6.0413 | dt:1693.24ms | tok/s:4,838 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.68%
mfu 0.68%
step: 222 | loss:6.3289 | dt:1706.95ms | tok/s:4,799 | MFU:0.68% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 223 | loss:6.2183 | dt:1699.14ms | tok/s:4,821 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 224 | loss:5.9011 | dt:1700.75ms | tok/s:4,817 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.68%
mfu 0.68%
step: 225 | loss:5.6825 | dt:1716.89ms | tok/s:4,771 | MFU:0.68% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 226 | loss:5.8686 | dt:1702.13ms | tok/s:4,813 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 227 | loss:5.9687 | dt:1692.34ms | tok/s:4,841 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.68%
step: 228 | loss:6.2145 | dt:1698.81ms | tok/s:4,822 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 229 | loss:5.5371 | dt:1693.07ms | tok/s:4,839 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 230 | loss:5.8344 | dt:1694.14ms | tok/s:4,835 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 231 | loss:5.7974 | dt:1701.35ms | tok/s:4,815 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 232 | loss:5.9625 | dt:1690.89ms | tok/s:4,845 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 233 | loss:5.7399 | dt:1697.46ms | tok/s:4,826 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 234 | loss:6.0438 | dt:1694.95ms | tok/s:4,833 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 235 | loss:6.2727 | dt:1687.09ms | tok/s:4,856 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 236 | loss:6.1107 | dt:1690.07ms | tok/s:4,847 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 237 | loss:6.0963 | dt:1688.97ms | tok/s:4,850 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 238 | loss:5.8396 | dt:1688.09ms | tok/s:4,853 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 239 | loss:6.1762 | dt:1692.12ms | tok/s:4,841 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 240 | loss:5.7171 | dt:1685.59ms | tok/s:4,860 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 241 | loss:6.3053 | dt:1683.54ms | tok/s:4,866 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 242 | loss:6.4949 | dt:1694.10ms | tok/s:4,836 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 243 | loss:5.9642 | dt:1694.62ms | tok/s:4,834 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 244 | loss:5.8862 | dt:1686.05ms | tok/s:4,859 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 245 | loss:6.0429 | dt:1686.06ms | tok/s:4,859 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 246 | loss:6.2052 | dt:1684.55ms | tok/s:4,863 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.70%
mfu 0.69%
step: 247 | loss:6.1608 | dt:1673.71ms | tok/s:4,895 | MFU:0.70% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.69%
mfu 0.69%
step: 248 | loss:6.1614 | dt:1681.99ms | tok/s:4,870 | MFU:0.69% | GPU RAM:7.37GB
active-> 110777344
MFU: 0.19%
mfu 0.64%
step: 249 | loss:6.4266 | dt:5987.61ms | tok/s:1,368 | MFU:0.19% | GPU RAM:7.37GB
--------val run-------- train loss 6.0026 | val loss 6.8115 | dt 67907.7935ms

ğŸ“Š PROFILER USAGE GUIDE:

1. Traces are saved to ./profiler_logs/
2. View in Chrome: chrome://tracing (load JSON files)
3. View in TensorBoard: 
   tensorboard --logdir=./profiler_logs
   
4. Key metrics to look for:
   - CUDA kernel launch overhead
   - Memory allocation patterns
   - CPU/GPU utilization
   - Communication overhead (DDP/FSDP)
   - Kernel execution time

active-> 110777344
MFU: 0.68%
mfu 0.64%
step: 250 | loss:5.9807 | dt:1721.32ms | tok/s:4,759 | MFU:0.68% | GPU RAM:7.37GB
wandb: 
wandb: Run history:
wandb:                    memory/allocated_gb â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–ˆâ–ˆâ–‡â–‡â–„â–†â–‚â–â–…â–‚â–„â–â–ƒâ–…â–‚â–ƒâ–‚â–†â–‡â–†â–†â–†â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆ
wandb:                memory/max_allocated_gb â–â–„â–„â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                     memory/reserved_gb â–â–â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆ
wandb:                 perf/iteration_time_ms â–â–‚â–ƒâ–†â–ˆâ–„â–„â–ƒâ–ƒâ–„â–†â–†â–…â–†â–„â–ƒâ–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–„â–„â–ƒâ–ƒâ–‚â–„â–„â–ƒâ–†â–…â–…â–†â–…â–…â–…â–…â–…â–…
wandb:                       perf/mfu_percent â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:         perf/throughput_tokens_per_sec â–‡â–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–…â–…â–†â–†â–…â–„â–„â–…â–„â–…â–‡â–†â–…â–ˆâ–†â–†â–‡â–‡â–‡â–†â–†â–†â–‡â–„â–†â–†â–â–…â–ƒâ–ƒâ–„â–„â–„
wandb: perf/throughput_tokens_per_sec_per_gpu â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                        train/grad_norm â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–…â–…â–…â–ƒâ–„â–„â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–â–â–‚â–â–ƒâ–â–‚â–
wandb:                             train/loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–…â–…â–…â–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–‚â–â–â–â–‚
wandb:                               train/lr â–â–‚â–‚â–‚â–‚â–ƒâ–„â–„â–„â–…â–…â–†â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–†â–…â–…â–…â–…â–…â–„â–„â–„â–„â–ƒâ–‚â–‚â–‚â–‚â–‚
wandb:                                     +1 ...
wandb: 
wandb: Run summary:
wandb:                    memory/allocated_gb 3.38351
wandb:                memory/max_allocated_gb 6.84066
wandb:                     memory/reserved_gb 7.36914
wandb:                 perf/iteration_time_ms 1721.31834
wandb:                       perf/mfu_percent 0.67741
wandb:         perf/throughput_tokens_per_sec 4759.14293
wandb: perf/throughput_tokens_per_sec_per_gpu 2379.57146
wandb:                        train/grad_norm 0.36639
wandb:                             train/loss 5.98074
wandb:                               train/lr 3e-05
wandb:                                     +1 ...
wandb: 
wandb: ğŸš€ View run shakespeare_gqa_20260115_082210 at: https://wandb.ai/adeeb-idris-coep-technological-university/llm-training/runs/wnr4043o
wandb: â­ï¸ View project at: https://wandb.ai/adeeb-idris-coep-technological-university/llm-training
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260115_082211-wnr4043o/logs
WandB run completed

ğŸ“Š PROFILER USAGE GUIDE:

1. Traces are saved to ./profiler_logs/
2. View in Chrome: chrome://tracing (load JSON files)
3. View in TensorBoard: 
   tensorboard --logdir=./profiler_logs
   
4. Key metrics to look for:
   - CUDA kernel launch overhead
   - Memory allocation patterns
   - CPU/GPU utilization
   - Communication overhead (DDP/FSDP)
   - Kernel execution time

[rank0]:[W115 08:36:03.742178788 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

Logs saved to: run_3_console_20260115_082157.log and run_3_error_20260115_082157.log

Copying output files for run 3...
  Scanning: /kaggle/working
  Scanning: .
  Scanning: ./run_3_logs
âœ“ Copied 0 files to ./Transformer_GPU/monitor_logs/run_3_20260115_083608

================================================================================
Committing and pushing to GitHub for run 3...
Configuring git identity...
âœ“ Files added to git
âœ“ Committed: Add monitor logs for run 3 - 0 files - 2026-01-15 08:36:08
âœ“ Pushed to GitHub successfully!

âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“
RUN 3 COMPLETED SUCCESSFULLY
Files saved: 0
Pushed to GitHub: âœ“
âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“

================================================================================
WAITING 10 SECONDS BEFORE NEXT RUN...
================================================================================

################################################################################
STARTING RUN 4/8
Time: 08:36:18
################################################################################

train.py updated with parallel_flag = 4
Running torchrun command for i=4...
W0115 08:36:20.737000 702 torch/distributed/run.py:774] 
W0115 08:36:20.737000 702 torch/distributed/run.py:774] *****************************************
W0115 08:36:20.737000 702 torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0115 08:36:20.737000 702 torch/distributed/run.py:774] *****************************************
[W115 08:36:21.973233282 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W115 08:36:21.974031570 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
parallel_flag :  4
parallel_flag :  4
[W115 08:36:25.144020609 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W115 08:36:25.144373420 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W115 08:36:25.144803818 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W115 08:36:25.145373649 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
Num GPUs = 2
=============
parallel_flag -  4
parallel_flag -  4
parallel_flag -  4
=============
=============
parallel_flag -  4
parallel_flag -  4
parallel_flag -  4
=============
total parameters = 186,274,816, active parameters = 110,777,344
Using compiled model
[rank1]:W0115 08:36:31.911000 710 torch/_logging/_internal.py:1154] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
wandb: Currently logged in as: adeeb-idris (adeeb-idris-coep-technological-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run ah0150my
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /kaggle/working/Transformer_GPU/project/wandb/run-20260115_083632-ah0150my
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run shakespeare_gqa_20260115_083631
wandb: â­ï¸ View project at https://wandb.ai/adeeb-idris-coep-technological-university/llm-training
wandb: ğŸš€ View run at https://wandb.ai/adeeb-idris-coep-technological-university/llm-training/runs/ah0150my
WandB initialized: project=llm-training, run=shakespeare_gqa_20260115_083631
[rank0]:W0115 08:36:34.041000 709 torch/_logging/_internal.py:1154] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank1]: Traceback (most recent call last):
[rank1]:   File "/kaggle/working/Transformer_GPU/project/main.py", line 818, in <module>
[rank1]:     _, loss, _ = model(x, y)
[rank1]:                  ^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 375, in __call__
[rank1]:     return super().__call__(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 736, in compile_wrapper
[rank1]:     return fn(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py", line 1495, in __call__
[rank1]:     return self._torchdynamo_orig_callable(
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py", line 1272, in __call__
[rank1]:     result = self._inner_convert(
[rank1]:              ^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py", line 629, in __call__
[rank1]:     return _compile(
[rank1]:            ^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py", line 1111, in _compile
[rank1]:     guarded_code = compile_inner(code, one_graph, hooks, transform)
[rank1]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_utils_internal.py", line 97, in wrapper_function
[rank1]:     return function(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py", line 793, in compile_inner
[rank1]:     return _compile_inner(code, one_graph, hooks, transform)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py", line 832, in _compile_inner
[rank1]:     out_code = transform_code_object(code, transform)
[rank1]:                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/bytecode_transformation.py", line 1424, in transform_code_object
[rank1]:     transformations(instructions, code_options)
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py", line 267, in _fn
[rank1]:     return fn(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py", line 753, in transform
[rank1]:     tracer.run()
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 3497, in run
[rank1]:     super().run()
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1363, in run
[rank1]:     while self.step():
[rank1]:           ^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1267, in step
[rank1]:     self.dispatch_table[inst.opcode](self, inst)
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 834, in wrapper
[rank1]:     return inner_fn(self, inst)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 2228, in CALL_FUNCTION_EX
[rank1]:     self.call_function(fn, argsvars.items, kwargsvars)
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1193, in call_function
[rank1]:     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]
[rank1]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/lazy.py", line 201, in realize_and_forward
[rank1]:     return getattr(self.realize(), name)(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/nn_module.py", line 1000, in call_function
[rank1]:     return variables.UserFunctionVariable(fn, source=source).call_function(
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py", line 529, in call_function
[rank1]:     return super().call_function(tx, args, kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py", line 293, in call_function
[rank1]:     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1210, in inline_user_function_return
[rank1]:     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 3698, in inline_call
[rank1]:     return tracer.inline_call_()
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 3901, in inline_call_
[rank1]:     self.run()
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1363, in run
[rank1]:     while self.step():
[rank1]:           ^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1267, in step
[rank1]:     self.dispatch_table[inst.opcode](self, inst)
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 834, in wrapper
[rank1]:     return inner_fn(self, inst)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 2910, in CALL
[rank1]:     self._call(inst)
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 2904, in _call
[rank1]:     self.call_function(fn, args, kwargs)
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1193, in call_function
[rank1]:     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]
[rank1]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/lazy.py", line 201, in realize_and_forward
[rank1]:     return getattr(self.realize(), name)(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py", line 529, in call_function
[rank1]:     return super().call_function(tx, args, kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py", line 293, in call_function
[rank1]:     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1210, in inline_user_function_return
[rank1]:     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 3698, in inline_call
[rank1]:     return tracer.inline_call_()
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 3901, in inline_call_
[rank1]:     self.run()
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1363, in run
[rank1]:     while self.step():
[rank1]:           ^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1267, in step
[rank1]:     self.dispatch_table[inst.opcode](self, inst)
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 834, in wrapper
[rank1]:     return inner_fn(self, inst)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 2910, in CALL
[rank1]:     self._call(inst)
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 2904, in _call
[rank1]:     self.call_function(fn, args, kwargs)
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1193, in call_function
[rank1]:     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]
[rank1]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/lazy.py", line 201, in realize_and_forward
[rank1]:     return getattr(self.realize(), name)(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py", line 529, in call_function
[rank1]:     return super().call_function(tx, args, kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py", line 293, in call_function
[rank1]:     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1210, in inline_user_function_return
[rank1]:     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 3698, in inline_call
[rank1]:     return tracer.inline_call_()
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 3901, in inline_call_
[rank1]:     self.run()
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1363, in run
[rank1]:     while self.step():
[rank1]:           ^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1267, in step
[rank1]:     self.dispatch_table[inst.opcode](self, inst)
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 834, in wrapper
[rank1]:     return inner_fn(self, inst)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 2910, in CALL
[rank1]:     self._call(inst)
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 2904, in _call
[rank1]:     self.call_function(fn, args, kwargs)
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1193, in call_function
[rank1]:     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]
[rank1]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/lazy.py", line 201, in realize_and_forward
[rank1]:     return getattr(self.realize(), name)(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py", line 529, in call_function
[rank1]:     return super().call_function(tx, args, kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py", line 293, in call_function
[rank1]:     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1210, in inline_user_function_return
[rank1]:     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 3698, in inline_call
[rank1]:     return tracer.inline_call_()
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 3901, in inline_call_
[rank1]:     self.run()
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1363, in run
[rank1]:     while self.step():
[rank1]:           ^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1267, in step
[rank1]:     self.dispatch_table[inst.opcode](self, inst)
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 2310, in STORE_ATTR
[rank1]:     BuiltinVariable(setattr).call_function(
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/builtin.py", line 1189, in call_function
[rank1]:     return handler(tx, args, kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/builtin.py", line 1000, in builtin_dispatch
[rank1]:     rv = handler(tx, args, kwargs)
[rank1]:          ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/builtin.py", line 887, in call_self_handler
[rank1]:     result = self_handler(tx, *args, **kwargs)
[rank1]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/builtin.py", line 2199, in call_setattr
[rank1]:     out = wrap_fx_proxy(
[rank1]:           ^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/builder.py", line 2559, in wrap_fx_proxy
[rank1]:     return wrap_fx_proxy_cls(target_cls=TensorVariable, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/builder.py", line 2625, in wrap_fx_proxy_cls
[rank1]:     return _wrap_fx_proxy(
[rank1]:            ^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/builder.py", line 2723, in _wrap_fx_proxy
[rank1]:     example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True)
[rank1]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py", line 3355, in get_fake_value
[rank1]:     raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py", line 3253, in get_fake_value
[rank1]:     ret_val = wrap_fake_exception(
[rank1]:               ^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py", line 2753, in wrap_fake_exception
[rank1]:     return fn()
[rank1]:            ^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py", line 3254, in <lambda>
[rank1]:     lambda: run_node(tx.output, node, args, kwargs, nnmodule)
[rank1]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py", line 3462, in run_node
[rank1]:     raise RuntimeError(make_error_message(e)).with_traceback(
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py", line 3421, in run_node
[rank1]:     return node.target(*args, **kwargs)
[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank1]: torch._dynamo.exc.TorchRuntimeError: Dynamo failed to run FX node with fake tensors: call_function <method 'set_' of 'torch._C.TensorBase' objects>(*(FakeTensor(..., device='cuda:1', size=(7,)), FakeTensor(..., device='cuda:1', size=(7,), dtype=torch.bfloat16)), **{}): got RuntimeError('Could not set tensor of type c10::BFloat16 to a tensor of type float')

[rank1]: from user code:
[rank1]:    File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py", line 70, in inner
[rank1]:     return fn(*args, **kwargs)
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 838, in forward
[rank1]:     args, kwargs = _root_pre_forward(self, self, args, kwargs)
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 517, in _root_pre_forward
[rank1]:     _lazy_init(state, module)
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 138, in _lazy_init
[rank1]:     _cast_buffers_to_dtype_and_device(buffers, buffer_dtypes, state.compute_device)
[rank1]:   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 1645, in _cast_buffers_to_dtype_and_device
[rank1]:     buffer.data = buffer.to(device=device, dtype=buffer_dtype)

[rank1]: Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"

Traceback (most recent call last):
  File "/kaggle/working/Transformer_GPU/project/main.py", line 818, in <module>
    _, loss, _ = model(x, y)
                 ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 375, in __call__
    return super().__call__(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1879, in _call_impl
    return inner()
           ^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1827, in inner
    result = forward_call(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 736, in compile_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py", line 1495, in __call__
    return self._torchdynamo_orig_callable(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py", line 1272, in __call__
    result = self._inner_convert(
             ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py", line 629, in __call__
    return _compile(
           ^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py", line 1111, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_utils_internal.py", line 97, in wrapper_function
    return function(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py", line 793, in compile_inner
    return _compile_inner(code, one_graph, hooks, transform)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py", line 832, in _compile_inner
    out_code = transform_code_object(code, transform)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/bytecode_transformation.py", line 1424, in transform_code_object
    transformations(instructions, code_options)
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py", line 267, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py", line 753, in transform
    tracer.run()
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 3497, in run
    super().run()
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1363, in run
    while self.step():
          ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1267, in step
    self.dispatch_table[inst.opcode](self, inst)
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 834, in wrapper
    return inner_fn(self, inst)
           ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 2228, in CALL_FUNCTION_EX
    self.call_function(fn, argsvars.items, kwargsvars)
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1193, in call_function
    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/lazy.py", line 201, in realize_and_forward
    return getattr(self.realize(), name)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/nn_module.py", line 1000, in call_function
    return variables.UserFunctionVariable(fn, source=source).call_function(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py", line 529, in call_function
    return super().call_function(tx, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py", line 293, in call_function
    return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1210, in inline_user_function_return
    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 3698, in inline_call
    return tracer.inline_call_()
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 3901, in inline_call_
    self.run()
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1363, in run
    while self.step():
          ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1267, in step
    self.dispatch_table[inst.opcode](self, inst)
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 834, in wrapper
    return inner_fn(self, inst)
           ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 2910, in CALL
    self._call(inst)
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 2904, in _call
    self.call_function(fn, args, kwargs)
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1193, in call_function
    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/lazy.py", line 201, in realize_and_forward
    return getattr(self.realize(), name)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py", line 529, in call_function
    return super().call_function(tx, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py", line 293, in call_function
    return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1210, in inline_user_function_return
    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 3698, in inline_call
    return tracer.inline_call_()
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 3901, in inline_call_
    self.run()
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1363, in run
    while self.step():
          ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1267, in step
    self.dispatch_table[inst.opcode](self, inst)
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 834, in wrapper
    return inner_fn(self, inst)
           ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 2910, in CALL
    self._call(inst)
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 2904, in _call
    self.call_function(fn, args, kwargs)
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1193, in call_function
    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/lazy.py", line 201, in realize_and_forward
    return getattr(self.realize(), name)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py", line 529, in call_function
    return super().call_function(tx, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py", line 293, in call_function
    return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1210, in inline_user_function_return
    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 3698, in inline_call
    return tracer.inline_call_()
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 3901, in inline_call_
    self.run()
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1363, in run
    while self.step():
          ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1267, in step
    self.dispatch_table[inst.opcode](self, inst)
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 834, in wrapper
    return inner_fn(self, inst)
           ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 2910, in CALL
    self._call(inst)
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 2904, in _call
    self.call_function(fn, args, kwargs)
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1193, in call_function
    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/lazy.py", line 201, in realize_and_forward
    return getattr(self.realize(), name)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py", line 529, in call_function
    return super().call_function(tx, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py", line 293, in call_function
    return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1210, in inline_user_function_return
    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 3698, in inline_call
    return tracer.inline_call_()
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 3901, in inline_call_
    self.run()
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1363, in run
    while self.step():
          ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1267, in step
    self.dispatch_table[inst.opcode](self, inst)
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 2310, in STORE_ATTR
    BuiltinVariable(setattr).call_function(
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/builtin.py", line 1189, in call_function
    return handler(tx, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/builtin.py", line 1000, in builtin_dispatch
    rv = handler(tx, args, kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/builtin.py", line 887, in call_self_handler
    result = self_handler(tx, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/builtin.py", line 2199, in call_setattr
    out = wrap_fx_proxy(
          ^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/builder.py", line 2559, in wrap_fx_proxy
    return wrap_fx_proxy_cls(target_cls=TensorVariable, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/builder.py", line 2625, in wrap_fx_proxy_cls
    return _wrap_fx_proxy(
           ^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/builder.py", line 2723, in _wrap_fx_proxy
    example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py", line 3355, in get_fake_value
    raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py", line 3253, in get_fake_value
    ret_val = wrap_fake_exception(
              ^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py", line 2753, in wrap_fake_exception
    return fn()
           ^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py", line 3254, in <lambda>
    lambda: run_node(tx.output, node, args, kwargs, nnmodule)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py", line 3462, in run_node
    raise RuntimeError(make_error_message(e)).with_traceback(
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py", line 3421, in run_node
    return node.target(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch._dynamo.exc.TorchRuntimeError: Dynamo failed to run FX node with fake tensors: call_function <method 'set_' of 'torch._C.TensorBase' objects>(*(FakeTensor(..., device='cuda:0', size=(7,)), FakeTensor(..., device='cuda:0', size=(7,), dtype=torch.bfloat16)), **{}): got RuntimeError('Could not set tensor of type c10::BFloat16 to a tensor of type float')

from user code:
   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py", line 70, in inner
    return fn(*args, **kwargs)
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 838, in forward
    args, kwargs = _root_pre_forward(self, self, args, kwargs)
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 517, in _root_pre_forward
    _lazy_init(state, module)
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 138, in _lazy_init
    _cast_buffers_to_dtype_and_device(buffers, buffer_dtypes, state.compute_device)
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 1645, in _cast_buffers_to_dtype_and_device
    buffer.data = buffer.to(device=device, dtype=buffer_dtype)

Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"

[rank0]: Traceback (most recent call last):
[rank0]:   File "/kaggle/working/Transformer_GPU/project/main.py", line 818, in <module>
[rank0]:     _, loss, _ = model(x, y)
[rank0]:                  ^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 375, in __call__
[rank0]:     return super().__call__(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1879, in _call_impl
[rank0]:     return inner()
[rank0]:            ^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1827, in inner
[rank0]:     result = forward_call(*args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 736, in compile_wrapper
[rank0]:     return fn(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py", line 1495, in __call__
[rank0]:     return self._torchdynamo_orig_callable(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py", line 1272, in __call__
[rank0]:     result = self._inner_convert(
[rank0]:              ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py", line 629, in __call__
[rank0]:     return _compile(
[rank0]:            ^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py", line 1111, in _compile
[rank0]:     guarded_code = compile_inner(code, one_graph, hooks, transform)
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_utils_internal.py", line 97, in wrapper_function
[rank0]:     return function(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py", line 793, in compile_inner
[rank0]:     return _compile_inner(code, one_graph, hooks, transform)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py", line 832, in _compile_inner
[rank0]:     out_code = transform_code_object(code, transform)
[rank0]:                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/bytecode_transformation.py", line 1424, in transform_code_object
[rank0]:     transformations(instructions, code_options)
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py", line 267, in _fn
[rank0]:     return fn(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py", line 753, in transform
[rank0]:     tracer.run()
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 3497, in run
[rank0]:     super().run()
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1363, in run
[rank0]:     while self.step():
[rank0]:           ^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1267, in step
[rank0]:     self.dispatch_table[inst.opcode](self, inst)
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 834, in wrapper
[rank0]:     return inner_fn(self, inst)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 2228, in CALL_FUNCTION_EX
[rank0]:     self.call_function(fn, argsvars.items, kwargsvars)
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1193, in call_function
[rank0]:     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/lazy.py", line 201, in realize_and_forward
[rank0]:     return getattr(self.realize(), name)(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/nn_module.py", line 1000, in call_function
[rank0]:     return variables.UserFunctionVariable(fn, source=source).call_function(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py", line 529, in call_function
[rank0]:     return super().call_function(tx, args, kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py", line 293, in call_function
[rank0]:     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1210, in inline_user_function_return
[rank0]:     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 3698, in inline_call
[rank0]:     return tracer.inline_call_()
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 3901, in inline_call_
[rank0]:     self.run()
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1363, in run
[rank0]:     while self.step():
[rank0]:           ^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1267, in step
[rank0]:     self.dispatch_table[inst.opcode](self, inst)
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 834, in wrapper
[rank0]:     return inner_fn(self, inst)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 2910, in CALL
[rank0]:     self._call(inst)
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 2904, in _call
[rank0]:     self.call_function(fn, args, kwargs)
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1193, in call_function
[rank0]:     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/lazy.py", line 201, in realize_and_forward
[rank0]:     return getattr(self.realize(), name)(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py", line 529, in call_function
[rank0]:     return super().call_function(tx, args, kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py", line 293, in call_function
[rank0]:     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1210, in inline_user_function_return
[rank0]:     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 3698, in inline_call
[rank0]:     return tracer.inline_call_()
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 3901, in inline_call_
[rank0]:     self.run()
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1363, in run
[rank0]:     while self.step():
[rank0]:           ^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1267, in step
[rank0]:     self.dispatch_table[inst.opcode](self, inst)
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 834, in wrapper
[rank0]:     return inner_fn(self, inst)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 2910, in CALL
[rank0]:     self._call(inst)
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 2904, in _call
[rank0]:     self.call_function(fn, args, kwargs)
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1193, in call_function
[rank0]:     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/lazy.py", line 201, in realize_and_forward
[rank0]:     return getattr(self.realize(), name)(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py", line 529, in call_function
[rank0]:     return super().call_function(tx, args, kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py", line 293, in call_function
[rank0]:     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1210, in inline_user_function_return
[rank0]:     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 3698, in inline_call
[rank0]:     return tracer.inline_call_()
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 3901, in inline_call_
[rank0]:     self.run()
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1363, in run
[rank0]:     while self.step():
[rank0]:           ^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1267, in step
[rank0]:     self.dispatch_table[inst.opcode](self, inst)
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 834, in wrapper
[rank0]:     return inner_fn(self, inst)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 2910, in CALL
[rank0]:     self._call(inst)
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 2904, in _call
[rank0]:     self.call_function(fn, args, kwargs)
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1193, in call_function
[rank0]:     self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]
[rank0]:               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/lazy.py", line 201, in realize_and_forward
[rank0]:     return getattr(self.realize(), name)(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py", line 529, in call_function
[rank0]:     return super().call_function(tx, args, kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py", line 293, in call_function
[rank0]:     return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1210, in inline_user_function_return
[rank0]:     return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 3698, in inline_call
[rank0]:     return tracer.inline_call_()
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 3901, in inline_call_
[rank0]:     self.run()
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1363, in run
[rank0]:     while self.step():
[rank0]:           ^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 1267, in step
[rank0]:     self.dispatch_table[inst.opcode](self, inst)
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py", line 2310, in STORE_ATTR
[rank0]:     BuiltinVariable(setattr).call_function(
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/builtin.py", line 1189, in call_function
[rank0]:     return handler(tx, args, kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/builtin.py", line 1000, in builtin_dispatch
[rank0]:     rv = handler(tx, args, kwargs)
[rank0]:          ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/builtin.py", line 887, in call_self_handler
[rank0]:     result = self_handler(tx, *args, **kwargs)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/builtin.py", line 2199, in call_setattr
[rank0]:     out = wrap_fx_proxy(
[rank0]:           ^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/builder.py", line 2559, in wrap_fx_proxy
[rank0]:     return wrap_fx_proxy_cls(target_cls=TensorVariable, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/builder.py", line 2625, in wrap_fx_proxy_cls
[rank0]:     return _wrap_fx_proxy(
[rank0]:            ^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/builder.py", line 2723, in _wrap_fx_proxy
[rank0]:     example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True)
[rank0]:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py", line 3355, in get_fake_value
[rank0]:     raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py", line 3253, in get_fake_value
[rank0]:     ret_val = wrap_fake_exception(
[rank0]:               ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py", line 2753, in wrap_fake_exception
[rank0]:     return fn()
[rank0]:            ^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py", line 3254, in <lambda>
[rank0]:     lambda: run_node(tx.output, node, args, kwargs, nnmodule)
[rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py", line 3462, in run_node
[rank0]:     raise RuntimeError(make_error_message(e)).with_traceback(
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py", line 3421, in run_node
[rank0]:     return node.target(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: torch._dynamo.exc.TorchRuntimeError: Dynamo failed to run FX node with fake tensors: call_function <method 'set_' of 'torch._C.TensorBase' objects>(*(FakeTensor(..., device='cuda:0', size=(7,)), FakeTensor(..., device='cuda:0', size=(7,), dtype=torch.bfloat16)), **{}): got RuntimeError('Could not set tensor of type c10::BFloat16 to a tensor of type float')

[rank0]: from user code:
[rank0]:    File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/external_utils.py", line 70, in inner
[rank0]:     return fn(*args, **kwargs)
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 838, in forward
[rank0]:     args, kwargs = _root_pre_forward(self, self, args, kwargs)
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 517, in _root_pre_forward
[rank0]:     _lazy_init(state, module)
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 138, in _lazy_init
[rank0]:     _cast_buffers_to_dtype_and_device(buffers, buffer_dtypes, state.compute_device)
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/distributed/fsdp/_runtime_utils.py", line 1645, in _cast_buffers_to_dtype_and_device
[rank0]:     buffer.data = buffer.to(device=device, dtype=buffer_dtype)

[rank0]: Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS="+dynamo"

[1;34mwandb[0m: 
[1;34mwandb[0m: ğŸš€ View run [33mshakespeare_gqa_20260115_083631[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20260115_083632-ah0150my/logs[0m
W0115 08:36:47.318000 702 torch/distributed/elastic/multiprocessing/api.py:900] Sending process 709 closing signal SIGTERM
E0115 08:36:47.433000 702 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 1 (pid: 710) of binary: /usr/bin/python3
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 10, in <module>
    sys.exit(main())
             ^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 277, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
main.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2026-01-15_08:36:47
  host      : 80cc1dabe30d
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 710)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================

Logs saved to: run_4_console_20260115_083618.log and run_4_error_20260115_083618.log

Copying output files for run 4...
  Scanning: /kaggle/working
  Scanning: .
  Scanning: ./run_4_logs
âœ“ Copied 0 files to ./Transformer_GPU/monitor_logs/run_4_20260115_083647

================================================================================
Committing and pushing to GitHub for run 4...
Configuring git identity...
âœ“ Files added to git
âœ“ Committed: Add monitor logs for run 4 - 0 files - 2026-01-15 08:36:47
âœ“ Pushed to GitHub successfully!

âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“
RUN 4 COMPLETED SUCCESSFULLY
Files saved: 0
Pushed to GitHub: âœ“
âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“

================================================================================
WAITING 10 SECONDS BEFORE NEXT RUN...
================================================================================

################################################################################
STARTING RUN 5/8
Time: 08:36:58
################################################################################

train.py updated with parallel_flag = 5
Running torchrun command for i=5...
W0115 08:37:00.187000 808 torch/distributed/run.py:774] 
W0115 08:37:00.187000 808 torch/distributed/run.py:774] *****************************************
W0115 08:37:00.187000 808 torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0115 08:37:00.187000 808 torch/distributed/run.py:774] *****************************************
[W115 08:37:00.306101689 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W115 08:37:00.306861299 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
parallel_flag :  5
parallel_flag :  5
[W115 08:37:04.447394700 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W115 08:37:04.448200987 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
Num GPUs = 2
=============
parallel_flag -  5
parallel_flag -  5
parallel_flag -  5
=============
[W115 08:37:04.461026005 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W115 08:37:04.461668519 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
=============
parallel_flag -  5
parallel_flag -  5
parallel_flag -  5
=============
total parameters = 181,550,080, active parameters = 106,052,608
Using compiled model
wandb: Currently logged in as: adeeb-idris (adeeb-idris-coep-technological-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /kaggle/working/Transformer_GPU/project/wandb/run-20260115_083710-g84r77dx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run shakespeare_gqa_20260115_083710
wandb: â­ï¸ View project at https://wandb.ai/adeeb-idris-coep-technological-university/llm-training
wandb: ğŸš€ View run at https://wandb.ai/adeeb-idris-coep-technological-university/llm-training/runs/g84r77dx
WandB initialized: project=llm-training, run=shakespeare_gqa_20260115_083710
[rank0]:W0115 08:37:17.998000 815 torch/_inductor/utils.py:1436] [3/0_1] Not enough SMs to use max_autotune_gemm mode
[rank1]:W0115 08:37:18.502000 816 torch/_inductor/utils.py:1436] [3/0_1] Not enough SMs to use max_autotune_gemm mode
active-> 106052608
MFU: 0.13%
mfu 0.13%
step: 0 | loss:11.1308 | dt:17845.06ms | tok/s:918 | MFU:0.13% | GPU RAM:4.74GB
active-> 106052608
MFU: 0.82%
mfu 0.20%
step: 1 | loss:11.1567 | dt:2736.96ms | tok/s:5,986 | MFU:0.82% | GPU RAM:5.93GB
active-> 106052608
MFU: 0.82%
mfu 0.26%
step: 2 | loss:11.1430 | dt:2746.95ms | tok/s:5,964 | MFU:0.82% | GPU RAM:5.93GB
active-> 106052608
MFU: 0.81%
mfu 0.31%
step: 3 | loss:11.1905 | dt:2767.60ms | tok/s:5,920 | MFU:0.81% | GPU RAM:5.94GB
active-> 106052608
MFU: 0.81%
mfu 0.36%
step: 4 | loss:11.0150 | dt:2778.90ms | tok/s:5,896 | MFU:0.81% | GPU RAM:5.95GB
active-> 106052608
MFU: 0.80%
mfu 0.41%
step: 5 | loss:11.1162 | dt:2799.87ms | tok/s:5,852 | MFU:0.80% | GPU RAM:5.95GB
active-> 106052608
MFU: 0.80%
mfu 0.44%
step: 6 | loss:11.2287 | dt:2819.41ms | tok/s:5,811 | MFU:0.80% | GPU RAM:5.95GB
active-> 106052608
MFU: 0.79%
mfu 0.48%
step: 7 | loss:11.1415 | dt:2828.89ms | tok/s:5,792 | MFU:0.79% | GPU RAM:5.95GB
active-> 106052608
MFU: 0.78%
mfu 0.51%
step: 8 | loss:11.1624 | dt:2863.53ms | tok/s:5,722 | MFU:0.78% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.78%
mfu 0.54%
step: 9 | loss:11.1122 | dt:2883.28ms | tok/s:5,682 | MFU:0.78% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.77%
mfu 0.56%
step: 10 | loss:11.1484 | dt:2931.45ms | tok/s:5,589 | MFU:0.77% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.76%
mfu 0.58%
step: 11 | loss:11.1934 | dt:2959.56ms | tok/s:5,536 | MFU:0.76% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.74%
mfu 0.60%
step: 12 | loss:11.0781 | dt:3043.66ms | tok/s:5,383 | MFU:0.74% | GPU RAM:5.96GB
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                               aten::mm         2.32%     218.834ms         7.12%     672.282ms     128.790us        4.232s        72.72%        4.232s     810.767us          5220   9854735.352  
       autograd::engine::evaluate_function: MmBackward0         0.29%      27.689ms         6.90%     651.822ms     432.817us       0.000us         0.00%        2.844s       1.889ms          1506            --  
                                            MmBackward0         0.37%      34.980ms         6.61%     624.133ms     414.431us       0.000us         0.00%        2.844s       1.889ms          1506            --  
                                           aten::linear         0.30%      28.399ms         4.85%     458.371ms     119.368us       0.000us         0.00%        2.626s     683.892us          3840            --  
                                           forward_pass         0.00%       0.000us         0.00%       0.000us       0.000us        2.207s        37.92%        2.207s     275.863ms             8            --  
                                          ProfilerStep*         0.34%      31.699ms        65.36%        6.171s        3.086s       0.000us         0.00%        1.952s     976.216ms             2            --  
                                           forward_pass         0.20%      18.472ms        24.95%        2.355s     294.434ms       0.000us         0.00%        1.842s     230.220ms             8            --  
                             Torch-Compiled Region: 0/0         0.00%     106.784us        24.75%        2.336s     292.047ms       0.000us         0.00%        1.842s     230.217ms             8            --  
                             Torch-Compiled Region: 1/0         0.72%      67.641ms        24.74%        2.336s     292.000ms       0.000us         0.00%        1.842s     230.217ms             8            --  
void magma_sgemmEx_kernel<float, __nv_bfloat16, __nv...         0.00%       0.000us         0.00%       0.000us       0.000us        1.659s        28.52%        1.659s     908.291us          1827            --  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 9.442s
Self CUDA time total: 5.819s

active-> 106052608
MFU: 0.05%
mfu 0.54%
step: 13 | loss:11.1441 | dt:46668.83ms | tok/s:351 | MFU:0.05% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.79%
mfu 0.57%
step: 14 | loss:11.1937 | dt:2851.78ms | tok/s:5,745 | MFU:0.79% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.80%
mfu 0.59%
step: 15 | loss:10.9215 | dt:2802.67ms | tok/s:5,846 | MFU:0.80% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.79%
mfu 0.61%
step: 16 | loss:11.0910 | dt:2830.94ms | tok/s:5,787 | MFU:0.79% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.79%
mfu 0.63%
step: 17 | loss:11.0677 | dt:2837.06ms | tok/s:5,775 | MFU:0.79% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.78%
mfu 0.64%
step: 18 | loss:11.0008 | dt:2870.79ms | tok/s:5,707 | MFU:0.78% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.78%
mfu 0.66%
step: 19 | loss:10.9528 | dt:2879.36ms | tok/s:5,690 | MFU:0.78% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.78%
mfu 0.67%
step: 20 | loss:11.0095 | dt:2881.54ms | tok/s:5,686 | MFU:0.78% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.78%
mfu 0.68%
step: 21 | loss:11.1116 | dt:2864.05ms | tok/s:5,721 | MFU:0.78% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.79%
mfu 0.69%
step: 22 | loss:10.7234 | dt:2843.89ms | tok/s:5,761 | MFU:0.79% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.79%
mfu 0.70%
step: 23 | loss:10.8913 | dt:2850.71ms | tok/s:5,747 | MFU:0.79% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.31%
mfu 0.66%
step: 24 | loss:10.9793 | dt:7209.25ms | tok/s:2,273 | MFU:0.31% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.78%
mfu 0.67%
step: 25 | loss:10.8886 | dt:2871.61ms | tok/s:5,706 | MFU:0.78% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.81%
mfu 0.69%
step: 26 | loss:10.9800 | dt:2775.44ms | tok/s:5,903 | MFU:0.81% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.81%
mfu 0.70%
step: 27 | loss:10.9466 | dt:2784.23ms | tok/s:5,885 | MFU:0.81% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.81%
mfu 0.71%
step: 28 | loss:10.9005 | dt:2766.16ms | tok/s:5,923 | MFU:0.81% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.80%
mfu 0.72%
step: 29 | loss:10.9632 | dt:2792.67ms | tok/s:5,867 | MFU:0.80% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.81%
mfu 0.73%
step: 30 | loss:10.8456 | dt:2760.79ms | tok/s:5,935 | MFU:0.81% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.81%
mfu 0.74%
step: 31 | loss:10.8305 | dt:2763.87ms | tok/s:5,928 | MFU:0.81% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.81%
mfu 0.74%
step: 32 | loss:10.8553 | dt:2776.95ms | tok/s:5,900 | MFU:0.81% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.81%
mfu 0.75%
step: 33 | loss:10.7533 | dt:2784.64ms | tok/s:5,884 | MFU:0.81% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.81%
mfu 0.76%
step: 34 | loss:10.8007 | dt:2784.61ms | tok/s:5,884 | MFU:0.81% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.80%
mfu 0.76%
step: 35 | loss:10.8733 | dt:2790.34ms | tok/s:5,872 | MFU:0.80% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.80%
mfu 0.76%
step: 36 | loss:10.7106 | dt:2791.73ms | tok/s:5,869 | MFU:0.80% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.80%
mfu 0.77%
step: 37 | loss:10.7705 | dt:2805.70ms | tok/s:5,840 | MFU:0.80% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.80%
mfu 0.77%
step: 38 | loss:10.8687 | dt:2794.88ms | tok/s:5,862 | MFU:0.80% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.80%
mfu 0.77%
step: 39 | loss:10.6092 | dt:2814.49ms | tok/s:5,821 | MFU:0.80% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.80%
mfu 0.78%
step: 40 | loss:10.5863 | dt:2808.69ms | tok/s:5,833 | MFU:0.80% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.80%
mfu 0.78%
step: 41 | loss:10.6600 | dt:2818.38ms | tok/s:5,813 | MFU:0.80% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.80%
mfu 0.78%
step: 42 | loss:10.5335 | dt:2812.77ms | tok/s:5,825 | MFU:0.80% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.80%
mfu 0.78%
step: 43 | loss:10.5947 | dt:2820.52ms | tok/s:5,809 | MFU:0.80% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 44 | loss:10.5066 | dt:2822.98ms | tok/s:5,804 | MFU:0.79% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 45 | loss:10.4206 | dt:2837.27ms | tok/s:5,775 | MFU:0.79% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 46 | loss:10.5883 | dt:2851.52ms | tok/s:5,746 | MFU:0.79% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 47 | loss:10.4693 | dt:2841.58ms | tok/s:5,766 | MFU:0.79% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 48 | loss:10.4223 | dt:2845.46ms | tok/s:5,758 | MFU:0.79% | GPU RAM:5.96GB
active-> 106052608
MFU: 0.31%
mfu 0.74%
step: 49 | loss:10.2104 | dt:7296.76ms | tok/s:2,245 | MFU:0.31% | GPU RAM:5.96GB
--------val run-------- train loss 10.3437 | val loss 10.3695 | dt 61105.4137ms
active-> 106052608
MFU: 0.76%
mfu 0.74%
step: 50 | loss:10.4730 | dt:2969.75ms | tok/s:5,517 | MFU:0.76% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.74%
step: 51 | loss:10.3812 | dt:2830.89ms | tok/s:5,788 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.75%
step: 52 | loss:10.3231 | dt:2817.63ms | tok/s:5,815 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.75%
step: 53 | loss:10.2084 | dt:2822.41ms | tok/s:5,805 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.76%
step: 54 | loss:10.2961 | dt:2825.78ms | tok/s:5,798 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.76%
step: 55 | loss:10.1111 | dt:2836.91ms | tok/s:5,775 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.76%
step: 56 | loss:10.0942 | dt:2844.35ms | tok/s:5,760 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.77%
step: 57 | loss:9.9639 | dt:2840.09ms | tok/s:5,769 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.77%
step: 58 | loss:10.0274 | dt:2834.54ms | tok/s:5,780 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.77%
step: 59 | loss:9.9657 | dt:2828.48ms | tok/s:5,793 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.77%
step: 60 | loss:9.9129 | dt:2825.28ms | tok/s:5,799 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 61 | loss:9.8319 | dt:2835.87ms | tok/s:5,777 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 62 | loss:9.9053 | dt:2830.06ms | tok/s:5,789 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 63 | loss:9.7163 | dt:2833.57ms | tok/s:5,782 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 64 | loss:9.7421 | dt:2827.79ms | tok/s:5,794 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 65 | loss:9.6572 | dt:2835.90ms | tok/s:5,777 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 66 | loss:9.7899 | dt:2821.91ms | tok/s:5,806 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 67 | loss:9.3928 | dt:2828.59ms | tok/s:5,792 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 68 | loss:10.0609 | dt:2831.57ms | tok/s:5,786 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.79%
step: 69 | loss:9.6463 | dt:2833.26ms | tok/s:5,783 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.79%
step: 70 | loss:9.4846 | dt:2835.34ms | tok/s:5,778 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.79%
step: 71 | loss:9.5432 | dt:2825.20ms | tok/s:5,799 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.79%
step: 72 | loss:9.3790 | dt:2827.39ms | tok/s:5,795 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.79%
step: 73 | loss:9.5064 | dt:2828.12ms | tok/s:5,793 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.31%
mfu 0.74%
step: 74 | loss:9.1571 | dt:7321.55ms | tok/s:2,238 | MFU:0.31% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.74%
step: 75 | loss:9.3117 | dt:2845.59ms | tok/s:5,758 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.75%
step: 76 | loss:9.0402 | dt:2840.03ms | tok/s:5,769 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.75%
step: 77 | loss:9.1443 | dt:2827.00ms | tok/s:5,796 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.76%
step: 78 | loss:9.1434 | dt:2818.56ms | tok/s:5,813 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.76%
step: 79 | loss:9.0639 | dt:2820.27ms | tok/s:5,809 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.76%
step: 80 | loss:8.8968 | dt:2826.98ms | tok/s:5,796 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.77%
step: 81 | loss:8.9130 | dt:2836.16ms | tok/s:5,777 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.77%
step: 82 | loss:8.8204 | dt:2839.53ms | tok/s:5,770 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.77%
step: 83 | loss:8.8432 | dt:2841.06ms | tok/s:5,767 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.77%
step: 84 | loss:8.4810 | dt:2854.25ms | tok/s:5,740 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.77%
step: 85 | loss:8.5617 | dt:2836.18ms | tok/s:5,777 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 86 | loss:8.3446 | dt:2849.25ms | tok/s:5,750 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 87 | loss:8.4183 | dt:2832.99ms | tok/s:5,783 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 88 | loss:8.3021 | dt:2848.66ms | tok/s:5,751 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 89 | loss:8.3117 | dt:2828.73ms | tok/s:5,792 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 90 | loss:8.4613 | dt:2849.10ms | tok/s:5,751 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 91 | loss:8.1933 | dt:2839.06ms | tok/s:5,771 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.78%
step: 92 | loss:8.1782 | dt:2819.35ms | tok/s:5,811 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.78%
step: 93 | loss:7.9102 | dt:2812.56ms | tok/s:5,825 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.79%
step: 94 | loss:7.7035 | dt:2811.29ms | tok/s:5,828 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.79%
step: 95 | loss:8.0474 | dt:2819.18ms | tok/s:5,812 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.79%
step: 96 | loss:8.0003 | dt:2795.99ms | tok/s:5,860 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.79%
step: 97 | loss:7.5276 | dt:2793.83ms | tok/s:5,864 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.79%
step: 98 | loss:7.8760 | dt:2795.17ms | tok/s:5,862 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.32%
mfu 0.74%
step: 99 | loss:7.4568 | dt:7044.42ms | tok/s:2,326 | MFU:0.32% | GPU RAM:6.36GB
--------val run-------- train loss 7.5674 | val loss 7.9851 | dt 59878.0024ms
active-> 106052608
MFU: 0.77%
mfu 0.75%
step: 100 | loss:7.3635 | dt:2904.27ms | tok/s:5,641 | MFU:0.77% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.81%
mfu 0.75%
step: 101 | loss:7.4437 | dt:2785.14ms | tok/s:5,883 | MFU:0.81% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.76%
step: 102 | loss:7.1510 | dt:2815.72ms | tok/s:5,819 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.76%
step: 103 | loss:7.2544 | dt:2810.95ms | tok/s:5,829 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.76%
step: 104 | loss:7.1360 | dt:2809.56ms | tok/s:5,832 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.81%
mfu 0.77%
step: 105 | loss:7.5211 | dt:2766.96ms | tok/s:5,921 | MFU:0.81% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.77%
step: 106 | loss:7.2560 | dt:2811.83ms | tok/s:5,827 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.81%
mfu 0.78%
step: 107 | loss:7.0525 | dt:2778.52ms | tok/s:5,897 | MFU:0.81% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.78%
step: 108 | loss:6.8306 | dt:2804.75ms | tok/s:5,842 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.78%
step: 109 | loss:7.1999 | dt:2789.34ms | tok/s:5,874 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.78%
step: 110 | loss:6.8998 | dt:2796.86ms | tok/s:5,858 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.78%
step: 111 | loss:6.5735 | dt:2805.71ms | tok/s:5,840 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.79%
step: 112 | loss:7.0545 | dt:2817.99ms | tok/s:5,814 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.79%
step: 113 | loss:7.1061 | dt:2786.96ms | tok/s:5,879 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.79%
step: 114 | loss:6.4803 | dt:2788.70ms | tok/s:5,875 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.81%
mfu 0.79%
step: 115 | loss:6.8958 | dt:2785.56ms | tok/s:5,882 | MFU:0.81% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.79%
step: 116 | loss:6.9437 | dt:2809.48ms | tok/s:5,832 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.79%
step: 117 | loss:7.0415 | dt:2813.28ms | tok/s:5,824 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.79%
step: 118 | loss:6.6988 | dt:2804.89ms | tok/s:5,841 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.81%
mfu 0.79%
step: 119 | loss:7.1182 | dt:2778.95ms | tok/s:5,896 | MFU:0.81% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.80%
step: 120 | loss:6.7596 | dt:2787.59ms | tok/s:5,877 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.80%
step: 121 | loss:6.7186 | dt:2787.46ms | tok/s:5,878 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.80%
step: 122 | loss:6.4203 | dt:2791.99ms | tok/s:5,868 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.81%
mfu 0.80%
step: 123 | loss:6.5371 | dt:2776.95ms | tok/s:5,900 | MFU:0.81% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.32%
mfu 0.75%
step: 124 | loss:6.8754 | dt:7042.49ms | tok/s:2,326 | MFU:0.32% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.81%
mfu 0.76%
step: 125 | loss:6.7230 | dt:2767.43ms | tok/s:5,920 | MFU:0.81% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.81%
mfu 0.76%
step: 126 | loss:6.6148 | dt:2761.02ms | tok/s:5,934 | MFU:0.81% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.77%
step: 127 | loss:6.9469 | dt:2788.46ms | tok/s:5,876 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.77%
step: 128 | loss:6.5056 | dt:2809.76ms | tok/s:5,831 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.77%
step: 129 | loss:6.7370 | dt:2802.23ms | tok/s:5,847 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.78%
step: 130 | loss:6.4289 | dt:2796.23ms | tok/s:5,859 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 131 | loss:6.1469 | dt:2831.32ms | tok/s:5,787 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 132 | loss:6.4352 | dt:2834.96ms | tok/s:5,779 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 133 | loss:6.2375 | dt:2836.65ms | tok/s:5,776 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 134 | loss:6.5356 | dt:2834.34ms | tok/s:5,781 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 135 | loss:6.5470 | dt:2833.03ms | tok/s:5,783 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 136 | loss:6.3362 | dt:2836.15ms | tok/s:5,777 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.78%
step: 137 | loss:6.0504 | dt:2818.25ms | tok/s:5,814 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.79%
step: 138 | loss:6.1453 | dt:2811.46ms | tok/s:5,828 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.79%
step: 139 | loss:6.5238 | dt:2841.05ms | tok/s:5,767 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.79%
step: 140 | loss:6.3708 | dt:2834.90ms | tok/s:5,779 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.79%
step: 141 | loss:6.6055 | dt:2804.55ms | tok/s:5,842 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.79%
step: 142 | loss:6.2025 | dt:2811.29ms | tok/s:5,828 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.79%
step: 143 | loss:6.5532 | dt:2831.40ms | tok/s:5,787 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.79%
step: 144 | loss:5.9365 | dt:2806.25ms | tok/s:5,838 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.79%
step: 145 | loss:6.5639 | dt:2826.38ms | tok/s:5,797 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.79%
step: 146 | loss:6.8191 | dt:2824.82ms | tok/s:5,800 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.79%
step: 147 | loss:6.0911 | dt:2828.72ms | tok/s:5,792 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.79%
step: 148 | loss:6.4709 | dt:2807.10ms | tok/s:5,837 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.32%
mfu 0.74%
step: 149 | loss:6.8340 | dt:6986.13ms | tok/s:2,345 | MFU:0.32% | GPU RAM:6.36GB
--------val run-------- train loss 6.3388 | val loss 6.9840 | dt 60239.5980ms
active-> 106052608
MFU: 0.77%
mfu 0.75%
step: 150 | loss:6.4861 | dt:2899.06ms | tok/s:5,651 | MFU:0.77% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.75%
step: 151 | loss:6.0074 | dt:2805.39ms | tok/s:5,840 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.76%
step: 152 | loss:6.3539 | dt:2794.99ms | tok/s:5,862 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.76%
step: 153 | loss:6.4534 | dt:2800.60ms | tok/s:5,850 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.77%
step: 154 | loss:6.3036 | dt:2816.99ms | tok/s:5,816 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.77%
step: 155 | loss:6.2476 | dt:2811.39ms | tok/s:5,828 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.77%
step: 156 | loss:6.3386 | dt:2850.79ms | tok/s:5,747 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.77%
step: 157 | loss:5.9261 | dt:2821.34ms | tok/s:5,807 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.77%
step: 158 | loss:5.9378 | dt:2840.12ms | tok/s:5,769 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 159 | loss:5.9473 | dt:2825.95ms | tok/s:5,798 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 160 | loss:6.5262 | dt:2836.15ms | tok/s:5,777 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.78%
step: 161 | loss:6.1845 | dt:2815.71ms | tok/s:5,819 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.78%
step: 162 | loss:6.2697 | dt:2817.03ms | tok/s:5,816 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.78%
step: 163 | loss:6.1044 | dt:2809.60ms | tok/s:5,831 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 164 | loss:6.3167 | dt:2842.65ms | tok/s:5,764 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 165 | loss:6.5016 | dt:2834.69ms | tok/s:5,780 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.79%
step: 166 | loss:5.9230 | dt:2821.03ms | tok/s:5,808 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.79%
step: 167 | loss:6.0584 | dt:2803.74ms | tok/s:5,844 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.79%
step: 168 | loss:5.9475 | dt:2808.83ms | tok/s:5,833 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.79%
step: 169 | loss:5.8893 | dt:2824.96ms | tok/s:5,800 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.79%
step: 170 | loss:6.0421 | dt:2818.07ms | tok/s:5,814 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.79%
step: 171 | loss:5.9125 | dt:2831.41ms | tok/s:5,787 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.79%
step: 172 | loss:5.7858 | dt:2822.53ms | tok/s:5,805 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.79%
step: 173 | loss:6.5991 | dt:2829.78ms | tok/s:5,790 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.32%
mfu 0.74%
step: 174 | loss:6.3252 | dt:7037.80ms | tok/s:2,328 | MFU:0.32% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.75%
step: 175 | loss:6.1921 | dt:2795.06ms | tok/s:5,862 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.75%
step: 176 | loss:6.0829 | dt:2818.03ms | tok/s:5,814 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.76%
step: 177 | loss:6.2731 | dt:2826.98ms | tok/s:5,796 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.76%
step: 178 | loss:6.0574 | dt:2815.65ms | tok/s:5,819 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.76%
step: 179 | loss:6.0445 | dt:2827.30ms | tok/s:5,795 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.77%
step: 180 | loss:6.3051 | dt:2815.84ms | tok/s:5,819 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.77%
step: 181 | loss:6.3907 | dt:2815.60ms | tok/s:5,819 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.77%
step: 182 | loss:6.2473 | dt:2829.14ms | tok/s:5,791 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.77%
step: 183 | loss:6.1007 | dt:2836.65ms | tok/s:5,776 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 184 | loss:6.2194 | dt:2822.55ms | tok/s:5,805 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.78%
step: 185 | loss:6.2745 | dt:2821.03ms | tok/s:5,808 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 186 | loss:5.9954 | dt:2825.31ms | tok/s:5,799 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 187 | loss:5.7600 | dt:2823.87ms | tok/s:5,802 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 188 | loss:5.8282 | dt:2834.79ms | tok/s:5,780 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 189 | loss:6.1243 | dt:2833.06ms | tok/s:5,783 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.78%
step: 190 | loss:6.1447 | dt:2810.86ms | tok/s:5,829 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.79%
step: 191 | loss:6.1534 | dt:2840.47ms | tok/s:5,768 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.79%
step: 192 | loss:6.0435 | dt:2844.33ms | tok/s:5,760 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.78%
mfu 0.79%
step: 193 | loss:6.1950 | dt:2871.14ms | tok/s:5,706 | MFU:0.78% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.79%
step: 194 | loss:6.1970 | dt:2829.93ms | tok/s:5,790 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.79%
step: 195 | loss:6.2898 | dt:2852.57ms | tok/s:5,744 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.79%
step: 196 | loss:5.9272 | dt:2850.69ms | tok/s:5,747 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.79%
step: 197 | loss:5.8812 | dt:2852.03ms | tok/s:5,745 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.79%
step: 198 | loss:6.0784 | dt:2847.47ms | tok/s:5,754 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.32%
mfu 0.74%
step: 199 | loss:6.1078 | dt:7036.82ms | tok/s:2,328 | MFU:0.32% | GPU RAM:6.36GB
--------val run-------- train loss 6.0405 | val loss 6.8302 | dt 60409.4041ms
active-> 106052608
MFU: 0.77%
mfu 0.74%
step: 200 | loss:6.0664 | dt:2916.77ms | tok/s:5,617 | MFU:0.77% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.75%
step: 201 | loss:5.7649 | dt:2826.77ms | tok/s:5,796 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.80%
mfu 0.75%
step: 202 | loss:6.0543 | dt:2812.50ms | tok/s:5,825 | MFU:0.80% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.76%
step: 203 | loss:6.3814 | dt:2826.65ms | tok/s:5,796 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.76%
step: 204 | loss:5.9698 | dt:2832.84ms | tok/s:5,784 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.76%
step: 205 | loss:5.5705 | dt:2822.93ms | tok/s:5,804 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.77%
step: 206 | loss:6.0791 | dt:2828.46ms | tok/s:5,793 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.77%
step: 207 | loss:6.3153 | dt:2835.21ms | tok/s:5,779 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.77%
step: 208 | loss:6.1935 | dt:2850.29ms | tok/s:5,748 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.77%
step: 209 | loss:5.9364 | dt:2844.00ms | tok/s:5,761 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.77%
step: 210 | loss:6.0704 | dt:2836.63ms | tok/s:5,776 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 211 | loss:6.2589 | dt:2836.23ms | tok/s:5,777 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 212 | loss:5.8568 | dt:2835.01ms | tok/s:5,779 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 213 | loss:6.6040 | dt:2851.30ms | tok/s:5,746 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.78%
mfu 0.78%
step: 214 | loss:5.7773 | dt:2870.52ms | tok/s:5,708 | MFU:0.78% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.78%
mfu 0.78%
step: 215 | loss:5.9379 | dt:2869.78ms | tok/s:5,709 | MFU:0.78% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 216 | loss:6.1911 | dt:2856.31ms | tok/s:5,736 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 217 | loss:6.0106 | dt:2845.55ms | tok/s:5,758 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 218 | loss:5.7719 | dt:2838.92ms | tok/s:5,771 | MFU:0.79% | GPU RAM:6.36GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 219 | loss:6.0679 | dt:2829.58ms | tok/s:5,790 | MFU:0.79% | GPU RAM:6.37GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 220 | loss:6.1008 | dt:2846.98ms | tok/s:5,755 | MFU:0.79% | GPU RAM:6.37GB
active-> 106052608
MFU: 0.78%
mfu 0.78%
step: 221 | loss:6.1808 | dt:2862.15ms | tok/s:5,724 | MFU:0.78% | GPU RAM:6.37GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 222 | loss:5.5771 | dt:2845.12ms | tok/s:5,759 | MFU:0.79% | GPU RAM:6.37GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 223 | loss:5.8061 | dt:2849.65ms | tok/s:5,749 | MFU:0.79% | GPU RAM:6.37GB
active-> 106052608
MFU: 0.32%
mfu 0.74%
step: 224 | loss:5.5332 | dt:7037.35ms | tok/s:2,328 | MFU:0.32% | GPU RAM:6.37GB
active-> 106052608
MFU: 0.79%
mfu 0.74%
step: 225 | loss:5.8821 | dt:2832.00ms | tok/s:5,785 | MFU:0.79% | GPU RAM:6.37GB
active-> 106052608
MFU: 0.79%
mfu 0.75%
step: 226 | loss:5.8703 | dt:2831.61ms | tok/s:5,786 | MFU:0.79% | GPU RAM:6.37GB
active-> 106052608
MFU: 0.79%
mfu 0.75%
step: 227 | loss:5.8879 | dt:2831.55ms | tok/s:5,786 | MFU:0.79% | GPU RAM:6.37GB
active-> 106052608
MFU: 0.79%
mfu 0.76%
step: 228 | loss:5.6593 | dt:2837.58ms | tok/s:5,774 | MFU:0.79% | GPU RAM:6.37GB
active-> 106052608
MFU: 0.79%
mfu 0.76%
step: 229 | loss:6.0795 | dt:2845.16ms | tok/s:5,759 | MFU:0.79% | GPU RAM:6.37GB
active-> 106052608
MFU: 0.79%
mfu 0.76%
step: 230 | loss:6.0646 | dt:2851.69ms | tok/s:5,745 | MFU:0.79% | GPU RAM:6.37GB
active-> 106052608
MFU: 0.79%
mfu 0.76%
step: 231 | loss:6.1384 | dt:2852.07ms | tok/s:5,745 | MFU:0.79% | GPU RAM:6.37GB
active-> 106052608
MFU: 0.79%
mfu 0.77%
step: 232 | loss:5.8754 | dt:2843.95ms | tok/s:5,761 | MFU:0.79% | GPU RAM:6.37GB
active-> 106052608
MFU: 0.79%
mfu 0.77%
step: 233 | loss:6.1669 | dt:2842.56ms | tok/s:5,764 | MFU:0.79% | GPU RAM:6.37GB
active-> 106052608
MFU: 0.79%
mfu 0.77%
step: 234 | loss:6.0482 | dt:2853.54ms | tok/s:5,742 | MFU:0.79% | GPU RAM:6.37GB
active-> 106052608
MFU: 0.79%
mfu 0.77%
step: 235 | loss:6.2188 | dt:2847.33ms | tok/s:5,754 | MFU:0.79% | GPU RAM:6.37GB
active-> 106052608
MFU: 0.79%
mfu 0.77%
step: 236 | loss:5.8299 | dt:2847.06ms | tok/s:5,755 | MFU:0.79% | GPU RAM:6.37GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 237 | loss:6.1348 | dt:2848.34ms | tok/s:5,752 | MFU:0.79% | GPU RAM:6.37GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 238 | loss:5.8319 | dt:2854.23ms | tok/s:5,740 | MFU:0.79% | GPU RAM:6.37GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 239 | loss:5.8368 | dt:2842.54ms | tok/s:5,764 | MFU:0.79% | GPU RAM:6.37GB
active-> 106052608
MFU: 0.78%
mfu 0.78%
step: 240 | loss:5.7894 | dt:2864.18ms | tok/s:5,720 | MFU:0.78% | GPU RAM:6.38GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 241 | loss:5.7134 | dt:2851.14ms | tok/s:5,746 | MFU:0.79% | GPU RAM:6.38GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 242 | loss:6.2018 | dt:2854.56ms | tok/s:5,740 | MFU:0.79% | GPU RAM:6.38GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 243 | loss:6.2095 | dt:2839.37ms | tok/s:5,770 | MFU:0.79% | GPU RAM:6.38GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 244 | loss:5.8636 | dt:2843.92ms | tok/s:5,761 | MFU:0.79% | GPU RAM:6.38GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 245 | loss:5.9644 | dt:2847.75ms | tok/s:5,753 | MFU:0.79% | GPU RAM:6.38GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 246 | loss:6.2754 | dt:2840.22ms | tok/s:5,769 | MFU:0.79% | GPU RAM:6.38GB
active-> 106052608
MFU: 0.78%
mfu 0.78%
step: 247 | loss:6.0980 | dt:2868.48ms | tok/s:5,712 | MFU:0.78% | GPU RAM:6.38GB
active-> 106052608
MFU: 0.79%
mfu 0.78%
step: 248 | loss:5.9397 | dt:2844.27ms | tok/s:5,760 | MFU:0.79% | GPU RAM:6.38GB
active-> 106052608
MFU: 0.31%
mfu 0.74%
step: 249 | loss:6.3305 | dt:7145.89ms | tok/s:2,293 | MFU:0.31% | GPU RAM:6.38GB
--------val run-------- train loss 5.9584 | val loss 6.7606 | dt 60572.0300ms
active-> 106052608
MFU: 0.77%
mfu 0.74%
step: 250 | loss:5.8476 | dt:2906.26ms | tok/s:5,637 | MFU:0.77% | GPU RAM:6.38GB

ğŸ“Š PROFILER USAGE GUIDE:

1. Traces are saved to ./profiler_logs/
2. View in Chrome: chrome://tracing (load JSON files)
3. View in TensorBoard: 
   tensorboard --logdir=./profiler_logs
   
4. Key metrics to look for:
   - CUDA kernel launch overhead
   - Memory allocation patterns
   - CPU/GPU utilization
   - Communication overhead (DDP/FSDP)
   - Kernel execution time

wandb: updating run metadata
wandb: 
wandb: Run history:
wandb:                    memory/allocated_gb â–â–â–â–â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–‡â–†â–…â–†â–‡â–‡â–‡â–†â–…â–†â–‡â–†â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                memory/max_allocated_gb â–â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                     memory/reserved_gb â–â–â–â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                 perf/iteration_time_ms â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–ˆâ–ˆâ–â–â–â–â–â–
wandb:                       perf/mfu_percent â–ˆâ–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–
wandb:         perf/throughput_tokens_per_sec â–ˆâ–ˆâ–ƒâ–†â–…â–†â–…â–â–…â–…â–…â–„â–…â–…â–†â–†â–†â–†â–‡â–†â–†â–…â–…â–…â–†â–…â–…â–…â–„â–„â–…â–…â–…â–…â–…â–„â–„â–„â–…â–„
wandb: perf/throughput_tokens_per_sec_per_gpu â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–
wandb:                        train/grad_norm â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–‚â–‚â–‚â–â–‚â–‚â–â–â–â–â–
wandb:                             train/loss â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–ƒâ–ƒâ–‚â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–â–â–‚â–‚
wandb:                               train/lr â–â–â–‚â–‚â–ƒâ–ƒâ–„â–…â–…â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–†â–†â–†â–…â–„â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–
wandb:                                     +1 ...
wandb: 
wandb: Run summary:
wandb:                    memory/allocated_gb 2.82293
wandb:                memory/max_allocated_gb 5.73395
wandb:                     memory/reserved_gb 6.38477
wandb:                 perf/iteration_time_ms 2906.25912
wandb:                       perf/mfu_percent 0.7717
wandb:         perf/throughput_tokens_per_sec 5637.48769
wandb: perf/throughput_tokens_per_sec_per_gpu 2818.74384
wandb:                        train/grad_norm 0.41926
wandb:                             train/loss 5.84759
wandb:                               train/lr 3e-05
wandb:                                     +1 ...
wandb: 
wandb: ğŸš€ View run shakespeare_gqa_20260115_083710 at: https://wandb.ai/adeeb-idris-coep-technological-university/llm-training/runs/g84r77dx
wandb: â­ï¸ View project at: https://wandb.ai/adeeb-idris-coep-technological-university/llm-training
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260115_083710-g84r77dx/logs
WandB run completed

ğŸ“Š PROFILER USAGE GUIDE:

1. Traces are saved to ./profiler_logs/
2. View in Chrome: chrome://tracing (load JSON files)
3. View in TensorBoard: 
   tensorboard --logdir=./profiler_logs
   
4. Key metrics to look for:
   - CUDA kernel launch overhead
   - Memory allocation patterns
   - CPU/GPU utilization
   - Communication overhead (DDP/FSDP)
   - Kernel execution time


Logs saved to: run_5_console_20260115_083658.log and run_5_error_20260115_083658.log

Copying output files for run 5...
  Scanning: /kaggle/working
  Scanning: .
  Scanning: ./run_5_logs
âœ“ Copied 0 files to ./Transformer_GPU/monitor_logs/run_5_20260115_085603

================================================================================
Committing and pushing to GitHub for run 5...
Configuring git identity...
âœ“ Files added to git
âœ“ Committed: Add monitor logs for run 5 - 0 files - 2026-01-15 08:56:03
âœ“ Pushed to GitHub successfully!

âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“
RUN 5 COMPLETED SUCCESSFULLY
Files saved: 0
Pushed to GitHub: âœ“
âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“

================================================================================
WAITING 10 SECONDS BEFORE NEXT RUN...
================================================================================

################################################################################
STARTING RUN 6/8
Time: 08:56:14
################################################################################

train.py updated with parallel_flag = 6
Running torchrun command for i=6...
W0115 08:56:16.249000 937 torch/distributed/run.py:774] 
W0115 08:56:16.249000 937 torch/distributed/run.py:774] *****************************************
W0115 08:56:16.249000 937 torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0115 08:56:16.249000 937 torch/distributed/run.py:774] *****************************************
[W115 08:56:16.294048204 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W115 08:56:16.294745224 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
parallel_flag :  6
parallel_flag :  6
[W115 08:56:20.526859537 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W115 08:56:20.527657667 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W115 08:56:20.527725540 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W115 08:56:20.528337649 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
DDP_WORLD_SIZE = 2
==========================

parallel_flag - parallel_flag -   66

parallel_flag - parallel_flag -   66

parallel_flag - parallel_flag -   66

==========================

total parameters = 186,274,816, active parameters = 110,777,344
Using compiled model
wandb: Currently logged in as: adeeb-idris (adeeb-idris-coep-technological-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /kaggle/working/Transformer_GPU/project/wandb/run-20260115_085626-gy5g94wk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run shakespeare_gqa_20260115_085626
wandb: â­ï¸ View project at https://wandb.ai/adeeb-idris-coep-technological-university/llm-training
wandb: ğŸš€ View run at https://wandb.ai/adeeb-idris-coep-technological-university/llm-training/runs/gy5g94wk
WandB initialized: project=llm-training, run=shakespeare_gqa_20260115_085626
NCCL version 2.27.3+cuda12.9
[rank1]:W0115 08:56:33.571000 945 torch/_inductor/utils.py:1436] [3/0_1] Not enough SMs to use max_autotune_gemm mode
[rank0]:W0115 08:56:33.596000 944 torch/_inductor/utils.py:1436] [3/0_1] Not enough SMs to use max_autotune_gemm mode
active-> 110777344
MFU: 0.15%
mfu 0.15%
step: 0 | loss:11.0531 | dt:15203.13ms | tok/s:1,078 | MFU:0.15% | GPU RAM:5.21GB
active-> 110777344
MFU: 0.76%
mfu 0.21%
step: 1 | loss:11.0627 | dt:3061.07ms | tok/s:5,352 | MFU:0.76% | GPU RAM:6.79GB
active-> 110777344
MFU: 0.75%
mfu 0.27%
step: 2 | loss:11.0778 | dt:3091.24ms | tok/s:5,300 | MFU:0.75% | GPU RAM:6.79GB
active-> 110777344
MFU: 0.75%
mfu 0.32%
step: 3 | loss:11.0457 | dt:3105.40ms | tok/s:5,276 | MFU:0.75% | GPU RAM:6.79GB
active-> 110777344
MFU: 0.74%
mfu 0.36%
step: 4 | loss:11.0642 | dt:3137.57ms | tok/s:5,222 | MFU:0.74% | GPU RAM:6.80GB
active-> 110777344
MFU: 0.73%
mfu 0.40%
step: 5 | loss:11.0059 | dt:3173.60ms | tok/s:5,163 | MFU:0.73% | GPU RAM:6.80GB
active-> 110777344
MFU: 0.73%
mfu 0.43%
step: 6 | loss:11.0365 | dt:3177.53ms | tok/s:5,156 | MFU:0.73% | GPU RAM:6.80GB
active-> 110777344
MFU: 0.73%
mfu 0.46%
step: 7 | loss:11.0445 | dt:3204.11ms | tok/s:5,113 | MFU:0.73% | GPU RAM:6.81GB
active-> 110777344
MFU: 0.72%
mfu 0.49%
step: 8 | loss:11.1085 | dt:3231.72ms | tok/s:5,070 | MFU:0.72% | GPU RAM:6.81GB
active-> 110777344
MFU: 0.72%
mfu 0.51%
step: 9 | loss:11.0703 | dt:3260.89ms | tok/s:5,024 | MFU:0.72% | GPU RAM:6.81GB
active-> 110777344
MFU: 0.71%
mfu 0.53%
step: 10 | loss:10.9789 | dt:3298.35ms | tok/s:4,967 | MFU:0.71% | GPU RAM:6.81GB
active-> 110777344
MFU: 0.70%
mfu 0.55%
step: 11 | loss:11.0453 | dt:3324.48ms | tok/s:4,928 | MFU:0.70% | GPU RAM:6.81GB
active-> 110777344
MFU: 0.69%
mfu 0.56%
step: 12 | loss:11.1250 | dt:3366.46ms | tok/s:4,867 | MFU:0.69% | GPU RAM:6.81GB
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                               aten::mm         2.05%     212.459ms        10.45%        1.084s     215.242us        4.447s        68.21%        4.447s     883.427us          5034  10163972.997  
                                           aten::linear         0.24%      24.715ms         4.13%     427.911ms     117.172us       0.000us         0.00%        2.891s     791.649us          3652            --  
       autograd::engine::evaluate_function: MmBackward0         0.27%      28.363ms         9.35%     970.169ms     643.348us       0.000us         0.00%        2.869s       1.903ms          1508            --  
                                            MmBackward0         0.33%      34.748ms         9.08%     941.806ms     624.540us       0.000us         0.00%        2.869s       1.903ms          1508            --  
                                           forward_pass         0.00%       0.000us         0.00%       0.000us       0.000us        2.453s        37.63%        2.453s     306.683ms             8            --  
                                          ProfilerStep*         0.27%      28.455ms        65.43%        6.788s        3.394s       0.000us         0.00%        2.261s        1.130s             2            --  
                                           forward_pass         0.17%      17.302ms        25.66%        2.662s     332.723ms       0.000us         0.00%        2.146s     268.294ms             8            --  
                             Torch-Compiled Region: 0/0         0.00%     100.163us        25.49%        2.644s     330.485ms       0.000us         0.00%        2.146s     268.291ms             8            --  
                             Torch-Compiled Region: 1/0         0.56%      57.810ms        25.48%        2.644s     330.439ms       0.000us         0.00%        2.146s     268.291ms             8            --  
void magma_sgemmEx_kernel<float, __nv_bfloat16, __nv...         0.00%       0.000us         0.00%       0.000us       0.000us        1.748s        26.81%        1.748s       1.008ms          1735            --  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 10.373s
Self CUDA time total: 6.520s

active-> 110777344
MFU: 0.05%
mfu 0.51%
step: 13 | loss:11.1234 | dt:46206.20ms | tok/s:355 | MFU:0.05% | GPU RAM:6.81GB
active-> 110777344
MFU: 0.74%
mfu 0.53%
step: 14 | loss:11.0028 | dt:3154.18ms | tok/s:5,194 | MFU:0.74% | GPU RAM:6.81GB
active-> 110777344
MFU: 0.76%
mfu 0.56%
step: 15 | loss:10.9536 | dt:3085.57ms | tok/s:5,310 | MFU:0.76% | GPU RAM:6.81GB
active-> 110777344
MFU: 0.75%
mfu 0.57%
step: 16 | loss:10.9962 | dt:3128.29ms | tok/s:5,237 | MFU:0.75% | GPU RAM:6.81GB
active-> 110777344
MFU: 0.74%
mfu 0.59%
step: 17 | loss:10.9451 | dt:3144.71ms | tok/s:5,210 | MFU:0.74% | GPU RAM:6.81GB
active-> 110777344
MFU: 0.73%
mfu 0.61%
step: 18 | loss:10.9736 | dt:3183.35ms | tok/s:5,147 | MFU:0.73% | GPU RAM:6.81GB
active-> 110777344
MFU: 0.73%
mfu 0.62%
step: 19 | loss:11.0099 | dt:3201.06ms | tok/s:5,118 | MFU:0.73% | GPU RAM:6.81GB
active-> 110777344
MFU: 0.73%
mfu 0.63%
step: 20 | loss:10.9535 | dt:3211.81ms | tok/s:5,101 | MFU:0.73% | GPU RAM:6.81GB
active-> 110777344
MFU: 0.73%
mfu 0.64%
step: 21 | loss:10.9074 | dt:3212.42ms | tok/s:5,100 | MFU:0.73% | GPU RAM:6.81GB
active-> 110777344
MFU: 0.73%
mfu 0.65%
step: 22 | loss:10.9219 | dt:3177.93ms | tok/s:5,156 | MFU:0.73% | GPU RAM:6.81GB
active-> 110777344
MFU: 0.74%
mfu 0.66%
step: 23 | loss:10.9116 | dt:3153.38ms | tok/s:5,196 | MFU:0.74% | GPU RAM:6.81GB
active-> 110777344
MFU: 0.31%
mfu 0.62%
step: 24 | loss:10.9392 | dt:7449.01ms | tok/s:2,199 | MFU:0.31% | GPU RAM:6.81GB
active-> 110777344
MFU: 0.74%
mfu 0.63%
step: 25 | loss:10.9350 | dt:3171.93ms | tok/s:5,165 | MFU:0.74% | GPU RAM:6.81GB
active-> 110777344
MFU: 0.76%
mfu 0.65%
step: 26 | loss:10.7875 | dt:3083.77ms | tok/s:5,313 | MFU:0.76% | GPU RAM:6.81GB
active-> 110777344
MFU: 0.76%
mfu 0.66%
step: 27 | loss:10.8838 | dt:3072.34ms | tok/s:5,333 | MFU:0.76% | GPU RAM:6.81GB
active-> 110777344
MFU: 0.76%
mfu 0.67%
step: 28 | loss:10.8175 | dt:3085.53ms | tok/s:5,310 | MFU:0.76% | GPU RAM:6.81GB
active-> 110777344
MFU: 0.76%
mfu 0.68%
step: 29 | loss:10.7957 | dt:3063.35ms | tok/s:5,348 | MFU:0.76% | GPU RAM:6.81GB
active-> 110777344
MFU: 0.76%
mfu 0.68%
step: 30 | loss:10.7390 | dt:3073.33ms | tok/s:5,331 | MFU:0.76% | GPU RAM:6.81GB
active-> 110777344
MFU: 0.76%
mfu 0.69%
step: 31 | loss:10.8045 | dt:3085.39ms | tok/s:5,310 | MFU:0.76% | GPU RAM:6.81GB
active-> 110777344
MFU: 0.75%
mfu 0.70%
step: 32 | loss:10.7158 | dt:3095.35ms | tok/s:5,293 | MFU:0.75% | GPU RAM:6.81GB
active-> 110777344
MFU: 0.76%
mfu 0.70%
step: 33 | loss:10.8368 | dt:3082.99ms | tok/s:5,314 | MFU:0.76% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.71%
step: 34 | loss:10.7779 | dt:3096.13ms | tok/s:5,292 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.71%
step: 35 | loss:10.6300 | dt:3103.43ms | tok/s:5,279 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.72%
step: 36 | loss:10.6653 | dt:3090.55ms | tok/s:5,301 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.72%
step: 37 | loss:10.5626 | dt:3103.52ms | tok/s:5,279 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.72%
step: 38 | loss:10.4508 | dt:3115.40ms | tok/s:5,259 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.73%
step: 39 | loss:10.6446 | dt:3119.34ms | tok/s:5,252 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 40 | loss:10.5822 | dt:3146.75ms | tok/s:5,207 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 41 | loss:10.5520 | dt:3152.51ms | tok/s:5,197 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 42 | loss:10.3873 | dt:3154.14ms | tok/s:5,194 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 43 | loss:10.5182 | dt:3163.07ms | tok/s:5,180 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 44 | loss:10.5871 | dt:3165.21ms | tok/s:5,176 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.73%
mfu 0.73%
step: 45 | loss:10.2731 | dt:3173.37ms | tok/s:5,163 | MFU:0.73% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.73%
mfu 0.73%
step: 46 | loss:10.3455 | dt:3175.88ms | tok/s:5,159 | MFU:0.73% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 47 | loss:10.4608 | dt:3160.58ms | tok/s:5,184 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 48 | loss:10.3497 | dt:3162.61ms | tok/s:5,181 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.31%
mfu 0.69%
step: 49 | loss:10.3818 | dt:7590.92ms | tok/s:2,158 | MFU:0.31% | GPU RAM:6.82GB
--------val run-------- train loss 10.2921 | val loss 10.3090 | dt 67506.7552ms
active-> 110777344
MFU: 0.71%
mfu 0.69%
step: 50 | loss:10.3991 | dt:3291.09ms | tok/s:4,978 | MFU:0.71% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.70%
step: 51 | loss:10.3490 | dt:3146.75ms | tok/s:5,207 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.70%
step: 52 | loss:10.2343 | dt:3147.99ms | tok/s:5,205 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.73%
mfu 0.70%
step: 53 | loss:10.1320 | dt:3178.34ms | tok/s:5,155 | MFU:0.73% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.73%
mfu 0.71%
step: 54 | loss:9.9715 | dt:3194.04ms | tok/s:5,130 | MFU:0.73% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.73%
mfu 0.71%
step: 55 | loss:10.0373 | dt:3191.98ms | tok/s:5,133 | MFU:0.73% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.73%
mfu 0.71%
step: 56 | loss:10.0520 | dt:3209.08ms | tok/s:5,106 | MFU:0.73% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.73%
mfu 0.71%
step: 57 | loss:10.0595 | dt:3211.69ms | tok/s:5,101 | MFU:0.73% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.73%
mfu 0.71%
step: 58 | loss:9.9187 | dt:3211.86ms | tok/s:5,101 | MFU:0.73% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.73%
mfu 0.72%
step: 59 | loss:9.8793 | dt:3203.45ms | tok/s:5,114 | MFU:0.73% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.73%
mfu 0.72%
step: 60 | loss:9.8765 | dt:3183.02ms | tok/s:5,147 | MFU:0.73% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.73%
mfu 0.72%
step: 61 | loss:9.9444 | dt:3190.99ms | tok/s:5,134 | MFU:0.73% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.72%
step: 62 | loss:10.1802 | dt:3163.84ms | tok/s:5,179 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.72%
step: 63 | loss:9.6168 | dt:3171.87ms | tok/s:5,165 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.72%
step: 64 | loss:9.9757 | dt:3142.17ms | tok/s:5,214 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 65 | loss:9.7180 | dt:3149.63ms | tok/s:5,202 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 66 | loss:9.5494 | dt:3132.21ms | tok/s:5,231 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.73%
step: 67 | loss:9.5771 | dt:3128.81ms | tok/s:5,236 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.73%
step: 68 | loss:9.6539 | dt:3119.94ms | tok/s:5,251 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.73%
step: 69 | loss:9.4166 | dt:3126.62ms | tok/s:5,240 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 70 | loss:9.2809 | dt:3130.69ms | tok/s:5,233 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.74%
step: 71 | loss:9.3520 | dt:3120.96ms | tok/s:5,250 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.74%
step: 72 | loss:9.3371 | dt:3128.55ms | tok/s:5,237 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.74%
step: 73 | loss:9.2796 | dt:3110.21ms | tok/s:5,268 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.29%
mfu 0.69%
step: 74 | loss:9.4057 | dt:7924.02ms | tok/s:2,068 | MFU:0.29% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.70%
step: 75 | loss:8.6746 | dt:3169.08ms | tok/s:5,170 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.70%
step: 76 | loss:8.9369 | dt:3136.36ms | tok/s:5,224 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.71%
step: 77 | loss:9.0895 | dt:3167.17ms | tok/s:5,173 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.71%
step: 78 | loss:8.9770 | dt:3135.78ms | tok/s:5,225 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.71%
step: 79 | loss:8.8919 | dt:3136.75ms | tok/s:5,223 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.71%
step: 80 | loss:8.7736 | dt:3171.71ms | tok/s:5,166 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.72%
step: 81 | loss:8.8324 | dt:3150.59ms | tok/s:5,200 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.72%
step: 82 | loss:8.6814 | dt:3169.54ms | tok/s:5,169 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.72%
step: 83 | loss:8.5896 | dt:3171.55ms | tok/s:5,166 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.72%
step: 84 | loss:8.4464 | dt:3145.90ms | tok/s:5,208 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.72%
step: 85 | loss:8.5276 | dt:3170.04ms | tok/s:5,168 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 86 | loss:8.1116 | dt:3142.62ms | tok/s:5,213 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 87 | loss:8.3212 | dt:3155.65ms | tok/s:5,192 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.73%
step: 88 | loss:8.1795 | dt:3122.04ms | tok/s:5,248 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.73%
step: 89 | loss:7.8291 | dt:3114.30ms | tok/s:5,261 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.73%
step: 90 | loss:8.1367 | dt:3129.70ms | tok/s:5,235 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.73%
step: 91 | loss:8.1951 | dt:3106.78ms | tok/s:5,274 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.74%
step: 92 | loss:8.0111 | dt:3118.05ms | tok/s:5,255 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.74%
step: 93 | loss:7.9403 | dt:3115.96ms | tok/s:5,258 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.74%
step: 94 | loss:8.0160 | dt:3115.29ms | tok/s:5,259 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.74%
step: 95 | loss:8.0879 | dt:3110.94ms | tok/s:5,267 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.74%
step: 96 | loss:7.6863 | dt:3111.69ms | tok/s:5,265 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.76%
mfu 0.74%
step: 97 | loss:7.8661 | dt:3084.46ms | tok/s:5,312 | MFU:0.76% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.74%
step: 98 | loss:7.6036 | dt:3098.35ms | tok/s:5,288 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.31%
mfu 0.70%
step: 99 | loss:7.9237 | dt:7606.13ms | tok/s:2,154 | MFU:0.31% | GPU RAM:6.82GB
--------val run-------- train loss 7.5403 | val loss 7.9508 | dt 66828.0931ms
active-> 110777344
MFU: 0.74%
mfu 0.70%
step: 100 | loss:7.5804 | dt:3166.97ms | tok/s:5,173 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.76%
mfu 0.71%
step: 101 | loss:7.5165 | dt:3067.89ms | tok/s:5,340 | MFU:0.76% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.71%
step: 102 | loss:7.2546 | dt:3101.79ms | tok/s:5,282 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.72%
step: 103 | loss:7.1723 | dt:3115.00ms | tok/s:5,260 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.72%
step: 104 | loss:7.4542 | dt:3127.54ms | tok/s:5,239 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.72%
step: 105 | loss:7.2281 | dt:3094.19ms | tok/s:5,295 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.73%
step: 106 | loss:7.2319 | dt:3127.65ms | tok/s:5,238 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.73%
step: 107 | loss:7.2803 | dt:3114.35ms | tok/s:5,261 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 108 | loss:7.8210 | dt:3134.03ms | tok/s:5,228 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.73%
step: 109 | loss:7.2553 | dt:3122.77ms | tok/s:5,247 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.76%
mfu 0.73%
step: 110 | loss:7.1871 | dt:3079.43ms | tok/s:5,320 | MFU:0.76% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.74%
step: 111 | loss:6.8670 | dt:3111.93ms | tok/s:5,265 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.74%
step: 112 | loss:6.9743 | dt:3143.99ms | tok/s:5,211 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.74%
step: 113 | loss:6.7693 | dt:3102.43ms | tok/s:5,281 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.74%
step: 114 | loss:6.9233 | dt:3098.86ms | tok/s:5,287 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.74%
step: 115 | loss:6.7107 | dt:3101.71ms | tok/s:5,282 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.76%
mfu 0.74%
step: 116 | loss:7.0024 | dt:3088.25ms | tok/s:5,305 | MFU:0.76% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.74%
step: 117 | loss:6.9727 | dt:3089.58ms | tok/s:5,303 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.76%
mfu 0.74%
step: 118 | loss:6.9815 | dt:3070.66ms | tok/s:5,336 | MFU:0.76% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.75%
step: 119 | loss:6.8252 | dt:3097.65ms | tok/s:5,289 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.76%
mfu 0.75%
step: 120 | loss:6.8989 | dt:3084.81ms | tok/s:5,311 | MFU:0.76% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.76%
mfu 0.75%
step: 121 | loss:6.7292 | dt:3081.22ms | tok/s:5,317 | MFU:0.76% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.76%
mfu 0.75%
step: 122 | loss:6.7013 | dt:3074.75ms | tok/s:5,329 | MFU:0.76% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.76%
mfu 0.75%
step: 123 | loss:6.5714 | dt:3086.65ms | tok/s:5,308 | MFU:0.76% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.31%
mfu 0.71%
step: 124 | loss:6.6942 | dt:7541.81ms | tok/s:2,172 | MFU:0.31% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.76%
mfu 0.71%
step: 125 | loss:6.9976 | dt:3049.47ms | tok/s:5,373 | MFU:0.76% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.76%
mfu 0.72%
step: 126 | loss:6.4917 | dt:3079.81ms | tok/s:5,320 | MFU:0.76% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.76%
mfu 0.72%
step: 127 | loss:6.5307 | dt:3061.15ms | tok/s:5,352 | MFU:0.76% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.76%
mfu 0.72%
step: 128 | loss:6.3914 | dt:3081.12ms | tok/s:5,318 | MFU:0.76% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.73%
step: 129 | loss:6.8281 | dt:3111.45ms | tok/s:5,266 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.73%
step: 130 | loss:6.6440 | dt:3121.26ms | tok/s:5,249 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.73%
step: 131 | loss:6.3612 | dt:3098.17ms | tok/s:5,288 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 132 | loss:6.5834 | dt:3132.66ms | tok/s:5,230 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.73%
step: 133 | loss:6.4884 | dt:3126.23ms | tok/s:5,241 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 134 | loss:6.5633 | dt:3136.96ms | tok/s:5,223 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.74%
step: 135 | loss:6.1994 | dt:3139.88ms | tok/s:5,218 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.74%
step: 136 | loss:6.6490 | dt:3137.93ms | tok/s:5,221 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.74%
step: 137 | loss:6.0440 | dt:3129.73ms | tok/s:5,235 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.74%
step: 138 | loss:6.1466 | dt:3116.11ms | tok/s:5,258 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.74%
step: 139 | loss:5.8280 | dt:3098.32ms | tok/s:5,288 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.74%
step: 140 | loss:6.5154 | dt:3142.93ms | tok/s:5,213 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.74%
step: 141 | loss:6.7100 | dt:3122.57ms | tok/s:5,247 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.76%
mfu 0.74%
step: 142 | loss:6.4932 | dt:3070.29ms | tok/s:5,336 | MFU:0.76% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.76%
mfu 0.74%
step: 143 | loss:7.0997 | dt:3066.25ms | tok/s:5,343 | MFU:0.76% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.76%
mfu 0.75%
step: 144 | loss:6.0596 | dt:3070.58ms | tok/s:5,336 | MFU:0.76% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.75%
step: 145 | loss:6.2146 | dt:3109.02ms | tok/s:5,270 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.75%
step: 146 | loss:6.0566 | dt:3102.06ms | tok/s:5,282 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.76%
mfu 0.75%
step: 147 | loss:6.4146 | dt:3080.06ms | tok/s:5,319 | MFU:0.76% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.76%
mfu 0.75%
step: 148 | loss:6.1566 | dt:3079.81ms | tok/s:5,320 | MFU:0.76% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.31%
mfu 0.71%
step: 149 | loss:6.3796 | dt:7502.61ms | tok/s:2,184 | MFU:0.31% | GPU RAM:6.82GB
--------val run-------- train loss 6.3006 | val loss 7.0742 | dt 66439.2001ms
active-> 110777344
MFU: 0.75%
mfu 0.71%
step: 150 | loss:6.2050 | dt:3115.64ms | tok/s:5,259 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.76%
mfu 0.71%
step: 151 | loss:6.2461 | dt:3065.23ms | tok/s:5,345 | MFU:0.76% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.76%
mfu 0.72%
step: 152 | loss:6.8865 | dt:3084.69ms | tok/s:5,311 | MFU:0.76% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.72%
step: 153 | loss:6.3162 | dt:3111.54ms | tok/s:5,266 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.72%
step: 154 | loss:6.2956 | dt:3101.59ms | tok/s:5,282 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.73%
step: 155 | loss:6.1912 | dt:3122.25ms | tok/s:5,247 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.73%
step: 156 | loss:6.4115 | dt:3115.12ms | tok/s:5,260 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 157 | loss:6.3486 | dt:3165.13ms | tok/s:5,176 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 158 | loss:6.5775 | dt:3138.21ms | tok/s:5,221 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.73%
step: 159 | loss:6.1650 | dt:3129.49ms | tok/s:5,235 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 160 | loss:6.2290 | dt:3153.13ms | tok/s:5,196 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 161 | loss:6.2014 | dt:3143.84ms | tok/s:5,211 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 162 | loss:6.3940 | dt:3142.11ms | tok/s:5,214 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.74%
step: 163 | loss:6.2472 | dt:3123.53ms | tok/s:5,245 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.74%
step: 164 | loss:6.3080 | dt:3110.38ms | tok/s:5,268 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.74%
step: 165 | loss:5.6539 | dt:3107.48ms | tok/s:5,272 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.74%
step: 166 | loss:6.1869 | dt:3118.49ms | tok/s:5,254 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.74%
step: 167 | loss:6.4482 | dt:3102.12ms | tok/s:5,282 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.76%
mfu 0.74%
step: 168 | loss:6.0658 | dt:3079.60ms | tok/s:5,320 | MFU:0.76% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.76%
mfu 0.74%
step: 169 | loss:5.9619 | dt:3081.03ms | tok/s:5,318 | MFU:0.76% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.74%
step: 170 | loss:6.2854 | dt:3097.98ms | tok/s:5,289 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.75%
step: 171 | loss:6.4274 | dt:3099.66ms | tok/s:5,286 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.75%
step: 172 | loss:5.9677 | dt:3104.51ms | tok/s:5,277 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.75%
step: 173 | loss:5.8416 | dt:3114.05ms | tok/s:5,261 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.32%
mfu 0.70%
step: 174 | loss:6.0898 | dt:7346.74ms | tok/s:2,230 | MFU:0.32% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.71%
step: 175 | loss:6.2845 | dt:3098.17ms | tok/s:5,288 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.71%
step: 176 | loss:5.6546 | dt:3108.27ms | tok/s:5,271 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.72%
step: 177 | loss:6.1533 | dt:3112.54ms | tok/s:5,264 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.72%
step: 178 | loss:6.3516 | dt:3118.43ms | tok/s:5,254 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.72%
step: 179 | loss:6.2302 | dt:3150.29ms | tok/s:5,201 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.72%
step: 180 | loss:6.1031 | dt:3158.58ms | tok/s:5,187 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.72%
step: 181 | loss:6.4926 | dt:3157.41ms | tok/s:5,189 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 182 | loss:6.4649 | dt:3161.76ms | tok/s:5,182 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 183 | loss:5.7645 | dt:3160.38ms | tok/s:5,184 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.73%
mfu 0.73%
step: 184 | loss:5.9843 | dt:3175.91ms | tok/s:5,159 | MFU:0.73% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 185 | loss:6.3568 | dt:3164.18ms | tok/s:5,178 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.73%
mfu 0.73%
step: 186 | loss:6.4299 | dt:3179.03ms | tok/s:5,154 | MFU:0.73% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.73%
mfu 0.73%
step: 187 | loss:6.0772 | dt:3179.91ms | tok/s:5,152 | MFU:0.73% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.73%
mfu 0.73%
step: 188 | loss:6.3560 | dt:3172.91ms | tok/s:5,164 | MFU:0.73% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 189 | loss:5.9598 | dt:3161.88ms | tok/s:5,182 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 190 | loss:6.2350 | dt:3158.74ms | tok/s:5,187 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 191 | loss:6.2092 | dt:3164.60ms | tok/s:5,177 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 192 | loss:6.2250 | dt:3139.13ms | tok/s:5,219 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.73%
step: 193 | loss:6.0683 | dt:3119.54ms | tok/s:5,252 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.74%
step: 194 | loss:5.5246 | dt:3142.36ms | tok/s:5,214 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.74%
step: 195 | loss:6.0892 | dt:3131.52ms | tok/s:5,232 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.74%
step: 196 | loss:5.9274 | dt:3132.80ms | tok/s:5,230 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.74%
step: 197 | loss:6.0148 | dt:3115.45ms | tok/s:5,259 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.74%
step: 198 | loss:6.0266 | dt:3123.10ms | tok/s:5,246 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.31%
mfu 0.70%
step: 199 | loss:5.9970 | dt:7519.17ms | tok/s:2,179 | MFU:0.31% | GPU RAM:6.82GB
--------val run-------- train loss 6.0411 | val loss 6.9188 | dt 66647.4878ms
active-> 110777344
MFU: 0.74%
mfu 0.70%
step: 200 | loss:6.1968 | dt:3146.94ms | tok/s:5,206 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.71%
step: 201 | loss:6.2028 | dt:3114.40ms | tok/s:5,261 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.71%
step: 202 | loss:6.1520 | dt:3107.10ms | tok/s:5,273 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.71%
step: 203 | loss:5.9049 | dt:3133.90ms | tok/s:5,228 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.72%
step: 204 | loss:6.2617 | dt:3148.92ms | tok/s:5,203 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.73%
mfu 0.72%
step: 205 | loss:6.2854 | dt:3188.82ms | tok/s:5,138 | MFU:0.73% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.73%
mfu 0.72%
step: 206 | loss:5.7416 | dt:3196.58ms | tok/s:5,125 | MFU:0.73% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.73%
mfu 0.72%
step: 207 | loss:6.2656 | dt:3190.31ms | tok/s:5,136 | MFU:0.73% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.73%
mfu 0.72%
step: 208 | loss:6.1231 | dt:3200.41ms | tok/s:5,119 | MFU:0.73% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.73%
mfu 0.72%
step: 209 | loss:5.7245 | dt:3213.67ms | tok/s:5,098 | MFU:0.73% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.73%
mfu 0.72%
step: 210 | loss:6.5032 | dt:3206.61ms | tok/s:5,109 | MFU:0.73% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.73%
mfu 0.72%
step: 211 | loss:6.0690 | dt:3183.59ms | tok/s:5,146 | MFU:0.73% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.73%
mfu 0.72%
step: 212 | loss:6.0826 | dt:3193.14ms | tok/s:5,131 | MFU:0.73% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 213 | loss:5.5516 | dt:3156.60ms | tok/s:5,190 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 214 | loss:6.3460 | dt:3159.34ms | tok/s:5,186 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 215 | loss:6.2424 | dt:3162.65ms | tok/s:5,180 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 216 | loss:6.1162 | dt:3145.89ms | tok/s:5,208 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 217 | loss:6.0957 | dt:3155.34ms | tok/s:5,192 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 218 | loss:5.9358 | dt:3132.11ms | tok/s:5,231 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 219 | loss:5.6598 | dt:3149.61ms | tok/s:5,202 | MFU:0.74% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.75%
mfu 0.73%
step: 220 | loss:6.3566 | dt:3128.16ms | tok/s:5,238 | MFU:0.75% | GPU RAM:6.82GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 221 | loss:6.2473 | dt:3140.23ms | tok/s:5,217 | MFU:0.74% | GPU RAM:6.84GB
active-> 110777344
MFU: 0.74%
mfu 0.74%
step: 222 | loss:6.3048 | dt:3137.35ms | tok/s:5,222 | MFU:0.74% | GPU RAM:6.84GB
active-> 110777344
MFU: 0.74%
mfu 0.74%
step: 223 | loss:5.8500 | dt:3149.18ms | tok/s:5,203 | MFU:0.74% | GPU RAM:6.84GB
active-> 110777344
MFU: 0.31%
mfu 0.69%
step: 224 | loss:6.1632 | dt:7439.63ms | tok/s:2,202 | MFU:0.31% | GPU RAM:6.84GB
active-> 110777344
MFU: 0.75%
mfu 0.70%
step: 225 | loss:5.8526 | dt:3112.36ms | tok/s:5,264 | MFU:0.75% | GPU RAM:6.84GB
active-> 110777344
MFU: 0.74%
mfu 0.70%
step: 226 | loss:5.7475 | dt:3133.21ms | tok/s:5,229 | MFU:0.74% | GPU RAM:6.84GB
active-> 110777344
MFU: 0.74%
mfu 0.71%
step: 227 | loss:5.6235 | dt:3137.44ms | tok/s:5,222 | MFU:0.74% | GPU RAM:6.84GB
active-> 110777344
MFU: 0.74%
mfu 0.71%
step: 228 | loss:6.0854 | dt:3163.22ms | tok/s:5,180 | MFU:0.74% | GPU RAM:6.84GB
active-> 110777344
MFU: 0.73%
mfu 0.71%
step: 229 | loss:6.0312 | dt:3175.54ms | tok/s:5,159 | MFU:0.73% | GPU RAM:6.84GB
active-> 110777344
MFU: 0.73%
mfu 0.71%
step: 230 | loss:5.9759 | dt:3196.89ms | tok/s:5,125 | MFU:0.73% | GPU RAM:6.84GB
active-> 110777344
MFU: 0.73%
mfu 0.72%
step: 231 | loss:6.0994 | dt:3215.93ms | tok/s:5,095 | MFU:0.73% | GPU RAM:6.84GB
active-> 110777344
MFU: 0.73%
mfu 0.72%
step: 232 | loss:6.1949 | dt:3208.86ms | tok/s:5,106 | MFU:0.73% | GPU RAM:6.84GB
active-> 110777344
MFU: 0.73%
mfu 0.72%
step: 233 | loss:6.3579 | dt:3215.59ms | tok/s:5,095 | MFU:0.73% | GPU RAM:6.84GB
active-> 110777344
MFU: 0.73%
mfu 0.72%
step: 234 | loss:5.5864 | dt:3209.14ms | tok/s:5,105 | MFU:0.73% | GPU RAM:6.84GB
active-> 110777344
MFU: 0.73%
mfu 0.72%
step: 235 | loss:5.8618 | dt:3209.39ms | tok/s:5,105 | MFU:0.73% | GPU RAM:6.84GB
active-> 110777344
MFU: 0.73%
mfu 0.72%
step: 236 | loss:6.4295 | dt:3206.44ms | tok/s:5,110 | MFU:0.73% | GPU RAM:6.84GB
active-> 110777344
MFU: 0.73%
mfu 0.72%
step: 237 | loss:6.0850 | dt:3189.15ms | tok/s:5,137 | MFU:0.73% | GPU RAM:6.84GB
active-> 110777344
MFU: 0.74%
mfu 0.72%
step: 238 | loss:6.0969 | dt:3169.82ms | tok/s:5,169 | MFU:0.74% | GPU RAM:6.84GB
active-> 110777344
MFU: 0.74%
mfu 0.72%
step: 239 | loss:6.0363 | dt:3163.93ms | tok/s:5,178 | MFU:0.74% | GPU RAM:6.85GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 240 | loss:5.9181 | dt:3162.27ms | tok/s:5,181 | MFU:0.74% | GPU RAM:6.85GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 241 | loss:6.3292 | dt:3164.22ms | tok/s:5,178 | MFU:0.74% | GPU RAM:6.85GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 242 | loss:5.8843 | dt:3162.69ms | tok/s:5,180 | MFU:0.74% | GPU RAM:6.85GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 243 | loss:5.8503 | dt:3133.29ms | tok/s:5,229 | MFU:0.74% | GPU RAM:6.85GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 244 | loss:5.9641 | dt:3142.99ms | tok/s:5,213 | MFU:0.74% | GPU RAM:6.85GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 245 | loss:5.9825 | dt:3136.65ms | tok/s:5,223 | MFU:0.74% | GPU RAM:6.85GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 246 | loss:6.1120 | dt:3148.08ms | tok/s:5,204 | MFU:0.74% | GPU RAM:6.85GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 247 | loss:6.0168 | dt:3138.76ms | tok/s:5,220 | MFU:0.74% | GPU RAM:6.85GB
active-> 110777344
MFU: 0.74%
mfu 0.73%
step: 248 | loss:6.1077 | dt:3143.84ms | tok/s:5,211 | MFU:0.74% | GPU RAM:6.85GB
active-> 110777344
MFU: 0.31%
mfu 0.69%
step: 249 | loss:5.9329 | dt:7480.48ms | tok/s:2,190 | MFU:0.31% | GPU RAM:6.85GB
--------val run-------- train loss 5.9979 | val loss 6.7824 | dt 67315.8075ms
active-> 110777344
MFU: 0.74%
mfu 0.70%
step: 250 | loss:5.5374 | dt:3170.18ms | tok/s:5,168 | MFU:0.74% | GPU RAM:6.85GB
âœ… Training completed successfully
ğŸ“Š PROFILER USAGE GUIDE:

1. Traces are saved to ./profiler_logs/
2. View in Chrome: chrome://tracing (load JSON files)
3. View in TensorBoard: 
   tensorboard --logdir=./profiler_logs
   
4. Key metrics to look for:
   - CUDA kernel launch overhead
   - Memory allocation patterns
   - CPU/GPU utilization
   - Communication overhead (DDP/FSDP)
   - Kernel execution time


wandb: updating run metadata
wandb: uploading history steps 500-501, summary, console lines 1024-1028
wandb: 
wandb: Run history:
wandb:                    memory/allocated_gb â–â–â–â–â–â–â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–…â–„â–‡â–†â–†â–†â–‡â–†â–†â–†â–…â–†â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                memory/max_allocated_gb â–â–â–â–â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb:                     memory/reserved_gb â–â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–…â–‡â–ˆâ–ˆâ–ˆ
wandb:                 perf/iteration_time_ms â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                       perf/mfu_percent â–ˆâ–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆ
wandb:         perf/throughput_tokens_per_sec â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆ
wandb: perf/throughput_tokens_per_sec_per_gpu â–‡â–„â–„â–ƒâ–„â–‡â–„â–„â–â–â–…â–…â–†â–…â–…â–„â–†â–†â–†â–†â–ˆâ–†â–…â–…â–…â–…â–…â–†â–ƒâ–…â–†â–†â–†â–†â–„â–…â–‚â–„â–…â–ƒ
wandb:                        train/grad_norm â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–†â–…â–„â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–‚â–â–â–â–â–â–â–
wandb:                             train/loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–…â–…â–„â–„â–„â–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–
wandb:                               train/lr â–â–â–‚â–‚â–‚â–„â–„â–…â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–…â–…â–…â–„â–„â–ƒâ–‚â–‚â–‚â–‚
wandb:                                     +1 ...
wandb: 
wandb: Run summary:
wandb:                    memory/allocated_gb 3.03093
wandb:                memory/max_allocated_gb 6.40322
wandb:                     memory/reserved_gb 6.8457
wandb:                 perf/iteration_time_ms 3170.18046
wandb:                       perf/mfu_percent 0.73563
wandb:         perf/throughput_tokens_per_sec 5168.16006
wandb: perf/throughput_tokens_per_sec_per_gpu 2584.08003
wandb:                        train/grad_norm 0.40048
wandb:                             train/loss 5.53743
wandb:                               train/lr 3e-05
wandb:                                     +1 ...
wandb: 
wandb: ğŸš€ View run shakespeare_gqa_20260115_085626 at: https://wandb.ai/adeeb-idris-coep-technological-university/llm-training/runs/gy5g94wk
wandb: â­ï¸ View project at: https://wandb.ai/adeeb-idris-coep-technological-university/llm-training
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260115_085626-gy5g94wk/logs
WandB run completed

ğŸ“Š PROFILER USAGE GUIDE:

1. Traces are saved to ./profiler_logs/
2. View in Chrome: chrome://tracing (load JSON files)
3. View in TensorBoard: 
   tensorboard --logdir=./profiler_logs
   
4. Key metrics to look for:
   - CUDA kernel launch overhead
   - Memory allocation patterns
   - CPU/GPU utilization
   - Communication overhead (DDP/FSDP)
   - Kernel execution time


Logs saved to: run_6_console_20260115_085614.log and run_6_error_20260115_085614.log

Copying output files for run 6...
  Scanning: /kaggle/working
  Scanning: .
  Scanning: ./run_6_logs
âœ“ Copied 0 files to ./Transformer_GPU/monitor_logs/run_6_20260115_091708

================================================================================
Committing and pushing to GitHub for run 6...
Configuring git identity...
âœ“ Files added to git
âœ“ Committed: Add monitor logs for run 6 - 0 files - 2026-01-15 09:17:08
âœ“ Pushed to GitHub successfully!

âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“
RUN 6 COMPLETED SUCCESSFULLY
Files saved: 0
Pushed to GitHub: âœ“
âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“

================================================================================
WAITING 10 SECONDS BEFORE NEXT RUN...
================================================================================

################################################################################
STARTING RUN 7/8
Time: 09:17:18
################################################################################

train.py updated with parallel_flag = 7
Running torchrun command for i=7...
W0115 09:17:20.736000 1102 torch/distributed/run.py:774] 
W0115 09:17:20.736000 1102 torch/distributed/run.py:774] *****************************************
W0115 09:17:20.736000 1102 torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0115 09:17:20.736000 1102 torch/distributed/run.py:774] *****************************************
[W115 09:17:20.926835319 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W115 09:17:20.927612831 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
parallel_flag :  7
parallel_flag :  7
[W115 09:17:25.095766201 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W115 09:17:25.096583295 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W115 09:17:25.101278991 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W115 09:17:25.102052209 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
Num GPUs = 2
=============
parallel_flag -  7
parallel_flag -  7
parallel_flag -  7
=============
=============
parallel_flag -  7
parallel_flag -  7
parallel_flag -  7
=============
total parameters = 186,274,816, active parameters = 110,777,344
Using compiled model
wandb: Currently logged in as: adeeb-idris (adeeb-idris-coep-technological-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /kaggle/working/Transformer_GPU/project/wandb/run-20260115_091731-yggpg69m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run shakespeare_gqa_20260115_091731
wandb: â­ï¸ View project at https://wandb.ai/adeeb-idris-coep-technological-university/llm-training
wandb: ğŸš€ View run at https://wandb.ai/adeeb-idris-coep-technological-university/llm-training/runs/yggpg69m
WandB initialized: project=llm-training, run=shakespeare_gqa_20260115_091731

============================================================
PIPELINE PARALLELISM: 2 stages, 8 microbatches
============================================================
Total layers: 12
  Rank 0 (GPU 0): Layers 0-5 (6 layers)
  Rank 1 (GPU 1): Layers 6-11 (6 layers)
============================================================

Total parameters: 186,274,816, Active parameters: 110,777,344

ğŸ” Profiler initialized
Warning: Adjusting chunks from 8 to 2
step: 0 | loss:10.9458 | dt:2031.16ms | tok/s:2,017 | MFU:0.44% | GPU RAM:3.04GB
step: 1 | loss:10.3281 | dt:365.93ms | tok/s:11,193 | MFU:2.42% | GPU RAM:3.13GB
step: 2 | loss:9.7271 | dt:268.80ms | tok/s:15,238 | MFU:3.29% | GPU RAM:3.13GB
step: 3 | loss:9.4289 | dt:249.41ms | tok/s:16,423 | MFU:3.55% | GPU RAM:3.13GB
step: 4 | loss:9.3024 | dt:232.08ms | tok/s:17,649 | MFU:3.81% | GPU RAM:3.13GB
step: 5 | loss:9.0114 | dt:250.38ms | tok/s:16,359 | MFU:3.53% | GPU RAM:3.13GB
step: 6 | loss:8.9913 | dt:237.01ms | tok/s:17,282 | MFU:3.73% | GPU RAM:3.13GB
step: 7 | loss:8.7939 | dt:248.66ms | tok/s:16,472 | MFU:3.56% | GPU RAM:3.13GB
step: 8 | loss:8.6793 | dt:227.13ms | tok/s:18,034 | MFU:3.89% | GPU RAM:3.13GB
step: 9 | loss:8.3962 | dt:229.21ms | tok/s:17,870 | MFU:3.86% | GPU RAM:3.13GB
step: 10 | loss:8.3150 | dt:246.11ms | tok/s:16,643 | MFU:3.59% | GPU RAM:3.13GB
step: 11 | loss:8.1734 | dt:252.09ms | tok/s:16,248 | MFU:3.51% | GPU RAM:3.13GB
step: 12 | loss:8.0013 | dt:314.38ms | tok/s:13,029 | MFU:2.81% | GPU RAM:3.13GB
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                           iteration_12         0.00%       0.000us         0.00%       0.000us       0.000us     313.091ms        82.24%     313.091ms     313.091ms             1            --  
                                           iteration_13         0.00%       0.000us         0.00%       0.000us       0.000us     265.065ms        69.62%     265.065ms     265.065ms             1            --  
                                           warmup_phase         0.00%       0.000us         0.00%       0.000us       0.000us     186.851ms        49.08%     186.851ms      93.426ms             2            --  
                                     record_param_comms         0.21%       2.462ms         0.38%       4.448ms      88.963us     175.953ms        46.22%     175.956ms       3.519ms            50            --  
ncclDevKernel_SendRecv(ncclDevKernelArgsStorage<4096...         0.00%       0.000us         0.00%       0.000us       0.000us     175.910ms        46.21%     175.910ms       7.996ms            22            --  
                                            c10d::recv_         0.01%     163.257us         0.08%     974.093us     162.349us       0.000us         0.00%     174.901ms      29.150ms             6            --  
                                         nccl:recv 0<-1         0.00%       0.000us         0.00%       0.000us       0.000us     174.901ms        45.94%     174.901ms      29.150ms             6            --  
                                         cooldown_phase        24.15%     281.126ms        26.60%     309.632ms     154.816ms       0.000us         0.00%     174.535ms      87.267ms             2            --  
                                           iteration_12         0.17%       1.946ms        27.08%     315.281ms     315.281ms       0.000us         0.00%     157.105ms     157.105ms             1            --  
                                          ProfilerStep*        27.24%     317.040ms        50.03%     582.440ms     291.220ms       0.000us         0.00%     137.121ms      68.561ms             2            --  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 1.164s
Self CUDA time total: 380.712ms

step: 13 | loss:7.8092 | dt:7838.84ms | tok/s:523 | MFU:0.11% | GPU RAM:3.13GB
step: 14 | loss:7.7229 | dt:260.84ms | tok/s:15,703 | MFU:3.39% | GPU RAM:3.13GB
âœ… Pipeline profiling complete! Check ./profiler_logs/ for traces
step: 15 | loss:7.5203 | dt:225.77ms | tok/s:18,142 | MFU:3.92% | GPU RAM:3.13GB
step: 16 | loss:7.5505 | dt:268.62ms | tok/s:15,248 | MFU:3.29% | GPU RAM:3.13GB
step: 17 | loss:7.4399 | dt:227.58ms | tok/s:17,998 | MFU:3.89% | GPU RAM:3.13GB
step: 18 | loss:7.1828 | dt:221.04ms | tok/s:18,531 | MFU:4.00% | GPU RAM:3.13GB
step: 19 | loss:7.0953 | dt:228.52ms | tok/s:17,924 | MFU:3.87% | GPU RAM:3.13GB
step: 20 | loss:7.0353 | dt:232.39ms | tok/s:17,625 | MFU:3.81% | GPU RAM:3.13GB
step: 21 | loss:7.0499 | dt:236.54ms | tok/s:17,316 | MFU:3.74% | GPU RAM:3.13GB
step: 22 | loss:6.6870 | dt:241.59ms | tok/s:16,955 | MFU:3.66% | GPU RAM:3.13GB
step: 23 | loss:6.9290 | dt:247.01ms | tok/s:16,582 | MFU:3.58% | GPU RAM:3.13GB
step: 24 | loss:6.6209 | dt:248.96ms | tok/s:16,452 | MFU:3.55% | GPU RAM:3.13GB
step: 25 | loss:6.3640 | dt:252.85ms | tok/s:16,199 | MFU:3.50% | GPU RAM:3.13GB
step: 26 | loss:6.6348 | dt:255.29ms | tok/s:16,044 | MFU:3.47% | GPU RAM:3.13GB
step: 27 | loss:6.5662 | dt:254.27ms | tok/s:16,109 | MFU:3.48% | GPU RAM:3.13GB
step: 28 | loss:6.7635 | dt:270.49ms | tok/s:15,143 | MFU:3.27% | GPU RAM:3.13GB
step: 29 | loss:6.4683 | dt:256.88ms | tok/s:15,945 | MFU:3.44% | GPU RAM:3.14GB
step: 30 | loss:6.4762 | dt:273.69ms | tok/s:14,966 | MFU:3.23% | GPU RAM:3.14GB
step: 31 | loss:6.4582 | dt:265.80ms | tok/s:15,410 | MFU:3.33% | GPU RAM:3.14GB
step: 32 | loss:6.2319 | dt:269.07ms | tok/s:15,223 | MFU:3.29% | GPU RAM:3.15GB
step: 33 | loss:6.3290 | dt:283.14ms | tok/s:14,467 | MFU:3.12% | GPU RAM:3.15GB
step: 34 | loss:6.3183 | dt:272.93ms | tok/s:15,007 | MFU:3.24% | GPU RAM:3.15GB
step: 35 | loss:6.1150 | dt:256.25ms | tok/s:15,984 | MFU:3.45% | GPU RAM:3.15GB
step: 36 | loss:6.2469 | dt:265.29ms | tok/s:15,440 | MFU:3.33% | GPU RAM:3.15GB
step: 37 | loss:6.2688 | dt:269.69ms | tok/s:15,188 | MFU:3.28% | GPU RAM:3.15GB
step: 38 | loss:6.4189 | dt:262.65ms | tok/s:15,595 | MFU:3.37% | GPU RAM:3.15GB
step: 39 | loss:6.3678 | dt:266.48ms | tok/s:15,371 | MFU:3.32% | GPU RAM:3.15GB
step: 40 | loss:6.4199 | dt:283.47ms | tok/s:14,450 | MFU:3.12% | GPU RAM:3.15GB
step: 41 | loss:6.3068 | dt:287.52ms | tok/s:14,246 | MFU:3.08% | GPU RAM:3.15GB
step: 42 | loss:6.2788 | dt:302.65ms | tok/s:13,534 | MFU:2.92% | GPU RAM:3.15GB
step: 43 | loss:6.2181 | dt:270.23ms | tok/s:15,157 | MFU:3.27% | GPU RAM:3.15GB
step: 44 | loss:6.1520 | dt:266.44ms | tok/s:15,373 | MFU:3.32% | GPU RAM:3.15GB
step: 45 | loss:6.2001 | dt:262.09ms | tok/s:15,628 | MFU:3.38% | GPU RAM:3.15GB
step: 46 | loss:5.9737 | dt:262.34ms | tok/s:15,613 | MFU:3.37% | GPU RAM:3.15GB
step: 47 | loss:6.0936 | dt:260.21ms | tok/s:15,741 | MFU:3.40% | GPU RAM:3.15GB
step: 48 | loss:6.0203 | dt:300.33ms | tok/s:13,639 | MFU:2.95% | GPU RAM:3.15GB
step: 49 | loss:6.1718 | dt:330.63ms | tok/s:12,389 | MFU:2.68% | GPU RAM:3.16GB
step: 50 | loss:6.0312 | dt:266.79ms | tok/s:15,353 | MFU:3.32% | GPU RAM:3.16GB
step: 51 | loss:6.0021 | dt:285.34ms | tok/s:14,355 | MFU:3.10% | GPU RAM:3.16GB
step: 52 | loss:6.3035 | dt:281.79ms | tok/s:14,536 | MFU:3.14% | GPU RAM:3.16GB
step: 53 | loss:5.9118 | dt:268.45ms | tok/s:15,258 | MFU:3.30% | GPU RAM:3.16GB
step: 54 | loss:6.1751 | dt:266.20ms | tok/s:15,387 | MFU:3.32% | GPU RAM:3.16GB
step: 55 | loss:6.3291 | dt:265.09ms | tok/s:15,451 | MFU:3.34% | GPU RAM:3.16GB
step: 56 | loss:5.9095 | dt:269.25ms | tok/s:15,213 | MFU:3.29% | GPU RAM:3.16GB
step: 57 | loss:6.2208 | dt:315.53ms | tok/s:12,981 | MFU:2.80% | GPU RAM:3.16GB
step: 58 | loss:5.8568 | dt:322.16ms | tok/s:12,714 | MFU:2.75% | GPU RAM:3.17GB
step: 59 | loss:6.2555 | dt:291.03ms | tok/s:14,074 | MFU:3.04% | GPU RAM:3.18GB
step: 60 | loss:6.1034 | dt:291.95ms | tok/s:14,030 | MFU:3.03% | GPU RAM:3.18GB
step: 61 | loss:5.9446 | dt:302.43ms | tok/s:13,543 | MFU:2.93% | GPU RAM:3.18GB
step: 62 | loss:5.7438 | dt:292.29ms | tok/s:14,013 | MFU:3.03% | GPU RAM:3.20GB
step: 63 | loss:6.1331 | dt:286.10ms | tok/s:14,317 | MFU:3.09% | GPU RAM:3.20GB
step: 64 | loss:5.9301 | dt:296.52ms | tok/s:13,813 | MFU:2.98% | GPU RAM:3.20GB
step: 65 | loss:6.1354 | dt:301.01ms | tok/s:13,607 | MFU:2.94% | GPU RAM:3.20GB
step: 66 | loss:5.7267 | dt:295.71ms | tok/s:13,851 | MFU:2.99% | GPU RAM:3.20GB
step: 67 | loss:6.0414 | dt:281.48ms | tok/s:14,552 | MFU:3.14% | GPU RAM:3.20GB
step: 68 | loss:5.6242 | dt:284.12ms | tok/s:14,416 | MFU:3.11% | GPU RAM:3.20GB
step: 69 | loss:6.0602 | dt:295.99ms | tok/s:13,838 | MFU:2.99% | GPU RAM:3.20GB
step: 70 | loss:5.6982 | dt:283.02ms | tok/s:14,473 | MFU:3.13% | GPU RAM:3.20GB
step: 71 | loss:6.0472 | dt:283.77ms | tok/s:14,434 | MFU:3.12% | GPU RAM:3.20GB
step: 72 | loss:5.7856 | dt:278.26ms | tok/s:14,720 | MFU:3.18% | GPU RAM:3.20GB
step: 73 | loss:5.8696 | dt:280.92ms | tok/s:14,581 | MFU:3.15% | GPU RAM:3.20GB
step: 74 | loss:5.7912 | dt:291.25ms | tok/s:14,064 | MFU:3.04% | GPU RAM:3.20GB
step: 75 | loss:5.9529 | dt:277.08ms | tok/s:14,783 | MFU:3.19% | GPU RAM:3.20GB
step: 76 | loss:5.6401 | dt:279.26ms | tok/s:14,667 | MFU:3.17% | GPU RAM:3.20GB
step: 77 | loss:5.7439 | dt:277.54ms | tok/s:14,758 | MFU:3.19% | GPU RAM:3.20GB
step: 78 | loss:5.7946 | dt:279.48ms | tok/s:14,656 | MFU:3.17% | GPU RAM:3.20GB
step: 79 | loss:5.7036 | dt:277.00ms | tok/s:14,787 | MFU:3.19% | GPU RAM:3.20GB
step: 80 | loss:5.4275 | dt:274.45ms | tok/s:14,924 | MFU:3.22% | GPU RAM:3.21GB
step: 81 | loss:5.4956 | dt:284.45ms | tok/s:14,400 | MFU:3.11% | GPU RAM:3.21GB
step: 82 | loss:5.9809 | dt:278.82ms | tok/s:14,691 | MFU:3.17% | GPU RAM:3.21GB
step: 83 | loss:5.5149 | dt:294.18ms | tok/s:13,923 | MFU:3.01% | GPU RAM:3.21GB
step: 84 | loss:5.5383 | dt:314.44ms | tok/s:13,027 | MFU:2.81% | GPU RAM:3.21GB
step: 85 | loss:5.5442 | dt:283.00ms | tok/s:14,474 | MFU:3.13% | GPU RAM:3.21GB
step: 86 | loss:5.6472 | dt:287.52ms | tok/s:14,246 | MFU:3.08% | GPU RAM:3.21GB
step: 87 | loss:5.9016 | dt:282.02ms | tok/s:14,524 | MFU:3.14% | GPU RAM:3.21GB
step: 88 | loss:5.5957 | dt:279.35ms | tok/s:14,663 | MFU:3.17% | GPU RAM:3.21GB
step: 89 | loss:6.0417 | dt:276.71ms | tok/s:14,802 | MFU:3.20% | GPU RAM:3.21GB
step: 90 | loss:5.6699 | dt:277.01ms | tok/s:14,786 | MFU:3.19% | GPU RAM:3.21GB
step: 91 | loss:5.8005 | dt:307.73ms | tok/s:13,310 | MFU:2.87% | GPU RAM:3.21GB
step: 92 | loss:5.7258 | dt:288.33ms | tok/s:14,206 | MFU:3.07% | GPU RAM:3.21GB
step: 93 | loss:5.7495 | dt:283.63ms | tok/s:14,441 | MFU:3.12% | GPU RAM:3.21GB
step: 94 | loss:5.6080 | dt:287.29ms | tok/s:14,257 | MFU:3.08% | GPU RAM:3.21GB
step: 95 | loss:5.6189 | dt:281.49ms | tok/s:14,551 | MFU:3.14% | GPU RAM:3.21GB
step: 96 | loss:5.6734 | dt:282.90ms | tok/s:14,479 | MFU:3.13% | GPU RAM:3.22GB
step: 97 | loss:5.4093 | dt:286.69ms | tok/s:14,287 | MFU:3.09% | GPU RAM:3.23GB
step: 98 | loss:5.5899 | dt:277.55ms | tok/s:14,758 | MFU:3.19% | GPU RAM:3.23GB
step: 99 | loss:5.6879 | dt:278.21ms | tok/s:14,723 | MFU:3.18% | GPU RAM:3.23GB
step: 100 | loss:5.6496 | dt:278.51ms | tok/s:14,707 | MFU:3.18% | GPU RAM:3.23GB
step: 101 | loss:5.4063 | dt:278.18ms | tok/s:14,724 | MFU:3.18% | GPU RAM:3.23GB
step: 102 | loss:5.5592 | dt:275.96ms | tok/s:14,842 | MFU:3.21% | GPU RAM:3.23GB
step: 103 | loss:5.9545 | dt:273.78ms | tok/s:14,961 | MFU:3.23% | GPU RAM:3.23GB
step: 104 | loss:5.6179 | dt:278.33ms | tok/s:14,716 | MFU:3.18% | GPU RAM:3.23GB
step: 105 | loss:5.4767 | dt:275.41ms | tok/s:14,872 | MFU:3.21% | GPU RAM:3.23GB
step: 106 | loss:5.8234 | dt:278.96ms | tok/s:14,683 | MFU:3.17% | GPU RAM:3.23GB
step: 107 | loss:5.5355 | dt:287.86ms | tok/s:14,229 | MFU:3.07% | GPU RAM:3.23GB
step: 108 | loss:5.2935 | dt:279.50ms | tok/s:14,654 | MFU:3.16% | GPU RAM:3.23GB
step: 109 | loss:5.4234 | dt:281.28ms | tok/s:14,562 | MFU:3.15% | GPU RAM:3.23GB
step: 110 | loss:5.3336 | dt:287.65ms | tok/s:14,239 | MFU:3.08% | GPU RAM:3.23GB
step: 111 | loss:5.3585 | dt:287.79ms | tok/s:14,233 | MFU:3.07% | GPU RAM:3.23GB
step: 112 | loss:5.7888 | dt:289.47ms | tok/s:14,150 | MFU:3.06% | GPU RAM:3.23GB
step: 113 | loss:5.4405 | dt:292.69ms | tok/s:13,994 | MFU:3.02% | GPU RAM:3.23GB
step: 114 | loss:5.6051 | dt:286.39ms | tok/s:14,302 | MFU:3.09% | GPU RAM:3.23GB
step: 115 | loss:5.5299 | dt:283.81ms | tok/s:14,432 | MFU:3.12% | GPU RAM:3.23GB
step: 116 | loss:5.6238 | dt:281.25ms | tok/s:14,564 | MFU:3.15% | GPU RAM:3.23GB
step: 117 | loss:5.2425 | dt:305.95ms | tok/s:13,388 | MFU:2.89% | GPU RAM:3.23GB
step: 118 | loss:5.6811 | dt:302.96ms | tok/s:13,520 | MFU:2.92% | GPU RAM:3.23GB
step: 119 | loss:5.3995 | dt:320.93ms | tok/s:12,763 | MFU:2.76% | GPU RAM:3.23GB
step: 120 | loss:5.4198 | dt:280.46ms | tok/s:14,604 | MFU:3.15% | GPU RAM:3.23GB
step: 121 | loss:5.4037 | dt:282.73ms | tok/s:14,488 | MFU:3.13% | GPU RAM:3.23GB
step: 122 | loss:5.6813 | dt:284.89ms | tok/s:14,378 | MFU:3.11% | GPU RAM:3.23GB
step: 123 | loss:5.5037 | dt:276.02ms | tok/s:14,840 | MFU:3.20% | GPU RAM:3.23GB
step: 124 | loss:5.7779 | dt:279.50ms | tok/s:14,655 | MFU:3.17% | GPU RAM:3.23GB
step: 125 | loss:5.7392 | dt:288.48ms | tok/s:14,199 | MFU:3.07% | GPU RAM:3.23GB
step: 126 | loss:4.9200 | dt:281.19ms | tok/s:14,567 | MFU:3.15% | GPU RAM:3.23GB
step: 127 | loss:5.4033 | dt:281.83ms | tok/s:14,534 | MFU:3.14% | GPU RAM:3.23GB
step: 128 | loss:5.1813 | dt:280.50ms | tok/s:14,602 | MFU:3.15% | GPU RAM:3.23GB
step: 129 | loss:5.5043 | dt:280.62ms | tok/s:14,596 | MFU:3.15% | GPU RAM:3.23GB
step: 130 | loss:5.6323 | dt:290.47ms | tok/s:14,102 | MFU:3.05% | GPU RAM:3.23GB
step: 131 | loss:5.5160 | dt:280.82ms | tok/s:14,586 | MFU:3.15% | GPU RAM:3.23GB
step: 132 | loss:5.4503 | dt:279.42ms | tok/s:14,659 | MFU:3.17% | GPU RAM:3.23GB
step: 133 | loss:5.1456 | dt:278.18ms | tok/s:14,724 | MFU:3.18% | GPU RAM:3.23GB
step: 134 | loss:5.2048 | dt:276.64ms | tok/s:14,806 | MFU:3.20% | GPU RAM:3.23GB
step: 135 | loss:5.4093 | dt:281.87ms | tok/s:14,531 | MFU:3.14% | GPU RAM:3.23GB
step: 136 | loss:5.2006 | dt:285.80ms | tok/s:14,332 | MFU:3.10% | GPU RAM:3.23GB
step: 137 | loss:5.5501 | dt:290.33ms | tok/s:14,108 | MFU:3.05% | GPU RAM:3.23GB
step: 138 | loss:5.3030 | dt:294.76ms | tok/s:13,896 | MFU:3.00% | GPU RAM:3.23GB
step: 139 | loss:5.6012 | dt:290.35ms | tok/s:14,107 | MFU:3.05% | GPU RAM:3.23GB
step: 140 | loss:5.4138 | dt:296.43ms | tok/s:13,818 | MFU:2.98% | GPU RAM:3.23GB
step: 141 | loss:5.6905 | dt:287.64ms | tok/s:14,240 | MFU:3.08% | GPU RAM:3.23GB
step: 142 | loss:5.2984 | dt:290.67ms | tok/s:14,092 | MFU:3.04% | GPU RAM:3.23GB
step: 143 | loss:5.0933 | dt:289.22ms | tok/s:14,162 | MFU:3.06% | GPU RAM:3.23GB
step: 144 | loss:5.4259 | dt:287.87ms | tok/s:14,228 | MFU:3.07% | GPU RAM:3.23GB
step: 145 | loss:5.4377 | dt:287.77ms | tok/s:14,233 | MFU:3.07% | GPU RAM:3.23GB
step: 146 | loss:5.6278 | dt:284.76ms | tok/s:14,384 | MFU:3.11% | GPU RAM:3.23GB
step: 147 | loss:5.3460 | dt:284.24ms | tok/s:14,410 | MFU:3.11% | GPU RAM:3.23GB
step: 148 | loss:5.3970 | dt:293.78ms | tok/s:13,943 | MFU:3.01% | GPU RAM:3.23GB
step: 149 | loss:5.2298 | dt:285.92ms | tok/s:14,326 | MFU:3.09% | GPU RAM:3.23GB
step: 150 | loss:4.9260 | dt:279.94ms | tok/s:14,632 | MFU:3.16% | GPU RAM:3.23GB
step: 151 | loss:5.3903 | dt:284.00ms | tok/s:14,422 | MFU:3.11% | GPU RAM:3.23GB
step: 152 | loss:5.0794 | dt:295.01ms | tok/s:13,884 | MFU:3.00% | GPU RAM:3.23GB
step: 153 | loss:5.4853 | dt:322.82ms | tok/s:12,688 | MFU:2.74% | GPU RAM:3.23GB
step: 154 | loss:5.0655 | dt:282.80ms | tok/s:14,484 | MFU:3.13% | GPU RAM:3.23GB
step: 155 | loss:5.2902 | dt:279.52ms | tok/s:14,653 | MFU:3.16% | GPU RAM:3.23GB
step: 156 | loss:5.2031 | dt:290.31ms | tok/s:14,109 | MFU:3.05% | GPU RAM:3.23GB
step: 157 | loss:5.0668 | dt:294.27ms | tok/s:13,919 | MFU:3.01% | GPU RAM:3.23GB
step: 158 | loss:4.8602 | dt:294.65ms | tok/s:13,901 | MFU:3.00% | GPU RAM:3.24GB
step: 159 | loss:5.7466 | dt:296.58ms | tok/s:13,811 | MFU:2.98% | GPU RAM:3.24GB
step: 160 | loss:5.0571 | dt:293.01ms | tok/s:13,979 | MFU:3.02% | GPU RAM:3.24GB
step: 161 | loss:5.0418 | dt:297.16ms | tok/s:13,784 | MFU:2.98% | GPU RAM:3.24GB
step: 162 | loss:4.9663 | dt:317.22ms | tok/s:12,912 | MFU:2.79% | GPU RAM:3.24GB
step: 163 | loss:5.2354 | dt:306.02ms | tok/s:13,385 | MFU:2.89% | GPU RAM:3.24GB
step: 164 | loss:5.0820 | dt:290.39ms | tok/s:14,105 | MFU:3.05% | GPU RAM:3.24GB
step: 165 | loss:5.0681 | dt:291.56ms | tok/s:14,048 | MFU:3.03% | GPU RAM:3.24GB
step: 166 | loss:5.0380 | dt:289.51ms | tok/s:14,148 | MFU:3.06% | GPU RAM:3.24GB
step: 167 | loss:4.7749 | dt:308.25ms | tok/s:13,288 | MFU:2.87% | GPU RAM:3.24GB
step: 168 | loss:5.3533 | dt:293.52ms | tok/s:13,955 | MFU:3.01% | GPU RAM:3.24GB
step: 169 | loss:4.8983 | dt:286.14ms | tok/s:14,315 | MFU:3.09% | GPU RAM:3.24GB
step: 170 | loss:5.3167 | dt:289.39ms | tok/s:14,154 | MFU:3.06% | GPU RAM:3.24GB
step: 171 | loss:4.7613 | dt:287.43ms | tok/s:14,251 | MFU:3.08% | GPU RAM:3.24GB
step: 172 | loss:5.1923 | dt:288.35ms | tok/s:14,205 | MFU:3.07% | GPU RAM:3.24GB
step: 173 | loss:4.7272 | dt:280.09ms | tok/s:14,624 | MFU:3.16% | GPU RAM:3.24GB
step: 174 | loss:4.8975 | dt:284.68ms | tok/s:14,388 | MFU:3.11% | GPU RAM:3.24GB
step: 175 | loss:5.0050 | dt:282.65ms | tok/s:14,491 | MFU:3.13% | GPU RAM:3.24GB
step: 176 | loss:4.7670 | dt:286.52ms | tok/s:14,296 | MFU:3.09% | GPU RAM:3.24GB
step: 177 | loss:5.0076 | dt:289.69ms | tok/s:14,139 | MFU:3.05% | GPU RAM:3.24GB
step: 178 | loss:5.1935 | dt:280.44ms | tok/s:14,606 | MFU:3.15% | GPU RAM:3.24GB
step: 179 | loss:4.7789 | dt:281.82ms | tok/s:14,534 | MFU:3.14% | GPU RAM:3.24GB
step: 180 | loss:4.8742 | dt:283.10ms | tok/s:14,468 | MFU:3.12% | GPU RAM:3.24GB
step: 181 | loss:4.9728 | dt:284.02ms | tok/s:14,421 | MFU:3.11% | GPU RAM:3.24GB
step: 182 | loss:5.1773 | dt:283.82ms | tok/s:14,432 | MFU:3.12% | GPU RAM:3.24GB
step: 183 | loss:5.1961 | dt:298.36ms | tok/s:13,728 | MFU:2.97% | GPU RAM:3.24GB
step: 184 | loss:5.2269 | dt:289.23ms | tok/s:14,162 | MFU:3.06% | GPU RAM:3.24GB
step: 185 | loss:4.8079 | dt:286.87ms | tok/s:14,278 | MFU:3.08% | GPU RAM:3.24GB
step: 186 | loss:4.6941 | dt:278.70ms | tok/s:14,697 | MFU:3.17% | GPU RAM:3.24GB
step: 187 | loss:5.0092 | dt:289.44ms | tok/s:14,152 | MFU:3.06% | GPU RAM:3.24GB
step: 188 | loss:4.6860 | dt:311.15ms | tok/s:13,164 | MFU:2.84% | GPU RAM:3.24GB
step: 189 | loss:5.0452 | dt:282.13ms | tok/s:14,518 | MFU:3.14% | GPU RAM:3.24GB
step: 190 | loss:4.5232 | dt:275.56ms | tok/s:14,864 | MFU:3.21% | GPU RAM:3.24GB
step: 191 | loss:4.3923 | dt:285.14ms | tok/s:14,365 | MFU:3.10% | GPU RAM:3.24GB
step: 192 | loss:4.7220 | dt:284.56ms | tok/s:14,394 | MFU:3.11% | GPU RAM:3.24GB
step: 193 | loss:4.8855 | dt:280.04ms | tok/s:14,626 | MFU:3.16% | GPU RAM:3.24GB
step: 194 | loss:4.9743 | dt:278.45ms | tok/s:14,710 | MFU:3.18% | GPU RAM:3.24GB
step: 195 | loss:5.1048 | dt:286.89ms | tok/s:14,277 | MFU:3.08% | GPU RAM:3.24GB
step: 196 | loss:4.9260 | dt:280.03ms | tok/s:14,627 | MFU:3.16% | GPU RAM:3.24GB
step: 197 | loss:4.6931 | dt:282.78ms | tok/s:14,485 | MFU:3.13% | GPU RAM:3.24GB
step: 198 | loss:5.0896 | dt:281.26ms | tok/s:14,563 | MFU:3.15% | GPU RAM:3.24GB
step: 199 | loss:4.8076 | dt:301.46ms | tok/s:13,587 | MFU:2.93% | GPU RAM:3.24GB
step: 200 | loss:4.8041 | dt:299.89ms | tok/s:13,658 | MFU:2.95% | GPU RAM:3.24GB
step: 201 | loss:4.8739 | dt:298.76ms | tok/s:13,710 | MFU:2.96% | GPU RAM:3.24GB
step: 202 | loss:4.0390 | dt:289.66ms | tok/s:14,141 | MFU:3.05% | GPU RAM:3.24GB
step: 203 | loss:4.9943 | dt:291.48ms | tok/s:14,052 | MFU:3.03% | GPU RAM:3.24GB
step: 204 | loss:4.7416 | dt:297.99ms | tok/s:13,745 | MFU:2.97% | GPU RAM:3.24GB
step: 205 | loss:4.9305 | dt:292.11ms | tok/s:14,022 | MFU:3.03% | GPU RAM:3.24GB
step: 206 | loss:4.7636 | dt:284.09ms | tok/s:14,418 | MFU:3.11% | GPU RAM:3.24GB
step: 207 | loss:4.9710 | dt:308.58ms | tok/s:13,274 | MFU:2.87% | GPU RAM:3.24GB
step: 208 | loss:4.7838 | dt:278.59ms | tok/s:14,703 | MFU:3.18% | GPU RAM:3.24GB
step: 209 | loss:4.9063 | dt:288.85ms | tok/s:14,180 | MFU:3.06% | GPU RAM:3.24GB
step: 210 | loss:4.7468 | dt:287.86ms | tok/s:14,229 | MFU:3.07% | GPU RAM:3.24GB
step: 211 | loss:4.7462 | dt:286.41ms | tok/s:14,301 | MFU:3.09% | GPU RAM:3.24GB
step: 212 | loss:5.1770 | dt:288.86ms | tok/s:14,180 | MFU:3.06% | GPU RAM:3.24GB
step: 213 | loss:4.8008 | dt:291.33ms | tok/s:14,060 | MFU:3.04% | GPU RAM:3.24GB
step: 214 | loss:4.7010 | dt:291.70ms | tok/s:14,042 | MFU:3.03% | GPU RAM:3.24GB
step: 215 | loss:4.7769 | dt:280.21ms | tok/s:14,618 | MFU:3.16% | GPU RAM:3.24GB
step: 216 | loss:5.0583 | dt:295.98ms | tok/s:13,839 | MFU:2.99% | GPU RAM:3.24GB
step: 217 | loss:5.0448 | dt:305.12ms | tok/s:13,424 | MFU:2.90% | GPU RAM:3.24GB
step: 218 | loss:4.6056 | dt:299.59ms | tok/s:13,672 | MFU:2.95% | GPU RAM:3.24GB
step: 219 | loss:4.2844 | dt:287.09ms | tok/s:14,267 | MFU:3.08% | GPU RAM:3.24GB
step: 220 | loss:4.5163 | dt:297.98ms | tok/s:13,746 | MFU:2.97% | GPU RAM:3.24GB
step: 221 | loss:4.8412 | dt:312.76ms | tok/s:13,096 | MFU:2.83% | GPU RAM:3.24GB
step: 222 | loss:5.1955 | dt:316.34ms | tok/s:12,948 | MFU:2.80% | GPU RAM:3.24GB
step: 223 | loss:5.0047 | dt:285.22ms | tok/s:14,361 | MFU:3.10% | GPU RAM:3.24GB
step: 224 | loss:4.6632 | dt:281.44ms | tok/s:14,554 | MFU:3.14% | GPU RAM:3.24GB
step: 225 | loss:4.3602 | dt:286.42ms | tok/s:14,301 | MFU:3.09% | GPU RAM:3.24GB
step: 226 | loss:4.6572 | dt:282.55ms | tok/s:14,496 | MFU:3.13% | GPU RAM:3.24GB
step: 227 | loss:5.0488 | dt:279.43ms | tok/s:14,658 | MFU:3.17% | GPU RAM:3.24GB
step: 228 | loss:4.7739 | dt:280.44ms | tok/s:14,606 | MFU:3.15% | GPU RAM:3.24GB
step: 229 | loss:4.6343 | dt:285.22ms | tok/s:14,361 | MFU:3.10% | GPU RAM:3.24GB
step: 230 | loss:4.8201 | dt:278.66ms | tok/s:14,699 | MFU:3.17% | GPU RAM:3.24GB
step: 231 | loss:4.9229 | dt:285.27ms | tok/s:14,358 | MFU:3.10% | GPU RAM:3.24GB
step: 232 | loss:4.8047 | dt:280.57ms | tok/s:14,599 | MFU:3.15% | GPU RAM:3.24GB
step: 233 | loss:4.6124 | dt:281.44ms | tok/s:14,554 | MFU:3.14% | GPU RAM:3.24GB
step: 234 | loss:4.7416 | dt:279.50ms | tok/s:14,655 | MFU:3.17% | GPU RAM:3.24GB
step: 235 | loss:4.6354 | dt:292.20ms | tok/s:14,018 | MFU:3.03% | GPU RAM:3.24GB
step: 236 | loss:4.0612 | dt:293.27ms | tok/s:13,967 | MFU:3.02% | GPU RAM:3.24GB
step: 237 | loss:4.7576 | dt:285.26ms | tok/s:14,359 | MFU:3.10% | GPU RAM:3.24GB
step: 238 | loss:4.4113 | dt:292.52ms | tok/s:14,002 | MFU:3.02% | GPU RAM:3.24GB
step: 239 | loss:4.6754 | dt:293.20ms | tok/s:13,970 | MFU:3.02% | GPU RAM:3.24GB
step: 240 | loss:5.0047 | dt:292.09ms | tok/s:14,023 | MFU:3.03% | GPU RAM:3.24GB
step: 241 | loss:4.8915 | dt:287.08ms | tok/s:14,268 | MFU:3.08% | GPU RAM:3.24GB
step: 242 | loss:4.5555 | dt:292.63ms | tok/s:13,997 | MFU:3.02% | GPU RAM:3.24GB
step: 243 | loss:4.5906 | dt:291.87ms | tok/s:14,034 | MFU:3.03% | GPU RAM:3.24GB
step: 244 | loss:4.7719 | dt:308.63ms | tok/s:13,271 | MFU:2.87% | GPU RAM:3.24GB
step: 245 | loss:4.7968 | dt:290.91ms | tok/s:14,080 | MFU:3.04% | GPU RAM:3.24GB
step: 246 | loss:4.8684 | dt:288.28ms | tok/s:14,208 | MFU:3.07% | GPU RAM:3.24GB
step: 247 | loss:4.9720 | dt:288.56ms | tok/s:14,194 | MFU:3.07% | GPU RAM:3.24GB
step: 248 | loss:4.8893 | dt:292.08ms | tok/s:14,024 | MFU:3.03% | GPU RAM:3.24GB
step: 249 | loss:4.4967 | dt:285.18ms | tok/s:14,363 | MFU:3.10% | GPU RAM:3.24GB
wandb: updating run metadata
wandb: ğŸš€ View run shakespeare_gqa_20260115_091731 at: https://wandb.ai/adeeb-idris-coep-technological-university/llm-training/runs/yggpg69m
wandb: â­ï¸ View project at: https://wandb.ai/adeeb-idris-coep-technological-university/llm-training
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260115_091731-yggpg69m/logs

âœ… Pipeline parallelism training complete!

Logs saved to: run_7_console_20260115_091718.log and run_7_error_20260115_091718.log

Copying output files for run 7...
  Scanning: /kaggle/working
  Scanning: .
  Scanning: ./run_7_logs
âœ“ Copied 0 files to ./Transformer_GPU/monitor_logs/run_7_20260115_091901

================================================================================
Committing and pushing to GitHub for run 7...
Configuring git identity...
âœ“ Files added to git
âœ“ Committed: Add monitor logs for run 7 - 0 files - 2026-01-15 09:19:01
âœ“ Pushed to GitHub successfully!

âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“
RUN 7 COMPLETED SUCCESSFULLY
Files saved: 0
Pushed to GitHub: âœ“
âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“

================================================================================
WAITING 10 SECONDS BEFORE NEXT RUN...
================================================================================

################################################################################
STARTING RUN 8/8
Time: 09:19:12
################################################################################

train.py updated with parallel_flag = 8
Running torchrun command for i=8...
W0115 09:19:13.794000 1228 torch/distributed/run.py:774] 
W0115 09:19:13.794000 1228 torch/distributed/run.py:774] *****************************************
W0115 09:19:13.794000 1228 torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0115 09:19:13.794000 1228 torch/distributed/run.py:774] *****************************************
[W115 09:19:13.883963097 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W115 09:19:13.884694542 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
/usr/local/lib/python3.12/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
parallel_flag :  8
parallel_flag :  8
[W115 09:19:18.051938214 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W115 09:19:18.052197910 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W115 09:19:18.052716812 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W115 09:19:18.053210280 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3
Num GPUs = 2
=============
parallel_flag -  8
parallel_flag -  8
parallel_flag -  8
=============
=============
parallel_flag -  8
parallel_flag -  8
parallel_flag -  8
=============
total parameters = 186,274,816, active parameters = 110,777,344
Using compiled model
[rank1]:W0115 09:19:24.754000 1236 torch/_logging/_internal.py:1154] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
wandb: Currently logged in as: adeeb-idris (adeeb-idris-coep-technological-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run bhfu8w66
wandb: Tracking run with wandb version 0.22.2
wandb: Run data is saved locally in /kaggle/working/Transformer_GPU/project/wandb/run-20260115_091924-bhfu8w66
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run shakespeare_gqa_20260115_091924
wandb: â­ï¸ View project at https://wandb.ai/adeeb-idris-coep-technological-university/llm-training
wandb: ğŸš€ View run at https://wandb.ai/adeeb-idris-coep-technological-university/llm-training/runs/bhfu8w66
WandB initialized: project=llm-training, run=shakespeare_gqa_20260115_091924
[rank0]:W0115 09:19:26.926000 1235 torch/_logging/_internal.py:1154] [0/0] Profiler function <class 'torch.autograd.profiler.record_function'> will be ignored
[rank1]:W0115 09:19:30.747000 1236 torch/_inductor/utils.py:1436] [7/0_1] Not enough SMs to use max_autotune_gemm mode
[rank0]:W0115 09:19:31.926000 1235 torch/_inductor/utils.py:1436] [7/0_1] Not enough SMs to use max_autotune_gemm mode
active-> 110777344
MFU: 0.08%
mfu 0.08%
step: 0 | loss:11.0135 | dt:14394.83ms | tok/s:569 | MFU:0.08% | GPU RAM:4.39GB
active-> 110777344
MFU: 1.19%
mfu 0.19%
step: 1 | loss:11.0999 | dt:983.25ms | tok/s:8,332 | MFU:1.19% | GPU RAM:5.68GB
active-> 110777344
MFU: 1.19%
mfu 0.29%
step: 2 | loss:11.0576 | dt:977.83ms | tok/s:8,378 | MFU:1.19% | GPU RAM:5.68GB
active-> 110777344
MFU: 1.18%
mfu 0.38%
step: 3 | loss:11.0937 | dt:988.03ms | tok/s:8,291 | MFU:1.18% | GPU RAM:5.70GB
active-> 110777344
MFU: 1.19%
mfu 0.46%
step: 4 | loss:11.1066 | dt:976.86ms | tok/s:8,386 | MFU:1.19% | GPU RAM:5.70GB
active-> 110777344
MFU: 1.19%
mfu 0.53%
step: 5 | loss:11.0355 | dt:982.25ms | tok/s:8,340 | MFU:1.19% | GPU RAM:5.70GB
active-> 110777344
MFU: 1.19%
mfu 0.60%
step: 6 | loss:11.0442 | dt:977.81ms | tok/s:8,378 | MFU:1.19% | GPU RAM:5.71GB
active-> 110777344
MFU: 1.18%
mfu 0.66%
step: 7 | loss:11.0815 | dt:985.41ms | tok/s:8,313 | MFU:1.18% | GPU RAM:5.71GB
active-> 110777344
MFU: 1.18%
mfu 0.71%
step: 8 | loss:10.9730 | dt:988.58ms | tok/s:8,287 | MFU:1.18% | GPU RAM:5.71GB
active-> 110777344
MFU: 1.18%
mfu 0.76%
step: 9 | loss:11.0060 | dt:987.48ms | tok/s:8,296 | MFU:1.18% | GPU RAM:5.71GB
active-> 110777344
MFU: 1.16%
mfu 0.80%
step: 10 | loss:11.0858 | dt:1001.63ms | tok/s:8,179 | MFU:1.16% | GPU RAM:5.71GB
active-> 110777344
MFU: 1.16%
mfu 0.83%
step: 11 | loss:10.9649 | dt:1003.73ms | tok/s:8,162 | MFU:1.16% | GPU RAM:5.72GB
active-> 110777344
MFU: 1.10%
mfu 0.86%
step: 12 | loss:11.0745 | dt:1058.11ms | tok/s:7,742 | MFU:1.10% | GPU RAM:5.72GB
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  Total MFLOPs  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                               aten::mm         2.60%      84.689ms         3.57%     116.031ms      46.375us        1.059s        48.32%        1.059s     423.118us          2502   2540993.249  
                        DistributedDataParallel.forward         0.00%       0.000us         0.00%       0.000us       0.000us     738.574ms        33.71%     738.574ms     184.643ms             4            --  
       autograd::engine::evaluate_function: MmBackward0         0.39%      12.701ms         3.61%     117.437ms     156.583us       0.000us         0.00%     694.328ms     925.771us           750            --  
                                            MmBackward0         0.51%      16.453ms         3.22%     104.736ms     139.649us       0.000us         0.00%     694.328ms     925.771us           750            --  
                                           aten::linear         0.38%      12.255ms         6.15%     199.788ms     110.258us       0.000us         0.00%     631.632ms     348.583us          1812            --  
                                          ProfilerStep*         0.87%      28.400ms        67.03%        2.179s        1.090s       0.000us         0.00%     600.101ms     300.050ms             2            --  
void magma_sgemmEx_kernel<float, __nv_bfloat16, __nv...         0.00%       0.000us         0.00%       0.000us       0.000us     522.519ms        23.85%     522.519ms     560.643us           932            --  
                                           forward_pass         0.03%       1.042ms        25.62%     833.023ms     208.256ms       0.000us         0.00%     512.986ms     128.247ms             4            --  
                             Torch-Compiled Region: 0/0         0.01%     178.258us        25.58%     831.717ms     207.929ms       0.000us         0.00%     512.978ms     128.245ms             4            --  
                        DistributedDataParallel.forward         0.18%       5.835ms        25.58%     831.539ms     207.885ms       0.000us         0.00%     512.978ms     128.245ms             4            --  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
Self CPU time total: 3.251s
Self CUDA time total: 2.191s

active-> 110777344
MFU: 0.05%
mfu 0.78%
step: 13 | loss:11.0238 | dt:23587.26ms | tok/s:347 | MFU:0.05% | GPU RAM:5.72GB
active-> 110777344
MFU: 1.06%
mfu 0.81%
step: 14 | loss:11.0084 | dt:1099.40ms | tok/s:7,451 | MFU:1.06% | GPU RAM:5.72GB
active-> 110777344
MFU: 1.17%
mfu 0.84%
step: 15 | loss:10.9578 | dt:993.59ms | tok/s:8,245 | MFU:1.17% | GPU RAM:5.72GB
active-> 110777344
MFU: 1.16%
mfu 0.88%
step: 16 | loss:11.0345 | dt:1007.07ms | tok/s:8,134 | MFU:1.16% | GPU RAM:5.72GB
active-> 110777344
MFU: 1.17%
mfu 0.91%
step: 17 | loss:11.0412 | dt:993.13ms | tok/s:8,249 | MFU:1.17% | GPU RAM:5.72GB
active-> 110777344
MFU: 1.16%
mfu 0.93%
step: 18 | loss:10.9964 | dt:1007.75ms | tok/s:8,129 | MFU:1.16% | GPU RAM:5.72GB
active-> 110777344
MFU: 1.16%
mfu 0.95%
step: 19 | loss:11.0226 | dt:1003.76ms | tok/s:8,161 | MFU:1.16% | GPU RAM:5.72GB
active-> 110777344
MFU: 1.15%
mfu 0.97%
step: 20 | loss:10.9723 | dt:1011.63ms | tok/s:8,098 | MFU:1.15% | GPU RAM:5.72GB
active-> 110777344
MFU: 1.16%
mfu 0.99%
step: 21 | loss:10.9435 | dt:1009.44ms | tok/s:8,115 | MFU:1.16% | GPU RAM:5.72GB
active-> 110777344
MFU: 1.15%
mfu 1.01%
step: 22 | loss:10.9938 | dt:1013.63ms | tok/s:8,082 | MFU:1.15% | GPU RAM:5.72GB
active-> 110777344
MFU: 1.15%
mfu 1.02%
step: 23 | loss:10.9072 | dt:1016.99ms | tok/s:8,055 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.04%
step: 24 | loss:10.9076 | dt:1005.61ms | tok/s:8,146 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.05%
step: 25 | loss:10.8646 | dt:1013.29ms | tok/s:8,085 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.06%
step: 26 | loss:10.9145 | dt:1017.18ms | tok/s:8,054 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.14%
mfu 1.07%
step: 27 | loss:10.8334 | dt:1021.47ms | tok/s:8,020 | MFU:1.14% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.14%
mfu 1.07%
step: 28 | loss:10.7720 | dt:1020.73ms | tok/s:8,026 | MFU:1.14% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.14%
mfu 1.08%
step: 29 | loss:10.8173 | dt:1021.97ms | tok/s:8,016 | MFU:1.14% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.14%
mfu 1.09%
step: 30 | loss:10.9060 | dt:1026.44ms | tok/s:7,981 | MFU:1.14% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.13%
mfu 1.09%
step: 31 | loss:10.8013 | dt:1027.50ms | tok/s:7,973 | MFU:1.13% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.13%
mfu 1.09%
step: 32 | loss:10.7289 | dt:1034.85ms | tok/s:7,916 | MFU:1.13% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.13%
mfu 1.10%
step: 33 | loss:10.7680 | dt:1028.69ms | tok/s:7,964 | MFU:1.13% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.14%
mfu 1.10%
step: 34 | loss:10.7652 | dt:1025.35ms | tok/s:7,989 | MFU:1.14% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.14%
mfu 1.11%
step: 35 | loss:10.6956 | dt:1023.59ms | tok/s:8,003 | MFU:1.14% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.13%
mfu 1.11%
step: 36 | loss:10.7253 | dt:1032.00ms | tok/s:7,938 | MFU:1.13% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.14%
mfu 1.11%
step: 37 | loss:10.7580 | dt:1022.63ms | tok/s:8,011 | MFU:1.14% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.14%
mfu 1.11%
step: 38 | loss:10.6491 | dt:1027.11ms | tok/s:7,976 | MFU:1.14% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.13%
mfu 1.12%
step: 39 | loss:10.5934 | dt:1028.18ms | tok/s:7,967 | MFU:1.13% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.14%
mfu 1.12%
step: 40 | loss:10.5534 | dt:1018.63ms | tok/s:8,042 | MFU:1.14% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.12%
step: 41 | loss:10.6840 | dt:1016.27ms | tok/s:8,061 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.12%
step: 42 | loss:10.6357 | dt:1015.17ms | tok/s:8,070 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.14%
mfu 1.13%
step: 43 | loss:10.4928 | dt:1019.04ms | tok/s:8,039 | MFU:1.14% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.13%
step: 44 | loss:10.4969 | dt:1013.87ms | tok/s:8,080 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.13%
step: 45 | loss:10.4194 | dt:1014.64ms | tok/s:8,074 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.13%
step: 46 | loss:10.4246 | dt:1012.63ms | tok/s:8,090 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.13%
step: 47 | loss:10.5074 | dt:1015.34ms | tok/s:8,068 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.14%
step: 48 | loss:10.3264 | dt:1015.68ms | tok/s:8,065 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 0.21%
mfu 1.04%
step: 49 | loss:10.4727 | dt:5532.10ms | tok/s:1,481 | MFU:0.21% | GPU RAM:5.73GB
--------val run-------- train loss 10.3597 | val loss 10.3432 | dt 44418.3333ms
active-> 110777344
MFU: 1.01%
mfu 1.04%
step: 50 | loss:10.3986 | dt:1151.77ms | tok/s:7,113 | MFU:1.01% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.14%
mfu 1.05%
step: 51 | loss:10.3346 | dt:1025.36ms | tok/s:7,989 | MFU:1.14% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.06%
step: 52 | loss:10.3625 | dt:994.28ms | tok/s:8,239 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.07%
step: 53 | loss:10.3168 | dt:1010.45ms | tok/s:8,107 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.08%
step: 54 | loss:9.9805 | dt:1005.69ms | tok/s:8,146 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.09%
step: 55 | loss:10.1857 | dt:1004.30ms | tok/s:8,157 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.10%
step: 56 | loss:10.1447 | dt:1008.33ms | tok/s:8,124 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.10%
step: 57 | loss:10.1574 | dt:1004.28ms | tok/s:8,157 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.11%
step: 58 | loss:9.9796 | dt:1003.98ms | tok/s:8,160 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.11%
step: 59 | loss:9.9854 | dt:1015.25ms | tok/s:8,069 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.12%
step: 60 | loss:10.0956 | dt:1011.09ms | tok/s:8,102 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.12%
step: 61 | loss:9.9904 | dt:1011.44ms | tok/s:8,099 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.12%
step: 62 | loss:9.8409 | dt:1015.55ms | tok/s:8,067 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.13%
step: 63 | loss:9.9883 | dt:1003.34ms | tok/s:8,165 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.13%
step: 64 | loss:9.8618 | dt:1013.33ms | tok/s:8,084 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.13%
step: 65 | loss:9.8746 | dt:1010.40ms | tok/s:8,108 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.13%
step: 66 | loss:9.9334 | dt:1000.21ms | tok/s:8,190 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.14%
step: 67 | loss:9.9170 | dt:1006.00ms | tok/s:8,143 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.14%
step: 68 | loss:9.8049 | dt:998.44ms | tok/s:8,205 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.14%
step: 69 | loss:9.7369 | dt:1003.65ms | tok/s:8,162 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.14%
step: 70 | loss:9.5742 | dt:1007.18ms | tok/s:8,134 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.15%
step: 71 | loss:9.5232 | dt:1002.69ms | tok/s:8,170 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.15%
step: 72 | loss:9.6283 | dt:998.86ms | tok/s:8,201 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.15%
step: 73 | loss:9.2877 | dt:1009.95ms | tok/s:8,111 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.15%
step: 74 | loss:9.3152 | dt:998.58ms | tok/s:8,204 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.15%
step: 75 | loss:9.2634 | dt:1007.85ms | tok/s:8,128 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.15%
step: 76 | loss:9.3596 | dt:1001.13ms | tok/s:8,183 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.15%
step: 77 | loss:9.3356 | dt:998.56ms | tok/s:8,204 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.16%
step: 78 | loss:9.0508 | dt:1000.32ms | tok/s:8,189 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.16%
step: 79 | loss:9.2254 | dt:1002.28ms | tok/s:8,173 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.16%
step: 80 | loss:9.0814 | dt:995.83ms | tok/s:8,226 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.16%
step: 81 | loss:8.8814 | dt:996.51ms | tok/s:8,221 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.18%
mfu 1.16%
step: 82 | loss:8.8734 | dt:991.95ms | tok/s:8,258 | MFU:1.18% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.16%
step: 83 | loss:8.8306 | dt:1005.75ms | tok/s:8,145 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.16%
step: 84 | loss:8.8203 | dt:1002.34ms | tok/s:8,173 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.16%
step: 85 | loss:8.9116 | dt:1011.47ms | tok/s:8,099 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.16%
step: 86 | loss:8.7630 | dt:998.74ms | tok/s:8,202 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.18%
mfu 1.16%
step: 87 | loss:8.8180 | dt:988.22ms | tok/s:8,290 | MFU:1.18% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.16%
step: 88 | loss:8.8392 | dt:994.56ms | tok/s:8,237 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.18%
mfu 1.17%
step: 89 | loss:8.4135 | dt:987.71ms | tok/s:8,294 | MFU:1.18% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.17%
step: 90 | loss:8.3485 | dt:1001.44ms | tok/s:8,180 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.18%
mfu 1.17%
step: 91 | loss:8.5590 | dt:992.04ms | tok/s:8,258 | MFU:1.18% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.18%
mfu 1.17%
step: 92 | loss:8.4777 | dt:990.22ms | tok/s:8,273 | MFU:1.18% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.18%
mfu 1.17%
step: 93 | loss:8.2506 | dt:988.35ms | tok/s:8,289 | MFU:1.18% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.18%
mfu 1.17%
step: 94 | loss:8.1293 | dt:987.69ms | tok/s:8,294 | MFU:1.18% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.18%
mfu 1.17%
step: 95 | loss:8.2238 | dt:990.63ms | tok/s:8,269 | MFU:1.18% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.18%
mfu 1.17%
step: 96 | loss:8.2610 | dt:989.01ms | tok/s:8,283 | MFU:1.18% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.19%
mfu 1.17%
step: 97 | loss:8.0892 | dt:983.98ms | tok/s:8,325 | MFU:1.19% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.18%
mfu 1.17%
step: 98 | loss:7.9520 | dt:987.81ms | tok/s:8,293 | MFU:1.18% | GPU RAM:5.73GB
active-> 110777344
MFU: 0.22%
mfu 1.08%
step: 99 | loss:7.7885 | dt:5310.16ms | tok/s:1,543 | MFU:0.22% | GPU RAM:5.73GB
--------val run-------- train loss 7.9443 | val loss 8.2888 | dt 40992.5140ms
active-> 110777344
MFU: 1.06%
mfu 1.08%
step: 100 | loss:8.1742 | dt:1104.64ms | tok/s:7,416 | MFU:1.06% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.12%
mfu 1.08%
step: 101 | loss:7.9649 | dt:1041.67ms | tok/s:7,864 | MFU:1.12% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.13%
mfu 1.09%
step: 102 | loss:7.9459 | dt:1027.85ms | tok/s:7,970 | MFU:1.13% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.18%
mfu 1.09%
step: 103 | loss:7.4439 | dt:989.71ms | tok/s:8,277 | MFU:1.18% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.14%
mfu 1.10%
step: 104 | loss:7.7157 | dt:1024.76ms | tok/s:7,994 | MFU:1.14% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.11%
step: 105 | loss:7.6487 | dt:994.85ms | tok/s:8,234 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.11%
step: 106 | loss:7.8870 | dt:995.02ms | tok/s:8,233 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.19%
mfu 1.12%
step: 107 | loss:7.3911 | dt:977.07ms | tok/s:8,384 | MFU:1.19% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.14%
mfu 1.12%
step: 108 | loss:7.9043 | dt:1021.51ms | tok/s:8,020 | MFU:1.14% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.13%
step: 109 | loss:7.5189 | dt:997.95ms | tok/s:8,209 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.13%
step: 110 | loss:7.5241 | dt:996.06ms | tok/s:8,224 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.18%
mfu 1.14%
step: 111 | loss:7.0349 | dt:987.92ms | tok/s:8,292 | MFU:1.18% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.18%
mfu 1.14%
step: 112 | loss:7.3681 | dt:990.89ms | tok/s:8,267 | MFU:1.18% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.14%
step: 113 | loss:7.4179 | dt:993.07ms | tok/s:8,249 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.18%
mfu 1.15%
step: 114 | loss:7.3723 | dt:987.46ms | tok/s:8,296 | MFU:1.18% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.15%
step: 115 | loss:7.3602 | dt:994.39ms | tok/s:8,238 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.15%
step: 116 | loss:7.4924 | dt:996.89ms | tok/s:8,218 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.19%
mfu 1.16%
step: 117 | loss:7.1148 | dt:978.68ms | tok/s:8,370 | MFU:1.19% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.19%
mfu 1.16%
step: 118 | loss:7.2331 | dt:979.83ms | tok/s:8,361 | MFU:1.19% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.18%
mfu 1.16%
step: 119 | loss:7.3907 | dt:991.57ms | tok/s:8,262 | MFU:1.18% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.16%
step: 120 | loss:7.3074 | dt:994.36ms | tok/s:8,238 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.16%
step: 121 | loss:7.0259 | dt:999.77ms | tok/s:8,194 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.19%
mfu 1.17%
step: 122 | loss:7.4936 | dt:979.37ms | tok/s:8,365 | MFU:1.19% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.18%
mfu 1.17%
step: 123 | loss:6.9642 | dt:984.64ms | tok/s:8,320 | MFU:1.18% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.19%
mfu 1.17%
step: 124 | loss:6.9154 | dt:978.80ms | tok/s:8,369 | MFU:1.19% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.19%
mfu 1.17%
step: 125 | loss:6.9487 | dt:978.66ms | tok/s:8,371 | MFU:1.19% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.19%
mfu 1.17%
step: 126 | loss:7.0390 | dt:977.79ms | tok/s:8,378 | MFU:1.19% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.17%
step: 127 | loss:7.2050 | dt:992.39ms | tok/s:8,255 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.19%
mfu 1.18%
step: 128 | loss:7.1255 | dt:981.67ms | tok/s:8,345 | MFU:1.19% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.19%
mfu 1.18%
step: 129 | loss:6.8766 | dt:983.25ms | tok/s:8,332 | MFU:1.19% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.20%
mfu 1.18%
step: 130 | loss:6.9468 | dt:975.23ms | tok/s:8,400 | MFU:1.20% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.20%
mfu 1.18%
step: 131 | loss:7.0454 | dt:973.25ms | tok/s:8,417 | MFU:1.20% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.20%
mfu 1.18%
step: 132 | loss:6.7846 | dt:970.97ms | tok/s:8,437 | MFU:1.20% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.19%
mfu 1.18%
step: 133 | loss:6.7142 | dt:982.44ms | tok/s:8,338 | MFU:1.19% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.18%
mfu 1.18%
step: 134 | loss:6.8540 | dt:988.56ms | tok/s:8,287 | MFU:1.18% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.18%
mfu 1.18%
step: 135 | loss:6.7861 | dt:986.84ms | tok/s:8,301 | MFU:1.18% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.20%
mfu 1.18%
step: 136 | loss:7.0357 | dt:973.88ms | tok/s:8,412 | MFU:1.20% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.19%
mfu 1.18%
step: 137 | loss:7.0783 | dt:981.82ms | tok/s:8,344 | MFU:1.19% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.19%
mfu 1.18%
step: 138 | loss:6.9867 | dt:982.30ms | tok/s:8,340 | MFU:1.19% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.18%
mfu 1.18%
step: 139 | loss:7.2794 | dt:988.99ms | tok/s:8,283 | MFU:1.18% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.19%
mfu 1.18%
step: 140 | loss:7.3635 | dt:983.52ms | tok/s:8,329 | MFU:1.19% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.19%
mfu 1.18%
step: 141 | loss:7.0805 | dt:978.06ms | tok/s:8,376 | MFU:1.19% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.18%
step: 142 | loss:7.1757 | dt:995.89ms | tok/s:8,226 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.18%
mfu 1.18%
step: 143 | loss:7.1234 | dt:989.87ms | tok/s:8,276 | MFU:1.18% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.18%
step: 144 | loss:6.6396 | dt:1003.48ms | tok/s:8,164 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.18%
step: 145 | loss:6.4720 | dt:1002.43ms | tok/s:8,172 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.18%
step: 146 | loss:6.8326 | dt:999.34ms | tok/s:8,197 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.18%
step: 147 | loss:6.5333 | dt:996.95ms | tok/s:8,217 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.18%
step: 148 | loss:6.7985 | dt:1004.54ms | tok/s:8,155 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 0.22%
mfu 1.08%
step: 149 | loss:7.0791 | dt:5335.25ms | tok/s:1,535 | MFU:0.22% | GPU RAM:5.73GB
--------val run-------- train loss 6.7429 | val loss 7.3049 | dt 40924.7783ms
active-> 110777344
MFU: 1.06%
mfu 1.08%
step: 150 | loss:6.7849 | dt:1099.03ms | tok/s:7,454 | MFU:1.06% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.18%
mfu 1.09%
step: 151 | loss:6.9566 | dt:990.26ms | tok/s:8,273 | MFU:1.18% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.09%
step: 152 | loss:6.4593 | dt:1016.78ms | tok/s:8,057 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.18%
mfu 1.10%
step: 153 | loss:6.5492 | dt:990.62ms | tok/s:8,270 | MFU:1.18% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.19%
mfu 1.11%
step: 154 | loss:6.8155 | dt:983.19ms | tok/s:8,332 | MFU:1.19% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.11%
step: 155 | loss:6.7287 | dt:1016.47ms | tok/s:8,059 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.12%
step: 156 | loss:6.6721 | dt:1002.67ms | tok/s:8,170 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.18%
mfu 1.12%
step: 157 | loss:6.4594 | dt:990.61ms | tok/s:8,270 | MFU:1.18% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.19%
mfu 1.13%
step: 158 | loss:6.7000 | dt:979.18ms | tok/s:8,366 | MFU:1.19% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.13%
step: 159 | loss:6.6213 | dt:1000.23ms | tok/s:8,190 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.14%
step: 160 | loss:6.7248 | dt:992.73ms | tok/s:8,252 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.14%
step: 161 | loss:7.1995 | dt:994.19ms | tok/s:8,240 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.15%
step: 162 | loss:6.6385 | dt:992.73ms | tok/s:8,252 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.15%
step: 163 | loss:6.5659 | dt:1009.80ms | tok/s:8,113 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.15%
step: 164 | loss:6.2933 | dt:1010.16ms | tok/s:8,110 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.15%
step: 165 | loss:6.4416 | dt:994.43ms | tok/s:8,238 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.15%
step: 166 | loss:6.9204 | dt:1001.39ms | tok/s:8,181 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.15%
step: 167 | loss:6.4864 | dt:1010.30ms | tok/s:8,108 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.15%
step: 168 | loss:6.6897 | dt:996.56ms | tok/s:8,220 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.16%
step: 169 | loss:6.6907 | dt:993.29ms | tok/s:8,247 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.16%
step: 170 | loss:6.3491 | dt:999.11ms | tok/s:8,199 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.16%
step: 171 | loss:5.9626 | dt:1002.75ms | tok/s:8,170 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.16%
step: 172 | loss:6.3957 | dt:1008.58ms | tok/s:8,122 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.16%
step: 173 | loss:6.8810 | dt:998.42ms | tok/s:8,205 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.16%
step: 174 | loss:6.3822 | dt:997.37ms | tok/s:8,214 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.16%
step: 175 | loss:6.9196 | dt:1008.82ms | tok/s:8,120 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.16%
step: 176 | loss:6.5795 | dt:998.02ms | tok/s:8,208 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.18%
mfu 1.16%
step: 177 | loss:6.4624 | dt:988.34ms | tok/s:8,289 | MFU:1.18% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.16%
step: 178 | loss:6.5731 | dt:994.13ms | tok/s:8,240 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.19%
mfu 1.17%
step: 179 | loss:6.2072 | dt:982.22ms | tok/s:8,340 | MFU:1.19% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.18%
mfu 1.17%
step: 180 | loss:6.4969 | dt:985.18ms | tok/s:8,315 | MFU:1.18% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.17%
step: 181 | loss:6.4960 | dt:999.66ms | tok/s:8,195 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.18%
mfu 1.17%
step: 182 | loss:6.8333 | dt:985.42ms | tok/s:8,313 | MFU:1.18% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.17%
step: 183 | loss:6.2420 | dt:1011.05ms | tok/s:8,102 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.17%
step: 184 | loss:6.2125 | dt:999.00ms | tok/s:8,200 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.17%
step: 185 | loss:6.4589 | dt:997.37ms | tok/s:8,214 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.17%
step: 186 | loss:6.5369 | dt:997.80ms | tok/s:8,210 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.17%
step: 187 | loss:6.3828 | dt:992.58ms | tok/s:8,253 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.17%
step: 188 | loss:6.2069 | dt:995.00ms | tok/s:8,233 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.18%
mfu 1.17%
step: 189 | loss:6.2615 | dt:992.19ms | tok/s:8,257 | MFU:1.18% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.17%
step: 190 | loss:6.2109 | dt:992.66ms | tok/s:8,253 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.17%
step: 191 | loss:6.4940 | dt:1003.27ms | tok/s:8,165 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.17%
step: 192 | loss:6.5291 | dt:1002.52ms | tok/s:8,171 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.17%
step: 193 | loss:6.5546 | dt:1002.36ms | tok/s:8,173 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.17%
step: 194 | loss:6.2147 | dt:995.24ms | tok/s:8,231 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.17%
step: 195 | loss:6.6961 | dt:996.98ms | tok/s:8,217 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.18%
mfu 1.17%
step: 196 | loss:6.6869 | dt:985.96ms | tok/s:8,309 | MFU:1.18% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.18%
mfu 1.17%
step: 197 | loss:6.4231 | dt:990.29ms | tok/s:8,272 | MFU:1.18% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.17%
step: 198 | loss:6.2711 | dt:1001.44ms | tok/s:8,180 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 0.22%
mfu 1.07%
step: 199 | loss:6.5804 | dt:5368.25ms | tok/s:1,526 | MFU:0.22% | GPU RAM:5.73GB
--------val run-------- train loss 6.3950 | val loss 7.0967 | dt 40846.5923ms
active-> 110777344
MFU: 1.09%
mfu 1.08%
step: 200 | loss:6.8039 | dt:1069.46ms | tok/s:7,660 | MFU:1.09% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.08%
step: 201 | loss:6.0378 | dt:1011.34ms | tok/s:8,100 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.17%
mfu 1.09%
step: 202 | loss:6.5251 | dt:996.83ms | tok/s:8,218 | MFU:1.17% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.10%
step: 203 | loss:6.0788 | dt:1009.10ms | tok/s:8,118 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.10%
step: 204 | loss:6.1828 | dt:1006.41ms | tok/s:8,140 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.18%
mfu 1.11%
step: 205 | loss:6.5011 | dt:991.57ms | tok/s:8,262 | MFU:1.18% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.12%
step: 206 | loss:6.3323 | dt:1018.07ms | tok/s:8,047 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.12%
step: 207 | loss:6.5178 | dt:1010.65ms | tok/s:8,106 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.12%
step: 208 | loss:6.3144 | dt:1016.02ms | tok/s:8,063 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.13%
step: 209 | loss:6.1527 | dt:1001.12ms | tok/s:8,183 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.13%
step: 210 | loss:6.5293 | dt:1015.41ms | tok/s:8,068 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.13%
step: 211 | loss:6.4303 | dt:1007.16ms | tok/s:8,134 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.13%
step: 212 | loss:6.6160 | dt:1006.65ms | tok/s:8,138 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.14%
step: 213 | loss:6.4155 | dt:1012.97ms | tok/s:8,087 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.14%
step: 214 | loss:6.8589 | dt:1010.60ms | tok/s:8,106 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.14%
step: 215 | loss:6.1409 | dt:1009.70ms | tok/s:8,113 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.14%
step: 216 | loss:6.4148 | dt:1013.80ms | tok/s:8,080 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.14%
mfu 1.14%
step: 217 | loss:6.3168 | dt:1019.45ms | tok/s:8,036 | MFU:1.14% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.14%
step: 218 | loss:6.3703 | dt:1008.61ms | tok/s:8,122 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.14%
step: 219 | loss:6.2275 | dt:1014.99ms | tok/s:8,071 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.14%
mfu 1.14%
step: 220 | loss:6.4175 | dt:1025.97ms | tok/s:7,985 | MFU:1.14% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.14%
step: 221 | loss:6.4250 | dt:1015.67ms | tok/s:8,066 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.14%
mfu 1.14%
step: 222 | loss:6.8147 | dt:1019.70ms | tok/s:8,034 | MFU:1.14% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.14%
step: 223 | loss:6.5708 | dt:1014.51ms | tok/s:8,075 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.14%
step: 224 | loss:6.0749 | dt:1012.67ms | tok/s:8,089 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.14%
mfu 1.14%
step: 225 | loss:6.0805 | dt:1020.89ms | tok/s:8,024 | MFU:1.14% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.15%
step: 226 | loss:6.3667 | dt:1003.91ms | tok/s:8,160 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.15%
step: 227 | loss:6.4300 | dt:1013.96ms | tok/s:8,079 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.15%
step: 228 | loss:6.4271 | dt:1013.79ms | tok/s:8,081 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.15%
step: 229 | loss:6.2487 | dt:1011.81ms | tok/s:8,096 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.15%
step: 230 | loss:6.1786 | dt:1004.70ms | tok/s:8,154 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.15%
step: 231 | loss:6.3959 | dt:1004.98ms | tok/s:8,151 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.15%
step: 232 | loss:6.4431 | dt:1014.40ms | tok/s:8,076 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.15%
step: 233 | loss:6.0037 | dt:1009.92ms | tok/s:8,112 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.15%
step: 234 | loss:6.2140 | dt:1012.71ms | tok/s:8,089 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.15%
step: 235 | loss:6.4792 | dt:1014.44ms | tok/s:8,075 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.14%
mfu 1.15%
step: 236 | loss:6.5612 | dt:1020.41ms | tok/s:8,028 | MFU:1.14% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.15%
mfu 1.15%
step: 237 | loss:6.6603 | dt:1012.31ms | tok/s:8,092 | MFU:1.15% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.15%
step: 238 | loss:6.3915 | dt:1008.54ms | tok/s:8,123 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.15%
step: 239 | loss:6.7039 | dt:1006.04ms | tok/s:8,143 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.15%
step: 240 | loss:6.1966 | dt:1009.29ms | tok/s:8,117 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.15%
step: 241 | loss:6.1320 | dt:1002.50ms | tok/s:8,172 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.15%
step: 242 | loss:6.6643 | dt:1004.60ms | tok/s:8,155 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.15%
step: 243 | loss:6.4123 | dt:1001.74ms | tok/s:8,178 | MFU:1.16% | GPU RAM:5.73GB
active-> 110777344
MFU: 1.16%
mfu 1.16%
step: 244 | loss:6.1731 | dt:1006.25ms | tok/s:8,141 | MFU:1.16% | GPU RAM:5.74GB
active-> 110777344
MFU: 1.16%
mfu 1.16%
step: 245 | loss:6.1161 | dt:1000.95ms | tok/s:8,184 | MFU:1.16% | GPU RAM:5.74GB
active-> 110777344
MFU: 1.15%
mfu 1.16%
step: 246 | loss:6.9579 | dt:1013.21ms | tok/s:8,085 | MFU:1.15% | GPU RAM:5.74GB
active-> 110777344
MFU: 1.17%
mfu 1.16%
step: 247 | loss:6.3004 | dt:999.83ms | tok/s:8,193 | MFU:1.17% | GPU RAM:5.74GB
active-> 110777344
MFU: 1.16%
mfu 1.16%
step: 248 | loss:6.3495 | dt:1008.60ms | tok/s:8,122 | MFU:1.16% | GPU RAM:5.74GB
active-> 110777344
MFU: 0.22%
mfu 1.06%
step: 249 | loss:6.6722 | dt:5282.66ms | tok/s:1,551 | MFU:0.22% | GPU RAM:5.74GB
--------val run-------- train loss 6.3662 | val loss 7.0604 | dt 41106.5614ms

ğŸ“Š PROFILER USAGE GUIDE:

1. Traces are saved to ./profiler_logs/
2. View in Chrome: chrome://tracing (load JSON files)
3. View in TensorBoard: 
   tensorboard --logdir=./profiler_logs
   
4. Key metrics to look for:
   - CUDA kernel launch overhead
   - Memory allocation patterns
   - CPU/GPU utilization
   - Communication overhead (DDP/FSDP)
   - Kernel execution time

active-> 110777344
MFU: 1.08%
mfu 1.06%
step: 250 | loss:6.3654 | dt:1084.59ms | tok/s:7,553 | MFU:1.08% | GPU RAM:5.74GB
wandb: 
wandb: Run history:
wandb:                    memory/allocated_gb â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–ƒâ–â–…â–ƒâ–‚â–ƒâ–„â–…â–‚â–„â–…â–…â–…â–†â–‡â–‡â–‡â–ˆâ–‡â–‡
wandb:                memory/max_allocated_gb â–â–„â–„â–„â–„â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:                     memory/reserved_gb â–â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–†â–ˆ
wandb:                 perf/iteration_time_ms â–â–â–â–â–â–â–ˆâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb:                       perf/mfu_percent â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡
wandb:         perf/throughput_tokens_per_sec â–â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
wandb: perf/throughput_tokens_per_sec_per_gpu â–‚â–â–â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–…â–„â–…â–„â–„â–†â–â–‚â–…â–‡â–‡â–ˆâ–ˆâ–…â–…â–†â–„â–ƒâ–ƒâ–ƒâ–†â–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–ƒ
wandb:                        train/grad_norm â–‡â–‡â–‡â–ˆâ–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–†â–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–
wandb:                             train/loss â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–…â–…â–…â–„â–„â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–‚â–â–â–â–‚â–â–â–â–â–‚
wandb:                               train/lr â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–†â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–„â–„â–„â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚
wandb:                                     +1 ...
wandb: 
wandb: Run summary:
wandb:                    memory/allocated_gb 3.36064
wandb:                memory/max_allocated_gb 5.40524
wandb:                     memory/reserved_gb 5.74414
wandb:                 perf/iteration_time_ms 1084.59289
wandb:                       perf/mfu_percent 1.07509
wandb:         perf/throughput_tokens_per_sec 7553.06443
wandb: perf/throughput_tokens_per_sec_per_gpu 3776.53221
wandb:                        train/grad_norm 0.27296
wandb:                             train/loss 6.36542
wandb:                               train/lr 3e-05
wandb:                                     +1 ...
wandb: 
wandb: ğŸš€ View run shakespeare_gqa_20260115_091924 at: https://wandb.ai/adeeb-idris-coep-technological-university/llm-training/runs/bhfu8w66
wandb: â­ï¸ View project at: https://wandb.ai/adeeb-idris-coep-technological-university/llm-training
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20260115_091924-bhfu8w66/logs
WandB run completed

ğŸ“Š PROFILER USAGE GUIDE:

1. Traces are saved to ./profiler_logs/
2. View in Chrome: chrome://tracing (load JSON files)
3. View in TensorBoard: 
   tensorboard --logdir=./profiler_logs
   
4. Key metrics to look for:
   - CUDA kernel launch overhead
   - Memory allocation patterns
   - CPU/GPU utilization
   - Communication overhead (DDP/FSDP)
   - Kernel execution time

[rank0]:[W115 09:28:17.661851450 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())

Logs saved to: run_8_console_20260115_091912.log and run_8_error_20260115_091912.log

Copying output files for run 8...
  Scanning: /kaggle/working
  Scanning: .
  Scanning: ./run_8_logs
âœ“ Copied 0 files to ./Transformer_GPU/monitor_logs/run_8_20260115_092819

================================================================================
Committing and pushing to GitHub for run 8...
Configuring git identity...
âœ“ Files added to git
âœ“ Committed: Add monitor logs for run 8 - 0 files - 2026-01-15 09:28:19
âœ“ Pushed to GitHub successfully!

âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“
RUN 8 COMPLETED SUCCESSFULLY
Files saved: 0
Pushed to GitHub: âœ“
âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“âœ“

================================================================================
CREATING FINAL EXECUTION SUMMARY...

ğŸ“‹ Execution summary saved to: ./Transformer_GPU/monitor_logs/execution_summary.json
Copying complete execution log to GitHub...
